python train.py --scenario wanderer1_2agents-1 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-4 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-4 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-4 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-1__2018-07-12_10-36-31...
200 50
steps: 9950, episodes: 200, mean episode reward: -187.1112150720295, time: 32.829
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.99497487  -0.45226131 -10.6       ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.70351759  -0.45728643 -11.75095477]
400 50
steps: 19950, episodes: 400, mean episode reward: -173.85449132298243, time: 38.219
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.005   -0.435  -10.4166]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.66    -0.505  -11.7641]
600 50
steps: 29950, episodes: 600, mean episode reward: -193.82532021814973, time: 37.417
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.79    -0.435  -10.3602]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.17    -0.495  -11.4855]
800 50
steps: 39950, episodes: 800, mean episode reward: -167.75649928629332, time: 37.773
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.75    -0.46   -10.2986]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.52    -0.55   -11.5939]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -169.6799316980874, time: 36.459
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.095   -0.415  -10.4255]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.365   -0.515  -11.4774]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -844.3162275331574, time: 50.142
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.72    -0.62   -15.0607]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.945   -0.49   -11.2239]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -367.64477003554333, time: 52.9
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.46    -0.25   -10.7595]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.795   -0.66   -16.3642]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -144.76756969519812, time: 52.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.685   -0.115   -4.5769]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.73    -0.135   -5.0705]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -138.9131043733684, time: 52.972
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.09    -0.15    -6.9051]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.91   -0.115  -3.3328]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -128.7194826640467, time: 53.269
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.05   -0.02   -2.3612]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.78   -0.075  -2.4751]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -126.30482044766562, time: 53.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.385 -0.025 -1.713]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.74   -0.065  -1.0807]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -132.62790488089334, time: 52.957
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.285  -0.075  -3.5294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.91   -0.07   -1.6601]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -110.77420972025472, time: 52.79
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.425  -0.055  -2.0536]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.125  -0.075  -0.7483]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -122.54164039901706, time: 52.947
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.985  -0.075  -2.3999]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.56   -0.145  -2.3376]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -111.43228714334538, time: 52.879
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.535  -0.085  -2.9613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.38  -0.095 -1.356]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -121.92603467329545, time: 53.934
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.04   -0.085  -3.2526]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.85   -0.085  -2.3865]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -126.78398389912887, time: 53.026
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.645   -0.09    -4.5717]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.08   -0.165  -3.8853]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -118.57448036684185, time: 52.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.69    -0.115   -5.7806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.845   -0.215   -6.4091]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -110.50852299358415, time: 52.939
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.575   -0.225   -9.3799]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.185  -0.1    -2.9213]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -113.05655705488145, time: 53.909
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.19    -0.2     -9.5947]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.98    -0.135   -4.9895]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -114.89820385248477, time: 52.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.27    -0.325  -14.8744]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.12   -0.155  -4.0485]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -107.46419895254257, time: 52.813
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.15    -0.43   -14.1271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.38  -0.115 -2.746]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -116.25117357016293, time: 52.937
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.005   -0.62   -18.9255]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.635   -0.22    -6.1326]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -104.69794251246421, time: 52.789
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.86    -0.735  -20.1241]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.75   -0.13   -4.3764]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -93.89799457338975, time: 53.816
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.07   -0.63  -19.845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.555   -0.25    -7.2973]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -91.72118873031553, time: 52.003
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.295   -0.62   -20.7971]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.86    -0.31    -8.2598]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -84.51535662160717, time: 52.702
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.065   -0.785  -21.8105]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.625   -0.405  -10.4813]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -81.66867260346096, time: 51.818
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.885  -0.88  -23.457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.865   -0.37   -10.0823]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -85.25715282981236, time: 52.849
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.5     -0.88   -23.1632]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.15    -0.475  -13.9633]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -85.47300824216413, time: 52.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.82    -0.9    -23.5409]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.035   -0.48   -14.4483]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -73.65893726528947, time: 54.012
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.655   -0.86   -23.3517]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.36    -0.565  -16.9711]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-2__2018-07-12_10-36-35...
200 50
steps: 9950, episodes: 200, mean episode reward: -173.24183255444265, time: 35.666
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.88442211  -0.49748744 -12.23115578]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.72361809  -0.42211055 -11.60723618]
400 50
steps: 19950, episodes: 400, mean episode reward: -175.56911895543698, time: 38.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.925   -0.49   -12.3046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.505   -0.425  -11.6035]
600 50
steps: 29950, episodes: 600, mean episode reward: -170.7476207850589, time: 38.046
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.44    -0.49   -12.0408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.755  -0.44  -11.68 ]
800 50
steps: 39950, episodes: 800, mean episode reward: -168.5125737877839, time: 38.534
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.125   -0.495  -12.2828]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.52    -0.475  -11.6966]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -163.83944421270692, time: 38.187
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.865   -0.51   -12.1906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.9     -0.435  -11.6797]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -625.7125384491395, time: 53.569
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.03    -0.38   -10.1077]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.875   -0.785  -19.5666]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -183.14465275151838, time: 55.66
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.025   -0.43    -7.0581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.265   -0.27    -8.0092]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -172.87414720981985, time: 54.926
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.41    -0.495   -9.2757]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.89    -0.505   -8.5507]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -152.78509044225638, time: 55.536
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.185  -0.365  -4.8734]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.15    -0.53    -9.7762]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -145.1314605923733, time: 55.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.645  -0.315  -3.1755]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.115   -0.585  -12.3919]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -129.85973159743938, time: 55.367
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.19   -0.215  -2.0074]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.665   -0.47    -8.6075]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -109.02399066809063, time: 54.517
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.59  -0.205 -1.587]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.72    -0.445   -9.4626]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -137.40993484930095, time: 53.67
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.255  -0.18   -1.2795]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.495  -0.53  -10.114]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -109.62390227383482, time: 54.442
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.475  -0.245  -1.3231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.16    -0.55   -12.4965]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -103.37045534315483, time: 54.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.17   -0.14   -1.1651]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.675  -0.665 -15.935]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -108.40665245005565, time: 54.645
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.91  -0.235 -1.508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.89    -0.715  -18.0291]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -98.23566759491803, time: 52.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.045   -0.205   -4.7766]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.475   -0.76   -19.0889]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -93.41818293292731, time: 52.152
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.995   -0.25    -6.0207]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.925   -0.86   -21.7912]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -86.80913412322124, time: 52.226
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.285   -0.365  -10.2854]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.87    -0.8    -20.5266]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -84.02116992061259, time: 51.875
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.825   -0.385  -10.4481]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.545   -0.825  -22.6825]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -87.4740814446499, time: 50.861
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.395   -0.405  -11.4322]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.45    -0.85   -22.2083]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -80.69788281332008, time: 51.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.765   -0.645  -19.2153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.51    -0.825  -23.1809]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -74.87981241072588, time: 51.803
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.495   -0.675  -21.6422]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.425   -0.85   -23.5473]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -77.60923502498287, time: 51.313
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.145   -0.705  -22.6742]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.13    -0.815  -23.3466]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -70.41132295763188, time: 50.959
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.11    -0.67   -22.4567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.92    -0.845  -23.9306]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -80.24545858573461, time: 51.748
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.125   -0.74   -22.6341]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.64    -0.83   -23.6718]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -74.24079038223246, time: 52.007
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.225  -0.665 -21.336]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.88    -0.85   -24.0015]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -65.1761378446812, time: 52.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.015   -0.685  -22.3077]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.33    -0.85   -24.1493]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -71.64532280430777, time: 51.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.37    -0.705  -22.9561]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.455   -0.91   -24.4024]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -71.49377711148567, time: 50.707
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.845   -0.685  -22.9549]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.345   -0.825  -24.2731]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -80.8237317985478, time: 53.131
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.41    -0.74   -22.7614]
agent1_energy_min, agent1_energy_max, agent1_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-4__2018-07-12_10-36-45...
200 50
steps: 9950, episodes: 200, mean episode reward: -164.41313778510602, time: 36.431
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.78894472  -0.48241206 -14.42482412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.07035176  -0.50753769 -12.82663317]
400 50
steps: 19950, episodes: 400, mean episode reward: -161.16931411365283, time: 37.739
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.005   -0.53   -14.6032]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.47    -0.45   -12.5189]
600 50
steps: 29950, episodes: 600, mean episode reward: -169.76518297965677, time: 37.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.53    -0.51   -14.2198]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.53    -0.535  -12.9754]
800 50
steps: 39950, episodes: 800, mean episode reward: -156.8485511326871, time: 38.148
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.965   -0.51   -14.5146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.035   -0.48   -12.7406]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -161.94204829084435, time: 38.01
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.505   -0.525  -14.2797]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.08    -0.56   -12.7409]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -573.7307547983161, time: 52.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.41    -0.735  -20.0107]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.545   -0.425   -8.5176]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -248.12795325848393, time: 54.431
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.865   -0.255   -9.1387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.595   -0.71   -16.1299]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -170.89273802866708, time: 54.722
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.02    -0.355  -12.8982]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.285  -0.255  -2.3295]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -144.71815173130935, time: 54.006
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.64    -0.27    -6.5534]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.655  -0.125  -1.0221]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -121.00093915252262, time: 54.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.925  -0.22   -3.8259]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.625  -0.165  -1.0269]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -115.70137544383633, time: 54.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.34   -0.25   -5.0429]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.485  -0.19   -1.6513]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -120.87116832888219, time: 53.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.66    -0.37    -6.2776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.335 -0.3   -3.339]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -104.09278505580455, time: 53.009
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.405   -0.46   -10.7516]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.405  -0.315  -3.8159]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -80.18683725361106, time: 53.73
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.975   -0.625  -20.6348]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.985  -0.28   -2.6613]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -106.92226731926642, time: 53.269
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.11    -0.8    -23.7676]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.385  -0.415  -3.3814]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -82.93378079704567, time: 54.266
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.99   -0.71  -23.708]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.385  -0.495  -5.4217]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -86.65594177538014, time: 53.597
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.805   -0.7    -22.8347]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.75   -0.44   -5.2371]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -78.57131331190281, time: 53.856
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.36    -0.69   -22.9179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.295  -0.53   -5.1968]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -78.23177820594348, time: 53.694
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.22    -0.73   -22.5716]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.545  -0.555  -4.8054]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -81.01036270600406, time: 53.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.46    -0.625  -20.9029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.615  -0.615  -4.6875]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -75.87706411472637, time: 54.571
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.39    -0.59   -23.0162]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.3    -0.59   -5.7737]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -77.52052067354639, time: 54.237
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.32   -0.705 -23.688]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.635   -0.51    -6.1801]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -74.47677215642857, time: 54.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.43    -0.61   -23.4486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.48    -0.8     -9.4562]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -74.53661833385848, time: 53.935
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.29    -0.76   -23.5886]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.425   -0.7     -7.9649]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -63.234116273852145, time: 53.479
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.355   -0.675  -24.0857]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.86    -0.795  -10.1924]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -68.1276645085016, time: 53.584
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.46   -0.795 -24.354]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.365   -0.755  -10.8958]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -70.2088594210902, time: 53.532
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.145   -0.75   -24.0953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.065  -0.845 -11.362]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -74.23646009756315, time: 53.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.525   -0.73   -23.8392]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.53    -0.77    -9.8492]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -83.6667606283059, time: 52.946
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.145   -0.785  -23.8008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.2     -0.77    -9.9514]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -71.32580957614918, time: 52.834
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.92    -0.785  -24.1311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.09    -0.885  -12.4626]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -84.39265229064306, time: 53.445
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.315  -0.765 -23.18 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.85    -0.92   -12.6428]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-4__2018-07-12_10-36-49...
200 50
steps: 9950, episodes: 200, mean episode reward: -148.43105751043367, time: 36.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.43718593  -0.42713568 -10.92311558]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.          -0.44221106 -13.21226131]
400 50
steps: 19950, episodes: 400, mean episode reward: -160.02061084686932, time: 37.442
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.525   -0.475  -11.1067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.63    -0.5    -13.5971]
600 50
steps: 29950, episodes: 600, mean episode reward: -157.20305201266416, time: 37.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.73    -0.465  -10.5403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.85    -0.485  -13.7487]
800 50
steps: 39950, episodes: 800, mean episode reward: -169.86600892525792, time: 37.424
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.205   -0.415  -10.7855]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.45    -0.52   -13.4738]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -147.3312923144537, time: 37.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.23    -0.42   -10.8292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.455   -0.505  -13.4311]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -358.4529859261264, time: 53.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.01    -0.54   -12.4258]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.015  -0.285  -7.571]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -229.81458714103093, time: 55.113
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.87    -0.39    -9.1555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.92    -0.46   -10.8906]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -164.85033322549316, time: 54.928
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.44    -0.42    -8.7591]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.82    -0.235   -5.6326]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -139.8681104896169, time: 54.508
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.535  -0.045  -0.8836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.03   -0.15   -3.9984]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -130.40959842105954, time: 54.944
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.58   -0.035  -0.9412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.2    -0.085  -3.0863]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -124.25920553117359, time: 54.236
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.06   -0.12   -4.1165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.67   -0.205  -4.3519]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -112.68881828848279, time: 53.641
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.315   -0.425  -10.8831]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.735  -0.21   -4.2158]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -84.46231057337963, time: 53.459
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.45   -0.7   -18.597]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.72    -0.315   -9.6843]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -82.74374831405186, time: 53.274
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.625   -0.925  -22.5812]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.125   -0.48   -12.8933]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -69.70988155237579, time: 53.494
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.115  -0.905 -23.671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.695   -0.475  -12.9616]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -55.650003974780766, time: 53.418
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.06   -0.875 -23.463]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.18   -0.62  -16.206]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -70.44357352813788, time: 53.403
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.95    -0.915  -23.5158]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.595   -0.63   -15.9077]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -61.431548004290754, time: 53.082
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.365   -0.915  -24.3104]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.31    -0.71   -18.2481]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -60.995656905167216, time: 53.737
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.925   -0.94   -24.7014]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.225   -0.67   -19.4167]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -63.18691911815306, time: 53.756
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.87    -0.935  -24.6276]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.475   -0.77   -19.0938]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -68.55792839890512, time: 53.82
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.49    -0.94   -24.4685]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.395   -0.91   -21.9816]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -57.18482139082152, time: 54.05
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.99    -0.95   -24.7381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.32    -0.9    -21.5891]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -58.905515032425825, time: 53.682
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.015   -0.955  -24.8015]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.715   -0.875  -21.1561]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -53.25352515915083, time: 53.626
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.275   -0.96   -24.9821]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.12    -0.925  -21.3413]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -56.37069508217646, time: 53.066
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.19    -0.99   -24.0788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.16    -0.96   -20.5063]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -59.62865952353741, time: 53.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.69    -0.99   -24.1296]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.8     -0.965  -21.0382]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -53.23372282941874, time: 53.665
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.52    -0.975  -24.5015]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.305  -0.99  -20.983]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -52.939818356905164, time: 53.601
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.265   -0.98   -24.9662]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.035   -0.97   -20.7732]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -55.75009782974148, time: 53.244
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.67    -0.965  -24.5939]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.27    -0.985  -20.6043]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -49.85005605166018, time: 52.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.76    -0.98   -24.5905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.465   -1.     -20.6888]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -55.19446660458947, time: 53.774
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.235   -0.98   -24.3047]
agent1_energy_min, agent1_energy_max, agent1_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-3__2018-07-12_10-36-44...
200 50
steps: 9950, episodes: 200, mean episode reward: -180.2487490835481, time: 36.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.48743719  -0.48241206 -12.11326633]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.15075377  -0.49748744 -12.44693467]
400 50
steps: 19950, episodes: 400, mean episode reward: -174.5089367460527, time: 37.981
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.41    -0.54   -12.0926]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.875   -0.5    -12.1742]
600 50
steps: 29950, episodes: 600, mean episode reward: -183.77600826681083, time: 37.728
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.23    -0.535  -11.8489]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.59    -0.52   -12.1523]
800 50
steps: 39950, episodes: 800, mean episode reward: -185.07872735408867, time: 38.206
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.615   -0.475  -11.9834]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.47    -0.515  -12.0808]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -172.36606643443594, time: 38.035
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.52    -0.505  -12.1132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.535   -0.505  -12.1346]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -622.9780698111415, time: 53.13
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.625   -0.415   -9.4653]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.13    -0.52   -12.4401]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -286.9175233955538, time: 54.158
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.535   -0.09    -6.5271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.625   -0.95   -23.2636]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -188.29210532452296, time: 54.999
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.72    -0.385  -11.8415]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.14    -0.645  -12.7661]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -306.185855632572, time: 54.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.04    -0.305  -10.3212]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.595   -0.725  -15.3475]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -276.9285235398943, time: 55.572
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.985   -0.4    -13.5531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.3     -0.23    -6.1239]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -179.25490680377214, time: 55.275
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.165  -0.07   -5.541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.26   -0.29   -8.416]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -182.4073485039856, time: 53.813
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.43    -0.145   -5.2475]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.205   -0.44   -13.8142]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -159.40289867723084, time: 53.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.685   -0.095   -5.4888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.71    -0.265  -12.1791]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -132.59480177788305, time: 53.933
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.435   -0.21    -7.8388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.45    -0.245  -10.4723]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -147.46407472094612, time: 54.189
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.68    -0.24    -9.1668]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.135   -0.325  -12.6018]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -129.08616989713946, time: 54.016
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.135   -0.32   -11.8825]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.155  -0.305 -12.682]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -139.97021064613463, time: 54.092
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.845   -0.31   -10.8119]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.13    -0.295  -10.0879]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -124.86557932189098, time: 54.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.955   -0.385  -11.7008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.135   -0.28    -9.6475]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -133.33178032771102, time: 54.122
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.585   -0.43   -13.0282]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.59  -0.3  -11.14]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -132.03268619011573, time: 54.047
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.275   -0.56   -14.9401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.115   -0.3    -10.7762]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -109.27308284733019, time: 54.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.825   -0.565  -17.4152]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.835   -0.445  -14.2323]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -101.61586149276891, time: 54.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.045   -0.51   -15.5116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.39    -0.335  -12.7987]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -107.48591613700637, time: 53.027
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.375   -0.695  -20.1694]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.47    -0.36   -10.7429]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -107.94188119900477, time: 53.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.71    -0.54   -19.5057]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.945   -0.345   -8.9535]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -106.85217726782999, time: 53.197
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.71    -0.585  -18.4248]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.435   -0.4     -9.4524]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -96.99209643556155, time: 53.785
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.05    -0.605  -19.8277]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.47    -0.38   -11.0569]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -102.63675563591514, time: 53.849
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.765  -0.59  -20.294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.58    -0.405  -10.6023]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -96.08739897077079, time: 53.408
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.455   -0.665  -21.5148]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.195   -0.415  -11.9356]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -91.01575902200861, time: 52.821
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.755   -0.645  -22.2173]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.7     -0.39   -10.7312]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -99.601604268445, time: 53.571
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.615   -0.57   -20.8733]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.48    -0.405  -10.6715]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -91.75236336264079, time: 54.003
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.765   -0.655  -21.0648]
agent1_energy_min, agent1_energy_max, agent1_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-4__2018-07-12_10-36-47...
200 50
steps: 9950, episodes: 200, mean episode reward: -193.37965269066362, time: 37.028
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.93969849  -0.44723618 -11.40090452]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.89949749  -0.42713568 -10.95226131]
400 50
steps: 19950, episodes: 400, mean episode reward: -191.09724081092804, time: 37.449
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.685   -0.535  -11.7662]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.92    -0.43   -10.8555]
600 50
steps: 29950, episodes: 600, mean episode reward: -184.46290440465467, time: 37.673
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.215   -0.46   -11.5268]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.98   -0.455 -10.921]
800 50
steps: 39950, episodes: 800, mean episode reward: -173.20694573520427, time: 37.142
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.405   -0.49   -11.5292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.76   -0.45  -10.864]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -181.7391879907427, time: 37.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.37    -0.48   -11.6537]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.01    -0.37   -11.0603]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -588.19687200301, time: 53.492
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.3     -0.415  -10.5371]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.595   -0.42    -9.6766]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -421.39049119632057, time: 54.657
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.505   -0.62   -15.6477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.66    -0.315   -8.2043]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -156.38667836383178, time: 54.886
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.81    -0.425  -12.4681]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.29    -0.31    -8.5781]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -131.60667065570829, time: 53.832
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.68    -0.265   -9.9353]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.83    -0.405  -10.7035]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -138.55685582068404, time: 54.797
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.99    -0.28   -14.1234]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.11    -0.295   -9.1173]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -122.96838918577701, time: 54.659
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.02    -0.27   -13.2935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.51    -0.175   -7.1681]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -109.49226534297185, time: 54.115
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.67   -0.405 -15.488]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.065  -0.02   -1.5676]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -98.50565385299394, time: 53.157
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.595   -0.5    -18.7156]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.81   -0.04   -1.3597]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -112.07959384518519, time: 53.833
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.325  -0.595 -20.985]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.825  -0.035  -0.7665]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -97.30118064931065, time: 54.468
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.325   -0.585  -21.5072]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.755  -0.115  -1.7387]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -99.7255321445207, time: 54.674
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.825   -0.7    -23.3408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.72   -0.09   -1.5625]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -91.3190961578537, time: 53.703
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.485   -0.835  -22.6971]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.63   -0.05   -0.9776]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -106.11087517477837, time: 53.666
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.015   -0.84   -22.0614]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.685  -0.05   -1.0126]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -99.83062775086782, time: 54.709
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.46    -0.815  -22.9547]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.095 -0.04  -1.153]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -99.02171508073364, time: 54.118
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.91    -0.885  -23.4032]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.27   -0.03   -0.5575]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -102.2506972699679, time: 53.734
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.38    -0.81   -22.9811]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.52   -0.015  -0.1737]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -97.71236128385978, time: 54.472
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.265   -0.86   -22.4169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.73   -0.05   -0.6494]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -90.86622722950915, time: 53.605
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.355   -0.865  -22.9973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.92   -0.075  -2.6777]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -93.34077226234177, time: 53.926
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.585   -0.89   -22.6807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.67   -0.13   -3.0602]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -101.49117809501193, time: 53.545
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.22    -0.81   -22.3993]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.545  -0.055  -2.2926]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -89.88861049188601, time: 54.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.145   -0.86   -23.2925]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.71   -0.08   -2.5525]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -80.38635294847002, time: 53.777
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.98    -0.765  -23.7001]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.555  -0.12   -3.3677]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -91.1455284325418, time: 54.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.68   -0.67  -23.331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.645   -0.64   -16.1967]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -80.54228881425627, time: 53.526
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.53   -0.755 -23.714]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.25    -0.925  -23.4739]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -77.09287100299062, time: 54.001
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.925   -0.785  -23.4237]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.355   -0.52   -15.3632]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -84.72613113695101, time: 53.116
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.27    -0.86   -24.2133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.325   -0.82   -23.0677]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-2__2018-07-12_10-36-37...
200 50
steps: 9950, episodes: 200, mean episode reward: -169.53330769675995, time: 35.886
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.24623116  -0.51256281 -12.02472362]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.95477387  -0.52261307 -13.1440201 ]
400 50
steps: 19950, episodes: 400, mean episode reward: -169.97237227316649, time: 38.233
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.635   -0.49   -12.2364]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.175   -0.515  -13.3059]
600 50
steps: 29950, episodes: 600, mean episode reward: -164.9103819824741, time: 37.146
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.445   -0.475  -11.9893]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.16    -0.525  -13.3074]
800 50
steps: 39950, episodes: 800, mean episode reward: -168.5709551534574, time: 38.148
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.56    -0.48   -12.1652]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.235   -0.53   -13.3539]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -165.43088352095327, time: 37.569
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.075   -0.39   -11.7321]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.21    -0.425  -13.3012]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -319.3323333427546, time: 52.631
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.41    -0.625  -14.7485]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.295   -0.605  -16.2396]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -138.80048599878444, time: 54.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.085   -0.715  -14.6026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.785   -0.425  -14.3824]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -139.43480393493417, time: 54.167
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.54    -0.705  -15.9385]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.59   -0.22   -4.1727]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -119.5703443767838, time: 54.04
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.65    -0.69   -13.6541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.125  -0.06   -0.7329]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -105.08987388298311, time: 54.084
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.69    -0.66   -13.8476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.665  -0.055  -0.4034]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -100.89564614579089, time: 54.53
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.01    -0.825  -19.1572]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.345  -0.025  -0.2298]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -101.67397545484747, time: 53.059
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.95    -0.87   -19.4292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.2    -0.095  -0.7244]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -97.89798624771075, time: 53.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.69    -0.925  -19.3849]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.13   -0.12   -1.0839]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -94.72979345343774, time: 53.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.56    -0.915  -18.0924]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.43   -0.18   -1.8438]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -85.66506647987349, time: 53.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.405  -0.835 -19.125]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.04   -0.06   -0.4756]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -83.1130787266306, time: 53.869
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.26    -0.925  -20.8252]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.26   -0.09   -0.6462]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -75.24905042930243, time: 53.899
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.065   -0.955  -18.1906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.2   -0.195 -1.71 ]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -76.62861301918502, time: 52.898
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.715   -0.99   -22.4434]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.165  -0.15   -1.2107]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -85.217389903033, time: 53.804
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.69    -0.995  -23.3417]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.205  -0.14   -1.1977]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -71.15921484347778, time: 53.66
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.975   -1.     -22.9439]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.115 -0.095 -0.677]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -73.27444964727168, time: 53.936
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.405   -1.     -23.1153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.45   -0.165  -1.0236]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -72.54311946889304, time: 53.836
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.63    -0.995  -23.3281]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.625  -0.3    -2.1062]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -69.44656950667643, time: 53.717
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.665   -1.     -24.2264]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.945  -0.27   -1.8013]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -74.93396460283888, time: 51.961
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.935   -1.     -23.8057]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.365  -0.23   -1.1023]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -72.2569139982683, time: 51.755
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.13    -0.995  -23.8623]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.71   -0.19   -0.5847]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -63.664059142169165, time: 52.197
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.785   -0.995  -24.1287]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.04   -0.355  -1.6451]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -62.14375352342863, time: 51.358
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.025   -1.     -23.1618]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.58  -0.34  -1.795]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -68.67885417979792, time: 51.755
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.885   -1.     -24.2044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.515  -0.315  -1.2551]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -68.70488815793107, time: 51.471
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.03    -0.995  -23.2703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.62   -0.325  -1.3275]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -58.22252888617589, time: 51.177
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.98    -1.     -24.1266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.93  -0.36  -1.583]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -53.732793590647596, time: 51.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.03    -1.     -23.0359]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.665  -0.395  -2.0655]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -71.27155412375122, time: 50.571Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-2__2018-07-12_10-36-33...
200 50
steps: 9950, episodes: 200, mean episode reward: -171.3570585216704, time: 34.388
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.01005025  -0.50251256 -13.70502513]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.33165829  -0.55778894 -14.89577889]
400 50
steps: 19950, episodes: 400, mean episode reward: -173.28198446946263, time: 37.821
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.475   -0.57   -13.5082]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.34    -0.54   -14.7522]
600 50
steps: 29950, episodes: 600, mean episode reward: -179.93025610829113, time: 37.971
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.555   -0.48   -13.4841]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.35    -0.575  -14.7563]
800 50
steps: 39950, episodes: 800, mean episode reward: -169.41174250998475, time: 38.535
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.92    -0.48   -13.7084]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.255   -0.615  -14.6366]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -175.2670628520049, time: 38.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.865   -0.505  -13.6413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.555   -0.525  -14.8692]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -482.0062954969227, time: 53.365
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.43   -0.37  -10.292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.22    -0.56   -12.1481]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -240.29388797106293, time: 55.362
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.115   -0.31    -8.8242]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.795  -0.53   -9.319]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -137.91117107865725, time: 54.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.755   -0.275   -7.8969]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.7     -0.68   -14.3357]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -128.69736448136678, time: 55.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.03    -0.625  -16.2618]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.2     -0.34   -11.5273]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -135.9715630618483, time: 54.163
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.52    -0.39   -10.2106]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.64    -0.255   -7.3017]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -125.65956261992818, time: 54.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.235  -0.25   -3.4677]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.145  -0.075  -3.1259]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -131.55179040859366, time: 51.843
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.425   -0.375   -6.6754]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.045   -0.24    -6.5335]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -120.08987050835673, time: 52.553
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.695  -0.23   -2.4734]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.08   -0.165  -3.6802]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -133.64509804402343, time: 52.366
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.395 -0.25  -2.021]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.57   -0.155  -1.5604]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -131.2259651602577, time: 52.304
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.645  -0.185  -1.8937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.63   -0.185  -1.8403]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -113.61513693534809, time: 53.27
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.395  -0.235  -1.3593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.97   -0.405  -9.789]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -85.05539453101532, time: 53.103
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.9    -0.155  -1.0962]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.635  -0.49  -14.792]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -93.50651154183099, time: 51.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.04   -0.165  -1.3648]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.045   -0.655  -18.9683]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -96.2951128963736, time: 53.236
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.885 -0.085 -0.868]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.24    -0.63   -19.9127]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -86.98794758476768, time: 53.241
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.025  -0.13   -2.0478]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.48   -0.6   -20.868]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -92.95276962147236, time: 53.189
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.895  -0.21   -2.0838]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.225   -0.63   -20.9318]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -90.21905751725139, time: 52.106
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.675  -0.295  -2.4914]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.21    -0.59   -20.1516]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -94.71196985465816, time: 52.872
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.485  -0.305  -2.5775]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.49    -0.59   -21.1124]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -92.06320756587483, time: 52.491
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.025  -0.225  -1.8758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.22    -0.585  -21.9184]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -91.04030071822906, time: 53.584
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.71   -0.05   -0.6634]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.21    -0.59   -22.1483]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -86.7812601540282, time: 53.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.855  -0.07   -0.7592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.4     -0.69   -22.8648]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -84.95184970808747, time: 53.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.44   -0.075  -1.1034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.255   -0.675  -22.8205]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -84.82516482103696, time: 53.431
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.955  -0.065  -0.7566]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.39    -0.695  -23.4094]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -83.41607703283832, time: 53.015
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.2   -0.245 -3.672]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.13    -0.69   -23.2766]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -86.56916853263623, time: 53.228
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.785   -0.235   -4.5981]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.      -0.61   -21.8444]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -80.09218645250552, time: 54.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.955  -0.175  -4.3589]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.4     -0.7    -22.9937]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -77.07918177212247, time: 52.653Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-3__2018-07-12_10-36-39...
200 50
steps: 9950, episodes: 200, mean episode reward: -188.41318425656274, time: 36.258
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.46231156  -0.48241206 -14.31386935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.8040201   -0.40703518 -11.15125628]
400 50
steps: 19950, episodes: 400, mean episode reward: -182.40680545930357, time: 37.955
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.755   -0.555  -13.9778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.355   -0.445  -10.9695]
600 50
steps: 29950, episodes: 600, mean episode reward: -172.8220463084447, time: 37.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.68    -0.48   -14.0538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.705   -0.425  -11.1583]
800 50
steps: 39950, episodes: 800, mean episode reward: -178.35408919760877, time: 38.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.745   -0.535  -13.9639]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.395   -0.43   -11.0137]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -172.71029619793626, time: 37.312
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.315   -0.48   -13.7236]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.32    -0.465  -10.9456]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -552.1489406314396, time: 51.927
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.105   -0.555  -14.7703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.085   -0.415   -9.7655]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -219.02933696515944, time: 54.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.69   -0.71  -13.013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.925   -0.515   -9.2502]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -677.4306942524845, time: 55.404
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.345  -0.63  -10.72 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.28    -0.355  -13.0344]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -237.39996429246057, time: 54.123
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.965   -0.63   -14.5646]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.26   -0.29   -9.694]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -143.68836143164324, time: 54.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.045   -0.315   -7.3087]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.63    -0.26    -8.5078]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -138.85901085299915, time: 54.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.38   -0.465  -8.733]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.355  -0.28   -8.913]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -135.62227111286697, time: 53.876
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.39    -0.32   -10.1198]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.59  -0.075 -1.4  ]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -119.39129706657643, time: 54.171
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.43    -0.485  -12.5629]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.45   -0.045  -0.3405]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -117.93515937707176, time: 53.519
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.865   -0.505  -13.5436]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.86   -0.06   -0.4779]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -109.72176468103815, time: 54.432
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.26   -0.665 -15.785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.39   -0.055  -0.7481]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -108.60127314466114, time: 55.147
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.785   -0.67   -19.5761]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.205  -0.02   -0.6049]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -116.31296877102956, time: 53.503
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.46    -0.53   -16.2477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.805 -0.04  -0.813]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -98.69085242024859, time: 53.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.245   -0.54   -17.3046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.065  -0.045  -1.0261]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -99.60385052900828, time: 53.792
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.68    -0.49   -15.0613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.33   -0.165  -3.1493]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -99.72882111888077, time: 53.27
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.185  -0.535 -16.836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.16  -0.155 -1.825]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -95.31329484441565, time: 54.098
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.47    -0.565  -14.5545]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.335  -0.08   -0.8081]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -89.65422689266046, time: 53.979
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.99    -0.49   -14.6172]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.205  -0.07   -1.1515]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -90.30420179983776, time: 53.04
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.095   -0.555  -15.9106]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.505  -0.16   -3.7736]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -98.71041766347895, time: 53.869
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.105   -0.58   -17.1664]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.385  -0.065  -1.1354]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -82.67777979500332, time: 53.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.325   -0.62   -14.5176]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.86   -0.095  -2.7639]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -73.92277790768762, time: 53.878
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.495   -0.615  -15.3739]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.985  -0.16   -2.9433]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -83.60932827824894, time: 54.138
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.33    -0.715  -16.1481]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.295   -0.185   -4.9485]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -89.68048586194749, time: 53.65
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.375   -0.61   -14.7126]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.59   -0.18   -4.3436]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -86.90951813738731, time: 53.033
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.99    -0.64   -15.4611]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.965  -0.205  -4.9416]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -82.56543198572689, time: 53.626
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.885   -0.655  -14.8061]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.445  -0.155  -3.4117]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -89.50069029050628, time: 53.529
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.2     -0.73   -15.5139]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.34  -0.22  -4.709]
6400 Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-3__2018-07-12_10-36-41...
200 50
steps: 9950, episodes: 200, mean episode reward: -167.06207509238894, time: 36.861
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.60301508  -0.48241206 -12.52763819]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.89949749  -0.56281407 -14.09326633]
400 50
steps: 19950, episodes: 400, mean episode reward: -155.21519157650553, time: 38.064
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.075   -0.49   -12.5138]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.18   -0.595 -14.347]
600 50
steps: 29950, episodes: 600, mean episode reward: -165.97062375499692, time: 38.085
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.73    -0.495  -12.7248]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.9     -0.54   -14.1238]
800 50
steps: 39950, episodes: 800, mean episode reward: -167.05493817156886, time: 37.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.705   -0.515  -12.7336]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.665   -0.555  -14.5469]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -178.6565126957495, time: 37.479
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.      -0.485  -12.8708]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.8     -0.565  -14.0867]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -365.2362841959033, time: 52.752
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.185   -0.42   -10.6827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.815   -0.53   -11.7163]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -194.98793894759675, time: 54.602
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.3     -0.645  -16.6707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.195   -0.415  -10.1542]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -132.26183274813468, time: 53.989
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.345   -0.34    -7.0531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.63    -0.31   -12.3355]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -91.19772352780626, time: 54.92
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.37   -0.075  -1.2069]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.93    -0.46   -15.1269]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -97.54915850992529, time: 53.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.37   -0.03   -0.2101]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.31    -0.395  -11.0067]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -109.8620391582419, time: 55.046
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.095  -0.04   -0.0784]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.575   -0.34   -11.0697]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -102.47179628330363, time: 53.016
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.19   -0.045  -0.1598]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.91    -0.465  -14.6618]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -102.85224291652929, time: 53.962
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.335  -0.08   -0.2933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.925   -0.465  -12.6681]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -97.61743269893857, time: 53.694
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.48   -0.075  -0.4319]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.29    -0.54   -16.0378]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -98.8455514905714, time: 54.255
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.1    -0.125  -0.9474]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.505   -0.7    -17.3811]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -101.00051659004191, time: 54.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.73   -0.205  -1.4083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.51   -0.625 -17.464]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -91.0839479450253, time: 54.124
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.95 -0.19 -0.87]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.415   -0.725  -20.9584]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -100.71362493754371, time: 53.185
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.485  -0.24   -1.2151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.63    -0.67   -19.6228]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -80.92103674998921, time: 53.941
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.61   -0.28   -1.2643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.88    -0.715  -19.1679]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -83.51684342668057, time: 53.604
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.79   -0.295  -1.4387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.08    -0.62   -17.8209]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -74.63217949476571, time: 53.97
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.605 -0.405 -2.086]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.355   -0.625  -20.1763]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -67.48265807486186, time: 54.116
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.4    -0.325  -1.6898]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.325   -0.69   -20.9622]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -70.61712019115542, time: 53.432
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.925  -0.375  -2.7564]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.525   -0.775  -23.2633]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -68.40737958101006, time: 53.613
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.96   -0.37   -1.6204]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.54    -0.745  -22.8516]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -68.70359720706475, time: 54.285
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.975  -0.445  -2.6911]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.55    -0.7    -22.0679]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -65.56156724292461, time: 54.723
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.795  -0.4    -2.4519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.345   -0.675  -18.2271]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -88.80127613841437, time: 53.523
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.575 -0.435 -2.457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.665  -0.79  -19.519]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -67.07322191874914, time: 54.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.49   -0.49   -4.5093]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.07    -0.64   -19.8783]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -72.18303887406239, time: 54.058
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.48   -0.37   -3.1608]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.385   -0.705  -16.4367]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -63.75086776531485, time: 54.309
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.115  -0.42   -2.8195]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.15   -0.66  -15.481]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -70.17392258973254, time: 54.279
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.575  -0.49   -4.2959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.42    -0.595  -15.5806]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -71.60958986050284, time: 53.242
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.045   -1.     -23.6608]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.31   -0.45   -2.2186]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -71.3321874068975, time: 50.738
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.995   -1.     -23.6542]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.755  -0.485  -2.5853]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -69.23337799320177, time: 51.048
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.435   -1.     -23.4716]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.155  -0.425  -2.1591]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -67.60388378544809, time: 51.142
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.09    -1.     -24.2781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.735  -0.48   -2.7033]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -78.51981943103047, time: 52.267
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.375  -0.75  -17.729]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.74   -0.49   -3.2624]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -65.34444509616188, time: 51.722
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.615   -0.96   -20.7856]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.325  -0.55   -2.8046]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -66.07707223315887, time: 51.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.995   -0.995  -21.8822]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.115 -0.5   -1.723]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -63.39488589773093, time: 51.874
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.97    -0.99   -20.3371]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.655 -0.46  -2.041]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -56.63982610830985, time: 52.486
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.925   -0.995  -18.6009]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.705 -0.575 -2.461]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -67.9045171485504, time: 52.49
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.75   -0.995 -18.476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.05   -0.48   -2.0878]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -61.47384402467316, time: 52.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.56    -0.985  -17.6847]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.83   -0.53   -3.4838]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -58.70903643912034, time: 51.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.89    -0.98   -20.0673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.655  -0.66   -4.1227]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -56.944714108657905, time: 51.594
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.395   -1.     -19.1496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.595  -0.605  -3.8118]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -56.14110213893396, time: 50.542
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.56    -1.     -19.6813]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.115  -0.675  -4.4372]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -61.63392670869585, time: 52.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.59    -0.995  -17.3149]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.02   -0.54   -4.0587]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -54.0683920369232, time: 50.703
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.76    -0.93   -14.1581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.835  -0.65   -5.3517]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -55.28168458776625, time: 51.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.55    -0.99   -15.1029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.03   -0.63   -4.3795]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -58.672906731534724, time: 50.596
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.685   -1.     -15.2795]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.865  -0.64   -4.6037]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -55.924618066413444, time: 51.426
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.1     -1.     -13.7151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.67   -0.755  -5.0454]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -47.61256712781553, time: 54.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.58    -1.     -14.5453]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.2    -0.73   -5.0752]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -52.33806988927297, time: 50.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.785  -1.    -14.645]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.72  -0.695 -5.239]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -49.928514587654156, time: 50.599
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.105   -1.     -12.7831]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.985   -0.87    -8.3216]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -50.54599041969312, time: 51.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.71    -1.     -14.1193]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.135   -0.815   -7.3492]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -56.422051598890064, time: 50.481
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.31    -0.995  -14.1275]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.68   -0.775  -6.4695]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -52.41632935536465, time: 51.441
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.93    -1.     -15.4747]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.66    -0.82    -7.9545]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -48.27713048001286, time: 50.482
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.26    -1.     -14.7638]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.78    -0.835   -8.4403]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -52.104945273774476, time: 50.596
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.05    -1.     -14.4027]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.985   -0.73    -8.0326]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -47.71832594436366, time: 51.054
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.94    -1.     -18.4469]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.15    -0.81    -8.9273]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -53.36215348406044, time: 50.786
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.39    -0.99   -15.2783]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.45    -0.785   -8.7476]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -51.043915757189914, time: 51.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.2     -1.     -14.4454]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.165   -0.81   -10.6374]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -44.860882092128634, time: 51.012
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.62    -0.975  -13.4207]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.26    -0.835   -9.3272]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -58.08037415909894, time: 51.329
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.17    -0.99   -14.5751]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.65    -0.795  -23.8518]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -75.39778901501448, time: 51.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.48    -0.68   -22.2074]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.85    -0.88   -24.1109]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -77.99396198015374, time: 52.308
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.32   -0.65  -22.762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.68    -0.795  -24.0065]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -63.470882877521035, time: 51.615
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.5     -0.64   -22.6609]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.165   -0.815  -24.1434]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -73.53849473269493, time: 51.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.04    -0.735  -23.2439]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.505   -0.825  -23.7971]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -72.3808727262164, time: 52.071
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.13    -0.69   -22.5653]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.38    -0.815  -23.6491]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -63.142537845350226, time: 52.059
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.66    -0.725  -23.5573]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.255   -0.855  -24.2265]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -67.64204545702687, time: 52.226
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.51    -0.725  -23.4761]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.435   -0.875  -24.4395]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -59.81315317840537, time: 51.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.94   -0.76  -23.781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.64    -0.88   -24.4847]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -72.40471780529055, time: 52.951
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.94    -0.83   -24.6192]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.075   -0.86   -24.1072]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -61.26391386572473, time: 52.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.75   -0.815 -24.467]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.82    -0.865  -24.6047]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -60.723265643242264, time: 52.051
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.455   -0.77   -24.2133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.255  -0.88  -24.888]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -66.00343756246762, time: 50.918
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.48    -0.785  -24.2151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.58    -0.89   -24.4811]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -63.663185067480555, time: 51.748
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.82    -0.785  -24.4311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.07    -0.885  -24.7733]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -57.871134343301385, time: 51.669
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.98    -0.82   -24.6286]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.99    -0.865  -24.7118]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -67.80769553589008, time: 52.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.78    -0.775  -24.4356]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.19    -0.755  -24.1348]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -64.78614586784053, time: 51.829
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.81    -0.745  -24.4285]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.1     -0.71   -23.9643]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -60.2045499568701, time: 51.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.165   -0.825  -24.7615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.06    -0.88   -24.7563]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -62.72459327879838, time: 52.162
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.015   -0.84   -24.7041]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.44    -0.825  -24.3113]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -78.80788650616873, time: 52.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.405   -0.93   -25.0557]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.775   -0.835  -23.9083]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -69.10749457790922, time: 52.138
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.255   -0.945  -24.9782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.3     -0.83   -24.2063]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -66.12004196487572, time: 51.599
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.64    -0.985  -25.2116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.73    -0.885  -24.4871]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -58.57573829716348, time: 51.695
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.425  -0.975 -25.058]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.105   -0.905  -24.8117]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -61.579940691800864, time: 51.919
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.31    -0.945  -25.0168]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.225   -0.935  -24.9326]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -59.68714869312223, time: 51.441
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.745   -0.975  -25.2997]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.39    -0.93   -25.0175]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -56.73746155786094, time: 52.267
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.72    -0.975  -25.2756]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.745   -0.935  -25.2958]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -59.12473629785808, time: 50.741
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.755   -0.995  -25.3356]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.5     -0.925  -25.0858]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -61.776467256909946, time: 50.69
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.46   -0.99  -25.151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.29    -0.92   -24.8937]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -62.1040119852297, time: 51.862
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.205   -0.99   -24.9629]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.045  -0.88  -24.677]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -73.84416170920433, time: 52.057
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.075   -0.99   -24.8993]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.15    -0.84   -23.1147]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -70.10521165029654, time: 52.194
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.815   -0.995  -25.0201]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.16    -0.875  -23.9581]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -67.72302204088736, time: 51.487
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.325   -1.     -24.7466]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.395   -0.81   -24.1055]
12600
6400 50
steps: 319950, episodes: 6400, mean episode reward: -86.49745292515472, time: 52.732
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.305   -0.93   -23.4823]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.73    -0.55   -18.2679]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -75.72486161662042, time: 52.044
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.055   -0.91   -23.6491]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.27    -0.54   -19.0905]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -69.1646851592614, time: 52.59
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.5     -0.94   -24.0284]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.18    -0.625  -20.2216]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -80.49447463467546, time: 52.785
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.51    -0.905  -22.9331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.65    -0.605  -19.3542]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -70.23003231261092, time: 52.488
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.11    -0.875  -23.5388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.635   -0.555  -19.8697]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -83.63287855820012, time: 52.241
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.59    -0.875  -23.7018]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.01   -0.59  -19.995]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -70.45429883967748, time: 53.08
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.26    -0.88   -23.6722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.87    -0.6    -19.7676]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -69.10523619971569, time: 52.623
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.715   -0.885  -23.2741]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.46    -0.615  -21.2717]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -62.95348752877594, time: 53.876
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.795   -0.91   -24.0004]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.64    -0.675  -22.3701]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -62.18443500021962, time: 53.853
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.97   -0.955 -24.202]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.62    -0.635  -19.9457]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -63.268956703920175, time: 53.133
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.41    -0.925  -24.3934]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.21    -0.63   -20.2269]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -55.69441846969467, time: 53.226
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.14    -0.95   -24.2696]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.335  -0.8   -21.104]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -64.5507864162033, time: 52.553
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.61    -0.945  -23.9254]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.24    -0.77   -21.8086]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -57.380096123800605, time: 52.205
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.885   -0.955  -24.0543]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.955   -0.745  -21.9761]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -58.038717111147456, time: 53.245
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.235   -0.975  -24.4429]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.525   -0.81   -21.9539]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -62.45809306765218, time: 52.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.105   -0.965  -23.8466]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.245  -0.745 -21.617]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -65.47691274827243, time: 52.393
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.35    -0.98   -23.3838]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.735   -0.78   -21.4399]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -62.898836818135685, time: 52.671
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.705   -0.95   -24.2291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.99    -0.68   -20.3059]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -67.49510691489934, time: 52.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.385   -0.99   -24.6748]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.66    -0.685  -21.9342]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -58.98967662588555, time: 52.354
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.785   -1.     -24.7816]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.665   -0.635  -19.8916]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -58.934834396051244, time: 52.284
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.62    -0.975  -24.6298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.455   -0.64   -20.9695]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -57.652330898318695, time: 52.47
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.535   -1.     -24.6146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.615   -0.77   -21.9488]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -62.67588096013347, time: 52.404
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.795   -0.98   -24.7848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.285   -0.745  -20.4874]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -62.46483787472078, time: 51.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.745  -0.985 -24.229]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.36    -0.775  -21.1813]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -58.31107003832313, time: 53.221
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.04    -0.995  -24.9241]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.445   -0.66   -20.3212]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -55.11719781237061, time: 51.922
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.2     -0.975  -24.3621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.21    -0.69   -21.9587]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -56.531063579267155, time: 52.199
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.85    -0.99   -24.8387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.66    -0.965  -24.2275]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -48.98762104495577, time: 51.65
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.85    -0.955  -24.7142]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.49    -0.69   -22.1898]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -45.88622865142892, time: 51.47
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.72   -0.97  -24.696]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.605   -0.725  -22.3575]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -47.14861130907009, time: 52.459
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.8     -0.94   -24.6956]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.64    -0.83   -22.9756]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -47.867112502373985, time: 51.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.965   -0.945  -24.7608]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.485   -0.94   -23.6545]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -48.99590640779448, time: 52.2550
steps: 319950, episodes: 6400, mean episode reward: -77.77188102313222, time: 52.094
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.135   -0.665  -15.7887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.71   -0.275  -5.3286]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -76.40408586210837, time: 53.326
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.97    -0.79   -16.6752]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.7    -0.115  -4.8128]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -77.86616014209599, time: 51.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.14    -0.78   -17.0426]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.805   -0.15    -5.0451]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -78.28890073172359, time: 51.345
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.725   -0.75   -18.7592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.585  -0.135  -3.9176]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -77.90521026473935, time: 52.046
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.255   -0.71   -20.7843]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.515   -0.2     -5.7042]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -67.8352022241871, time: 50.969
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.52    -0.805  -20.4937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.74   -0.105  -3.9303]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -69.26041564629254, time: 52.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.47    -0.88   -20.9451]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.38    -0.235   -6.0373]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -69.51002288066854, time: 51.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.635   -0.905  -20.9918]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.65    -0.34    -6.2275]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -77.12231836819954, time: 52.026
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.355   -0.895  -19.0845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.92   -0.19   -3.3237]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -78.45217046001397, time: 51.878
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.56    -0.83   -19.8641]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.04    -0.36    -6.1987]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -79.79056440505774, time: 51.843
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.08    -0.89   -20.8845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.56    -0.325   -5.9742]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -81.37497610998916, time: 50.763
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.77    -0.845  -20.3938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.19   -0.16   -3.1081]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -68.32037939468688, time: 52.187
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.995   -0.845  -19.2613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.28   -0.135  -3.4176]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -70.61651463966659, time: 54.06
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.99   -0.95  -21.469]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.555  -0.315  -5.5543]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -76.71868161902049, time: 50.69
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.185   -0.84   -20.3473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.545 -0.28  -4.654]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -69.52123402432674, time: 51.879
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.02    -0.94   -20.3071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.735  -0.265  -3.2587]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -69.13708574233011, time: 50.941
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.025   -0.875  -17.7165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.09   -0.11   -2.1082]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -68.88664757518224, time: 51.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.09    -0.955  -18.8555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.275 -0.27  -2.988]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -63.74315378407517, time: 51.61
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.47    -0.86   -17.4936]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.565  -0.365  -3.3201]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -67.92965154320305, time: 52.425
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.755   -0.9    -16.1641]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.365  -0.25   -2.5611]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -64.78731979685054, time: 50.951
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.475   -0.855  -15.6297]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.635 -0.325 -3.414]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -67.853996337336, time: 51.639
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.665   -0.96   -19.8088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.435  -0.31   -2.6349]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -77.25849059631534, time: 51.166
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.605   -0.92   -18.8806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.005  -0.37   -3.0612]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -63.07583643363102, time: 51.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.755   -0.935  -17.4998]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.86   -0.375  -4.1357]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -63.75744761422224, time: 51.258
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.73    -0.9    -15.8744]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.01   -0.41   -3.7927]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -63.7035872580463, time: 51.279
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.645   -0.92   -16.3025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.37   -0.475  -4.4806]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -67.83861642013855, time: 51.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.905   -0.98   -18.3539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.935  -0.35   -2.9366]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -72.56193578230979, time: 50.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.535   -0.99   -15.1082]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.305  -0.385  -3.6631]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -70.33053228578233, time: 50.966
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.395   -0.975  -16.1447]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.24   -0.38   -4.6379]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -60.666156429172204, time: 51.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.535   -0.945  -12.8107]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.545  -0.315  -3.0592]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -65.30685769949295, time: 51.47
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.055   -0.955  -15.0078]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.145  -0.36   -5.862]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -66.60002521031117, time: 51.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.66    -0.97   -20.8844]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -53.026340556192146, time: 53.062
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.82    -0.98   -24.6922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.035  -0.9   -21.055]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -50.74279623082417, time: 53.979
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.825   -0.98   -24.6856]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.1     -0.91   -21.5874]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -50.84503116874254, time: 53.006
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.075   -0.985  -24.8548]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.81   -0.915 -21.753]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -49.473799119588314, time: 53.185
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.41    -0.995  -25.0671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.175   -0.94   -21.9483]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -53.173831544583685, time: 54.147
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.29    -0.975  -24.9714]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.915   -0.915  -21.7061]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -47.1277540284711, time: 53.726
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.98    -0.995  -24.8019]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.64    -0.885  -23.4594]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -54.753633255877006, time: 53.293
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.405   -0.995  -25.1108]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.655   -0.925  -22.3925]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -47.687630227703714, time: 54.968
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.47    -0.99   -25.1369]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.81    -0.89   -23.4424]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -45.62705140673811, time: 54.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.555   -0.99   -25.1894]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.015   -0.93   -23.8609]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -47.8726646158135, time: 53.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.97    -0.99   -24.7739]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.7     -0.915  -23.6084]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -47.07297090262213, time: 54.253
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.33    -1.     -25.0567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.62    -0.95   -23.8011]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -47.36035851402628, time: 53.376
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.255   -0.995  -24.9767]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.455   -0.94   -24.4888]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -43.37961332246131, time: 52.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.635   -0.995  -25.2463]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.335   -0.94   -24.0687]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -46.523580364381786, time: 53.547
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.45    -0.995  -25.1291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.205   -0.97   -24.5377]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -44.448814187696435, time: 53.17
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.295   -0.995  -25.0452]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.94    -0.995  -24.6133]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -47.93125590466581, time: 53.893
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.78    -0.95   -24.7299]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.09    -0.97   -24.1833]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -47.60659180955118, time: 53.302
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.3     -0.98   -23.9218]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.53    -0.975  -24.3773]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -54.293343635292565, time: 54.427
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.395   -0.985  -24.0195]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.525   -0.925  -23.5003]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -50.230229179167985, time: 52.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.56    -0.975  -23.7006]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.925   -0.97   -23.0396]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -48.25992895036675, time: 52.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.21    -0.98   -24.3619]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.93    -1.     -22.9254]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -45.62691906300219, time: 53.403
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.83    -1.     -24.5678]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.65    -0.995  -21.9735]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -52.29889444187204, time: 53.039
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.09    -0.995  -24.5521]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.41    -1.     -20.5711]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -47.403909624522576, time: 53.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.605  -0.975 -24.504]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.235   -0.975  -22.2848]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -47.286450835808374, time: 52.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.255   -0.95   -23.2181]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.655  -0.985 -22.116]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -45.33544329009009, time: 53.076
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.055   -0.98   -23.6225]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.885   -0.97   -20.8754]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -45.90812170725142, time: 53.011
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.165  -0.965 -23.088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.59    -0.98   -21.8655]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -49.029775255835865, time: 51.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.21   -0.99  -23.519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.285  -0.975 -19.07 ]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -48.67201672329227, time: 52.535
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.015   -0.995  -23.9434]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.305   -0.985  -21.7312]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -42.670129080363374, time: 52.41
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.03    -0.965  -23.0635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.055   -0.985  -19.8872]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -46.76884605612892, time: 52.927
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.595   -0.945  -23.1129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.06    -0.985  -22.6138]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -44.50180305536265, time: 52.21
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.38    -0.985  -22.4857]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.535   -0.985  -20.2413]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.49    -0.28    -8.7236]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.49    -0.705  -23.0249]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -68.37924353080331, time: 52.589
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.58    -0.34    -8.0658]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.825   -0.74   -23.0855]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -79.05260114608595, time: 52.711
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.9     -0.285   -8.1791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.28    -0.73   -23.0927]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -67.41026880288227, time: 52.575
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.35    -0.285   -8.5929]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.     -0.71  -23.282]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -79.47317116307316, time: 53.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.63    -0.315  -12.0296]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.45    -0.705  -23.5107]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -70.17342062642444, time: 52.471
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.595   -0.39   -11.4823]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.83   -0.735 -23.753]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -68.4668808635818, time: 53.462
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.075   -0.325  -13.2552]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.675   -0.79   -24.4643]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -78.96238576271027, time: 53.608
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.775   -0.335  -14.3974]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.41    -0.8    -24.2467]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -74.08819614787299, time: 53.336
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.405   -0.295  -13.7578]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.995   -0.79   -23.9391]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -73.10867715864819, time: 53.564
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.965   -0.445  -14.1192]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.605   -0.78   -24.3796]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -70.55486468409157, time: 52.818
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.95    -0.39   -14.8124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.46    -0.805  -24.2862]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -70.45643510104301, time: 52.396
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.13    -0.31   -13.2559]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.725   -0.785  -24.5061]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -72.52527814518076, time: 52.08
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.975   -0.32   -12.1234]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.38    -0.76   -24.3179]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -77.13941945023396, time: 52.457
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.445   -0.325  -16.0984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.36    -0.715  -24.2481]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -69.98355210276492, time: 53.532
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.665   -0.42   -16.5344]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.57    -0.68   -24.3199]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -65.5671477595127, time: 53.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.955   -0.29   -15.4421]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.56   -0.705 -24.276]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -74.74373118810496, time: 53.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.885   -0.405  -18.4621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.215   -0.81   -24.1396]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -71.2075790508165, time: 53.402
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.32    -0.44   -19.2762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.475   -0.76   -24.2786]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -64.23374683957798, time: 51.434
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.74    -0.495  -18.3268]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.29    -0.795  -24.8943]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -73.5844951632699, time: 53.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.895   -0.45   -18.5006]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.065   -0.805  -24.7478]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -69.01441095217487, time: 51.678
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.13    -0.41   -19.3402]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.245   -0.835  -24.9055]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -63.68406147499481, time: 52.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.2     -0.48   -18.8274]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.95    -0.84   -24.6579]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -63.140505694049516, time: 52.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.825   -0.505  -17.7388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.56    -0.855  -24.2158]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -61.294947098913546, time: 52.415
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.675   -0.455  -17.0605]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.875   -0.87   -24.6156]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -61.810766557273816, time: 53.134
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.275   -0.435  -14.4381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.66    -0.87   -24.5342]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -59.123948635533964, time: 52.752
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.575   -0.375  -15.3247]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.9     -0.9    -24.7349]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -58.79661977694243, time: 52.371
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.755   -0.495  -19.3442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.66    -0.915  -25.2159]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -65.4034435364818, time: 52.183
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.505  -0.495 -19.317]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.325  -0.845 -24.938]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -59.25622325785002, time: 51.674
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.675   -0.435  -19.2639]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.62    -0.915  -25.1785]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -57.8753229880686, time: 52.959
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.19    -0.355  -17.4293]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.6     -0.915  -25.1817]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -63.633359025143726, time: 51.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.155  -0.52  -19.998]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.6     -0.915  -25.2191]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -56.505320923870805, time: 52.332
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.755  -0.435  -3.7427]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.75    -0.515  -15.9893]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -72.2773531549463, time: 53.834
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.97  -0.39  -3.736]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.54    -0.62   -14.9184]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -68.95790550685344, time: 53.221
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.81   -0.445  -3.4902]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.325   -0.73   -13.7906]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -60.52228064028548, time: 53.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.845  -0.52   -4.1214]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.245   -0.65   -11.1781]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -60.83046355879391, time: 54.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.2   -0.51  -4.522]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.245   -0.75   -10.8765]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -69.11492272309808, time: 54.326
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.015  -0.51   -3.4053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.585   -0.84   -13.0201]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -61.84908837558354, time: 54.308
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.795  -0.535  -3.5205]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.42    -0.705  -12.6974]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -62.163060721478644, time: 54.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.15   -0.49   -2.9486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.07    -0.75   -12.2066]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -65.20588827433396, time: 54.839
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.94   -0.615  -3.5361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.77    -0.815  -11.1787]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -72.11730581828631, time: 54.589
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.94   -0.54   -3.5352]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.095   -0.755  -11.3726]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -67.48388155168139, time: 52.055
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.22  -0.54  -3.932]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.745   -0.695  -10.3927]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -66.92412503113138, time: 52.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.435  -0.47   -3.0458]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.77    -0.575   -9.7328]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -67.90543087759735, time: 52.929
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.125  -0.555  -3.3099]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.27    -0.63    -9.7553]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -68.85123109343887, time: 52.089
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.045  -0.575  -2.8961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.875   -0.75    -9.4672]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -62.10907665802266, time: 51.982
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.125  -0.49   -2.8941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.005   -0.795  -10.5483]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -58.80310259922005, time: 52.22
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.73   -0.61   -3.4127]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.655   -0.885  -10.3689]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -55.349235885896796, time: 51.984
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.575 -0.52  -4.002]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.065   -0.885  -11.1023]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -70.15263977775828, time: 52.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.3    -0.525  -3.8067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.72    -0.9    -11.1229]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -59.02231276114181, time: 51.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.485  -0.585  -4.9777]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.515  -0.89  -12.048]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -61.75903503798168, time: 52.394
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.645 -0.64  -4.54 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.41    -0.855  -10.9261]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -56.63497690370964, time: 51.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.855  -0.5    -4.5664]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.415   -0.94   -11.1449]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -63.426471372287125, time: 51.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.79   -0.6    -3.9756]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.87    -0.975  -11.2122]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -56.80218084886678, time: 50.893
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.815  -0.555  -4.8429]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.025  -0.875 -11.092]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -62.5141143245256, time: 51.248
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.97   -0.52   -4.2575]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.12    -0.875  -10.7193]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -56.07241065887908, time: 50.935
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.71  -0.585 -5.737]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.115   -0.945  -10.4316]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -43.37445952995065, time: 50.738
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.41   -0.575  -4.6645]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.4     -0.955   -9.9558]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -54.9241164436087, time: 50.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.725   -0.635   -7.6309]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.995   -0.985  -10.8773]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -63.16612431402198, time: 51.103
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.505   -0.52    -7.0023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.035   -0.95   -10.2958]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -53.26562481578074, time: 50.93
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.945  -0.595  -6.6306]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.685  -0.985 -14.089]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -64.25151882550065, time: 52.389
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.075   -0.57    -7.4407]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.93    -0.955  -16.3362]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -53.14500081634337, time: 54.933
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.01    -0.585   -6.2711]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.375   -0.965  -13.7937]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -61.88871796464576, time: 51.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.485   -0.57    -8.2697]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
6400 50
steps: 319950, episodes: 6400, mean episode reward: -77.52410565373346, time: 53.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.31    -0.81   -24.0263]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.925   -0.865  -11.5405]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -71.74155936840019, time: 53.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.81   -0.73  -23.963]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.95    -0.86   -11.5094]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -81.07886764799657, time: 53.168
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.995   -0.855  -24.3119]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.74    -0.91   -11.5612]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -80.59992392154955, time: 52.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.13   -0.8   -23.527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.265   -0.895  -11.1154]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -60.2035940146567, time: 54.067
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.775   -0.62   -22.9385]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.115   -0.905  -11.7381]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -79.82383135275562, time: 53.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.05    -0.8    -24.0146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.785   -0.95   -14.3439]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -76.95652077789879, time: 53.023
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.435   -0.84   -24.3274]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.285   -0.94   -12.8218]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -68.16332427967077, time: 53.931
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.395   -0.75   -23.4873]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.55    -0.875  -12.1968]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -79.46277251588732, time: 54.406
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.58    -0.845  -24.4933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.215   -0.87   -11.5381]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -71.68409700033881, time: 54.088
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.535   -0.815  -23.6082]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.34    -0.75   -10.6913]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -60.828452443356085, time: 54.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.865   -0.76   -23.7609]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.995   -0.835  -11.8145]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -65.21940956430286, time: 53.972
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.485   -0.83   -23.5364]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.69    -0.91   -12.4622]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -61.27335253419753, time: 53.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.4     -0.845  -23.4011]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.67    -0.95   -15.4359]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -65.21440692043086, time: 53.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.01    -0.865  -23.9111]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.54    -0.905  -11.9295]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -90.87322608915866, time: 52.983
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.64    -0.88   -23.4571]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.55    -0.91   -14.0859]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -64.18633111909861, time: 53.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.875   -0.91   -24.0444]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.575   -0.9    -12.2139]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -54.624042832113375, time: 53.427
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.91    -0.825  -23.9433]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.505  -0.955 -12.268]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -52.376434799827074, time: 53.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.035   -0.905  -24.7249]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.945   -0.97   -14.3366]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -53.432280454644015, time: 52.766
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.845   -0.925  -24.6393]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.56    -0.92   -13.4119]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -61.412097937067195, time: 53.967
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.595   -0.875  -24.3641]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.99    -0.795  -12.6209]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -53.04191230671832, time: 52.521
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.355   -0.885  -24.1835]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.16    -0.96   -14.1381]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -61.01490777931038, time: 53.605
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.575   -0.935  -24.5322]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.055   -0.63   -11.1816]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -61.98665658944822, time: 52.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.515   -0.965  -24.4464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.605   -0.9    -14.7234]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -55.76316454655998, time: 53.15
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.92    -0.905  -24.7208]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.6     -0.925  -13.5727]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -65.18678239997216, time: 52.389
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.115   -0.92   -24.2599]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.425   -0.905  -14.7243]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -64.44764320223717, time: 52.326
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.245   -0.91   -24.3669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.     -0.875 -13.331]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -52.60272403592935, time: 53.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.675   -0.92   -24.5044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.68    -0.84   -12.6497]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -62.545342047814536, time: 52.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.815   -0.965  -24.6983]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.      -0.945  -14.1636]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -53.522239058861445, time: 53.035
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.08    -0.885  -24.7571]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.505   -0.915  -13.5872]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -51.759486078890546, time: 53.108
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.515   -0.825  -24.2958]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.665   -0.815  -12.0946]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -59.641197090528486, time: 53.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.335   -0.94   -24.9859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.825   -0.915  -13.2183]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -58.402007501533824, time: 52.347
[-19.9     -0.425   -9.2107]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -86.74520785997127, time: 53.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.47    -0.7    -22.3729]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.525   -0.34    -9.3067]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -93.09810561137994, time: 54.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.84   -0.775 -23.243]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.13    -0.41   -10.2242]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -91.24722105535719, time: 53.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.54    -0.72   -22.4228]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.24    -0.465  -10.7063]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -89.8718580069362, time: 52.642
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.075  -0.695 -23.113]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.47    -0.45   -11.0697]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -84.08582085825343, time: 55.239
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.705   -0.715  -22.2963]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.1     -0.465  -13.1889]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -95.22860783711305, time: 54.264
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.67    -0.72   -22.4005]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.79   -0.455 -12.244]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -85.33889826767309, time: 54.188
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.055   -0.7    -22.7339]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.095   -0.38    -9.8062]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -86.63217411847756, time: 54.879
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.22    -0.69   -23.5555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.885   -0.405   -9.5003]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -82.70641272916637, time: 53.782
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.31    -0.675  -22.8335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.485   -0.51   -11.0619]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -88.42619335398017, time: 54.61
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.495   -0.77   -23.6094]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.34    -0.49   -11.1063]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -86.65919641456274, time: 53.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.845   -0.79   -23.9342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.365   -0.475   -8.4448]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -83.94646019131393, time: 53.989
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.245   -0.785  -24.0752]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.885   -0.47   -11.5209]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -83.4907994152615, time: 53.604
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.205   -0.75   -24.0814]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.455   -0.45    -8.4884]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -77.52408887226535, time: 53.286
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.025   -0.8    -24.0796]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.3     -0.545   -8.8621]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -87.62888095792013, time: 54.593
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.405   -0.785  -23.6988]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.445   -0.505   -9.7762]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -86.4928752191622, time: 53.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.36    -0.715  -22.8842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.58    -0.52    -7.8676]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -75.85738536181613, time: 53.789
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.575   -0.76   -23.7473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.46    -0.41    -8.2446]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -92.30492905376443, time: 53.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.51    -0.81   -23.7737]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.695   -0.46    -8.2193]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -80.82438928201458, time: 54.21
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.945   -0.795  -24.0227]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.145   -0.47    -7.0935]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -88.32525582637713, time: 52.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.335   -0.9    -24.4555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.875   -0.43    -7.9226]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -78.09112771791033, time: 53.566
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.875   -0.83   -24.0531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.125   -0.455   -6.9885]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -73.76298072913075, time: 53.651
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.295  -0.81  -24.174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.04    -0.425   -6.7028]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -87.05057305531655, time: 52.163
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.58    -0.81   -23.8857]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.785   -0.51    -7.0427]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -89.22613902151947, time: 51.983
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.15    -0.795  -24.2189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.85    -0.405   -5.6994]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -77.7659305567625, time: 52.26
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.33    -0.82   -24.3549]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.82    -0.5     -7.3368]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -73.57601271444584, time: 52.117
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.745   -0.785  -24.5286]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.155   -0.425   -6.5989]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -73.95858527453656, time: 51.697
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.57    -0.835  -24.4183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.36    -0.44    -8.2708]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -77.00602852104763, time: 51.881
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.525   -0.8    -24.3328]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.76    -0.455   -7.0534]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -73.32658139115158, time: 51.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.165   -0.885  -24.8227]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.54    -0.475   -7.0855]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -75.33829481211573, time: 52.131
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.07    -0.845  -24.7384]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.33    -0.405   -7.4538]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -75.2935663991115, time: 52.935
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.685   -0.86   -24.4837]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.875   -0.345   -5.3949]
12600 
6400 50
steps: 319950, episodes: 6400, mean episode reward: -71.23693478832001, time: 53.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.37    -0.845  -24.3294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.395   -0.47   -20.9243]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -69.01404055823565, time: 54.096
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.055   -0.765  -23.9659]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.915   -0.83   -23.2693]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -74.71002600867142, time: 52.756
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.14    -0.72   -23.4807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.83    -0.825  -22.4991]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -76.02528620518578, time: 52.771
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.135  -0.735 -23.384]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.245   -0.69   -21.9017]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -76.74042232778673, time: 53.827
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.94    -0.745  -22.8392]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.175   -0.69   -22.5721]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -76.33915378151423, time: 54.027
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.25    -0.84   -23.5876]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.48    -0.76   -22.8825]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -69.02066458154003, time: 53.807
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.98    -0.85   -23.4481]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.225   -0.78   -22.1426]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -62.976021912355094, time: 54.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.49    -0.77   -23.6675]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.935   -0.83   -22.5328]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -64.36876803771209, time: 54.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.98    -0.9    -24.0216]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.73    -0.865  -22.9991]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -84.0286976550286, time: 54.007
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.545   -0.85   -23.3129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.78    -0.675  -21.0776]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -65.68795409644132, time: 54.985
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.39    -0.68   -22.1451]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.825   -0.755  -22.4061]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -71.47566837538272, time: 54.169
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.795  -0.81  -22.798]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.38    -0.755  -23.3188]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -60.24775987375792, time: 52.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.2     -0.69   -21.9918]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.255   -0.865  -23.3103]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -63.62781226109968, time: 54.115
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.375   -0.83   -23.5155]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.475   -0.63   -21.7316]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -66.75976294558103, time: 53.473
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.305   -0.855  -23.6475]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.065   -0.79   -23.1961]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -67.28651402941014, time: 53.766
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.675   -0.785  -23.3358]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.225   -0.865  -23.4534]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -62.992818372486, time: 53.744
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.42    -0.735  -22.9788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.08    -0.915  -24.0399]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -64.16966356144077, time: 55.864
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.525  -0.825 -23.739]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.52    -0.89   -23.6947]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -65.5780104783389, time: 53.77
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.755   -0.89   -23.6574]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.785   -0.89   -23.2793]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -66.92421077927496, time: 53.673
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.45   -0.82  -23.249]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.665   -0.855  -23.1524]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -55.03848001096133, time: 53.028
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.065   -0.89   -23.5424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.045   -0.855  -23.9227]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -64.81052794954698, time: 53.391
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.49    -0.88   -23.3587]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.575   -0.955  -23.7879]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -60.0239852693674, time: 53.054
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.755   -0.79   -23.2352]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.895   -0.94   -23.9965]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -57.056366253481265, time: 52.972
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.43    -0.915  -23.7011]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.125   -0.855  -23.3575]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -65.61729141099191, time: 54.293
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.785   -0.93   -24.0338]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.515   -0.915  -23.7313]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -60.90247518032049, time: 53.351
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.16    -0.745  -22.0223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.675   -0.885  -23.8271]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -57.849622895222055, time: 53.781
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.785   -0.82   -22.4521]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.535   -0.97   -24.4591]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -60.7053857552407, time: 52.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.645   -0.87   -23.8998]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.13    -0.925  -24.0476]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -55.99514585483735, time: 52.847
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.45    -0.825  -22.2923]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.3    -0.935 -24.958]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -55.36898213077432, time: 53.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.53    -0.855  -23.7565]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.25    -0.87   -24.8658]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -60.498721395482555, time: 53.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.28    -0.825  -22.7963]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.185   -0.925  -24.8867]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -55.781633442463615, time: 52.416
[-17.4     -0.865  -10.0479]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -50.52771149749975, time: 50.311
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.375   -1.     -16.4019]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.07    -0.88   -10.1121]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -49.368587968268336, time: 50.624
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.415   -0.99   -14.5507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.22    -0.905  -10.6367]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -58.968139746213744, time: 50.724
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.175   -1.     -15.0855]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.355   -0.905  -13.4086]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -57.632229655434905, time: 51.405
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.275   -0.995  -15.4175]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.31    -0.885  -11.8865]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -67.84100273756529, time: 51.764
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.87    -0.99   -14.2499]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.01    -0.89   -11.0985]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -51.69862033375679, time: 50.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.14    -1.     -13.7066]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.42   -0.855 -10.612]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -48.527940826919874, time: 51.51
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.44    -1.     -13.8373]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.385   -0.87   -10.5242]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -48.602244452139985, time: 51.658
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.11    -1.     -13.6571]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.23    -0.915  -10.5383]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -45.47691592479985, time: 51.675
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.675  -0.995 -14.87 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.31   -0.885  -9.093]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -50.35776872376063, time: 51.158
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.14    -0.985  -13.0121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.57    -0.885  -10.8847]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -61.727680428534896, time: 50.633
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.955   -0.98   -13.2109]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.84   -0.855 -10.195]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -42.95929554164756, time: 50.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.205   -0.99   -11.0442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.24    -0.9     -9.6954]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -49.7192826467134, time: 50.967
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.37    -0.99   -12.3984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.065   -0.925  -10.2789]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -52.52681671854206, time: 51.299
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.275   -1.     -13.7139]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.87    -0.88   -10.1061]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -47.61187752326091, time: 50.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.56    -0.995  -15.2884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.375   -0.865  -10.0641]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -44.88863550227868, time: 50.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.265   -0.995  -11.3661]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.95    -0.91    -9.9832]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -44.45190269631003, time: 51.24
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.725   -0.99   -11.5718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.245   -0.91   -11.0053]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -48.14381112978111, time: 52.29
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.455   -0.98   -12.1464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.615   -0.875   -9.7323]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -40.881454897733036, time: 51.794
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.69    -0.995  -11.7959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.36    -0.93   -10.7351]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -47.760028538573685, time: 51.206
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.635   -1.     -10.7597]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.365   -0.965  -10.2576]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -45.35455623133821, time: 51.331
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.82    -0.99   -11.3122]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.5     -0.91   -10.8524]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -54.76364808593084, time: 54.11
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.985   -0.995  -11.4188]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.61    -0.87   -10.4541]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -44.74811891396215, time: 51.989
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.35    -1.     -11.5741]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.89    -0.9    -11.5445]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -47.059850775899285, time: 51.298
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.595   -1.     -13.8519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.415   -0.935  -12.5182]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -49.84625211185838, time: 51.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.25    -1.     -12.5386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.36    -0.92   -12.7776]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -53.65515313104284, time: 51.534
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.7     -1.     -10.9423]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.555   -0.95   -11.5254]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -49.235217004818985, time: 51.516
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.185   -0.995  -11.6698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.36    -0.86   -11.4788]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -46.766169156669, time: 52.098
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.115  -1.    -11.083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.975   -0.925  -12.0627]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -39.19681962645629, time: 51.567
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.105   -0.995  -11.2042]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.325   -0.92   -10.8211]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -46.07413223637559, time: 51.098
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.15    -0.995  -11.1075]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.845   -0.91   -10.2905]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -48.433751789279455, time: 52.394
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.1     -0.99   -10.9349]
agent1_energy_min, agent1_energy_max, agent1_energy_avg 50
steps: 629950, episodes: 12600, mean episode reward: -63.83694538044814, time: 54.854
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.895   -1.     -24.2057]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.63    -0.805  -24.2878]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -65.40973796421433, time: 51.491
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.635   -1.     -24.8984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.26    -0.855  -24.8588]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -61.86844494641764, time: 51.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.445   -1.     -24.8816]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.17    -0.84   -24.8023]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -72.30153183735912, time: 52.163
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.615   -1.     -24.5396]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.045  -0.845 -24.725]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -64.56406653184895, time: 51.711
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.28    -0.995  -24.4443]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.97    -0.865  -24.6503]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -63.1725389705116, time: 51.826
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.785   -0.99   -24.9619]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.515   -0.85   -25.0788]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -64.82117635237859, time: 51.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.97    -1.     -24.5319]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.305   -0.855  -24.9705]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -61.273849345323114, time: 51.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.78    -0.99   -24.9266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.39    -0.87   -24.9863]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -68.32360151552761, time: 52.009
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.685   -0.99   -24.5906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.49    -0.88   -25.0347]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -60.07292053168545, time: 51.913
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.425   -0.99   -24.6324]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.235   -0.88   -24.8725]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -58.72033132118817, time: 51.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.525   -0.995  -25.2544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.565  -0.885 -25.135]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -57.117998405283195, time: 51.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.565   -1.     -24.8185]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.375  -0.875 -24.96 ]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -58.89703344934842, time: 51.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.875   -0.995  -23.8129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.44    -0.91   -25.0101]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -58.37277456948312, time: 52.59
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.98    -1.     -24.9822]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.485   -0.905  -25.0469]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -62.68650947430488, time: 52.4
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.63    -0.99   -25.2841]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.34    -0.915  -24.9705]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -58.24474059511818, time: 52.37
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.47    -0.995  -25.1483]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.705   -0.955  -25.2758]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -55.32993443818223, time: 50.929
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.55    -1.     -25.1862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.395   -0.94   -25.0084]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -69.2668173352038, time: 52.219
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.375   -0.995  -25.1431]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.285   -0.915  -24.9966]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -55.975003496820335, time: 51.633
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.535   -0.985  -25.2617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.49    -0.87   -25.0559]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -57.65347482900597, time: 51.403
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.115   -0.99   -25.1171]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -0.905  -25.0547]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -58.88896563443125, time: 51.628
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.57    -1.     -24.9126]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -0.895  -25.0723]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -56.368224849773306, time: 54.931
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.795   -1.     -23.9342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.44    -0.92   -25.0357]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -56.637032668711385, time: 51.167
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.59    -1.     -24.9384]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595  -0.935 -25.166]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -53.62585070520868, time: 52.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.125   -1.     -23.1131]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.265   -0.89   -24.9216]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -57.26770576846673, time: 52.653
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.62    -1.     -23.0582]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.215   -0.845  -24.8707]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -55.93245267304789, time: 51.806
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.985  -1.    -23.944]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.67    -0.885  -25.2068]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -57.42781970189489, time: 51.914
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.635   -1.     -23.3387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.84    -0.935  -25.3677]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -62.916500210341674, time: 51.685
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.41   -1.    -24.395]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.785   -0.9    -25.3025]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -64.54832010271444, time: 52.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.84   -0.995 -24.201]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.835   -0.93   -25.3705]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -58.437135381540664, time: 52.41
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.07   -1.    -23.854]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.67   -0.895 -25.231]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -56.685086739994375, time: 51.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.56    -0.995  -25.3084]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.79   -0.94  -25.333]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -59.64048383151168, time: 52.167
[-22.05    -0.98   -14.0006]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.405 -0.28  -2.965]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -64.7337175657305, time: 51.031
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.295   -0.975  -15.2736]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.19   -0.335  -3.9837]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -70.35142066188835, time: 51.402
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.435   -0.935  -15.8334]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.765   -0.42    -6.2296]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -62.11966063777718, time: 52.821
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.86    -0.995  -16.0901]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.05   -0.335  -2.2433]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -63.25103922880025, time: 51.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.795   -0.975  -15.3882]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.71   -0.32   -3.1958]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -64.56332919573306, time: 51.363
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.455   -0.99   -16.6375]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.58   -0.41   -4.5225]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -68.28544904359032, time: 51.998
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.315   -0.975  -17.4226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.395 -0.425 -5.423]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -64.7327568844781, time: 51.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.68    -0.98   -17.5281]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.835  -0.45   -4.7271]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -68.3256783332302, time: 52.51
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.235   -1.     -18.8056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.065   -0.61    -6.8459]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -64.84754694153341, time: 51.105
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.325   -0.99   -16.8584]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.925  -0.435  -4.6226]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -56.3042553540407, time: 51.003
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.79   -1.    -14.941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.215  -0.505  -5.5751]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -66.16103801459076, time: 52.068
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.425  -0.995 -14.683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.      -0.45    -6.3081]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -62.1951494547774, time: 51.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.58    -0.995  -14.7499]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.405  -0.48   -5.867]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -60.845888170801125, time: 52.019
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.975  -1.    -15.755]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.985 -0.35  -4.235]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -67.65384874658378, time: 51.248
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.68   -0.975 -14.539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.535  -0.395  -4.6993]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -60.08485469886037, time: 51.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.19    -0.995  -14.8817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.88   -0.415  -4.8862]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -58.08433535375834, time: 51.6
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.625   -0.965  -12.3634]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.745   -0.345   -4.9788]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -56.43705109299507, time: 51.086
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.795   -0.94   -11.6459]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.085   -0.42    -6.5195]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -56.43315865028617, time: 52.15
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.28    -0.945  -12.5282]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.255   -0.49    -6.7311]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -65.76565920304081, time: 52.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.035   -0.985  -13.8873]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.135 -0.415 -4.021]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -65.74429578378319, time: 51.409
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.51   -0.98  -13.266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.35   -0.345  -4.9606]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -60.827075670067416, time: 50.861
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.47    -0.99   -12.4206]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.695  -0.345  -3.9097]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -67.46915515040078, time: 51.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.635   -1.     -14.6728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.94  -0.68  -5.569]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -62.00217216107245, time: 52.396
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.345   -1.     -14.4762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.04   -0.53   -5.0315]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -57.13995402612725, time: 51.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.95    -1.     -13.2388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.11   -0.54   -4.4439]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -65.26716669603935, time: 51.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.83    -0.99   -15.3702]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.125   -0.62    -7.1801]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -61.982219052946256, time: 51.429
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.67    -1.     -15.5743]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.335  -0.575  -5.1874]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -61.152831532770016, time: 52.409
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.125   -0.995  -16.5386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.205  -0.6    -6.0305]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -66.66236264561557, time: 52.534
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.73   -0.995 -15.224]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.72    -0.53    -6.9102]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -57.69668811830279, time: 51.367
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.905   -0.995  -15.3797]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.205  -0.595  -6.1378]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -63.516502395590514, time: 51.529
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.61    -0.98   -14.5892]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.125  -0.685  -6.2978]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -52.277263498444235, time: 52.082
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.95    -0.99   -13.0668]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.46    -0.97   -25.1353]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.305   -0.625  -20.5469]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -52.21200943403569, time: 52.252
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.89    -0.975  -24.7703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.005   -0.96   -24.0914]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -49.619609985330136, time: 52.203
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.375   -0.995  -25.1046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.26    -0.94   -23.4035]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -51.57698311671471, time: 52.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.225   -0.995  -24.9817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.07    -0.995  -23.6108]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -46.60813290829712, time: 52.652
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.585  -0.995 -25.228]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.94    -0.985  -23.9478]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -50.605040211723825, time: 51.81
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.66    -1.     -23.5416]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.43    -0.965  -23.6429]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -47.21168225919441, time: 52.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.69    -1.     -25.3003]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.67    -1.     -24.5683]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -44.8038383998, time: 52.779
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.6     -0.99   -25.2366]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.54    -1.     -24.5137]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -55.515111724072106, time: 53.118
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.275   -1.     -25.0102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.555   -0.995  -23.9624]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -43.89369571842764, time: 51.514
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.18    -0.985  -24.9539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.825   -1.     -24.7617]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -46.40209181870905, time: 51.287
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.545   -0.995  -24.5157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.6     -1.     -24.5618]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -46.22379031382316, time: 51.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.74    -1.     -25.3524]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.825   -1.     -24.7105]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -45.49969186840324, time: 51.587
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.575   -1.     -25.2524]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.155   -1.     -24.9841]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -42.90257144605104, time: 52.585
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.48    -1.     -25.1508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.08    -1.     -24.9513]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -47.49807127086797, time: 52.518
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.27    -1.     -24.3268]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.33    -1.     -25.0736]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -57.05142817805388, time: 52.551
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.2     -0.995  -25.0063]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.21   -1.    -25.005]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -41.14552752695923, time: 51.646
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.39    -0.985  -24.5615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.935  -0.995 -25.465]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -43.81011891064658, time: 52.248
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.26    -0.995  -25.0078]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -1.     -25.2857]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -41.6409403463313, time: 52.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.525  -1.    -25.176]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.61    -1.     -25.2204]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -44.87444956418586, time: 52.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.395   -0.99   -25.1086]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -1.     -25.1909]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -40.759092261313214, time: 52.398
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.655  -0.995 -25.236]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.82    -0.995  -25.3907]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -38.2580979576161, time: 51.521
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.435   -0.995  -25.1218]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.445   -1.     -25.1433]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -46.3249734821771, time: 51.895
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.83    -0.995  -24.6943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.12    -1.     -24.9162]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -45.38145286229133, time: 52.451
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.91    -0.995  -24.7619]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.515   -1.     -25.2684]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -46.15203090780688, time: 52.65
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.53   -1.    -25.222]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.5    -1.    -25.233]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -35.90177695254097, time: 52.987
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.34    -1.     -25.0271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.865   -1.     -25.4113]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -42.45654253090756, time: 52.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.175   -1.     -24.8772]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.72    -1.     -25.3012]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -41.0313214609082, time: 52.689
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.275   -1.     -25.0293]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.445   -1.     -25.1381]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -41.6241709063927, time: 52.999
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.525   -1.     -25.1957]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.485   -1.     -25.1196]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -31.313869240797445, time: 52.159
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.775  -1.    -25.344]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.78    -1.     -25.3248]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -37.75560246742751, time: 53.04
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.47    -1.     -24.5559]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.845   -0.995  -25.3817]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -34.81909090198419, time: 52.055
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.675   -0.985  -13.6118]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -47.960718559546166, time: 51.537
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.21    -0.595   -7.2598]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.035   -0.965  -13.3422]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -53.476888052873164, time: 50.601
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.9     -0.67    -8.3392]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.925   -0.965  -14.8809]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -53.06972405191592, time: 51.833
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.99    -0.555   -7.4958]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.18    -0.95   -15.4786]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -56.670572698998896, time: 50.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.685   -0.64    -7.7325]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.215   -1.     -14.2031]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -60.23608712902602, time: 51.275
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.575   -0.515   -6.9829]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.91    -0.965  -15.8806]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -61.873115618317314, time: 51.712
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.765   -0.56    -7.6686]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.37    -0.975  -16.5188]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -52.23895135366605, time: 50.88
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.005   -0.595   -6.8264]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.955   -0.99   -13.7434]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -85.90852661314383, time: 52.18
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.7     -0.635   -8.1412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.835   -0.975  -14.9209]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -63.01017167686362, time: 50.029
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.595   -0.605   -7.3268]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.79   -0.955 -13.831]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -53.3207104618392, time: 50.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.16    -0.59    -8.0749]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.8     -0.925  -10.6235]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -56.04225439175837, time: 50.57
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.175   -0.645   -8.0174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.13    -0.98   -11.3071]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -57.47691650136818, time: 50.309
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.35    -0.58    -7.4524]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.715   -0.995  -11.3705]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -51.86840691277654, time: 53.049
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.92   -0.58   -6.5525]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.835   -0.95   -10.1804]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -46.5613145447912, time: 51.777
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.785   -0.56    -7.4026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.475   -0.95    -9.4937]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -55.331267120419206, time: 51.08
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.86    -0.55    -7.1783]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.79    -0.955   -9.5754]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -52.93244149728734, time: 51.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.14    -0.61    -8.2014]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.82   -0.975 -10.24 ]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -45.16992398328967, time: 51.861
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.675  -0.585  -6.4318]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.925   -0.925   -9.7551]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -50.527719509715936, time: 51.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.575  -0.535  -5.5295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.55    -0.97    -9.8785]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -46.745189780311314, time: 51.32
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.805  -0.69   -7.2633]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.345   -0.98   -10.4005]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -44.713185739414804, time: 51.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.49   -0.635  -7.0782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.145  -0.965  -9.719]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -57.20749173397335, time: 51.018
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.345  -0.57   -6.2936]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.06    -0.92   -10.1513]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -50.532092028951304, time: 51.786
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.2     -0.55    -7.3909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.26    -0.97    -9.9778]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -52.20274468147958, time: 52.082
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.085  -0.58   -7.64 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.98    -0.965  -10.2478]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -51.390787338117896, time: 51.771
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.005  -0.55   -6.6793]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.91    -0.92   -10.2086]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -46.57747624135099, time: 51.92
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.175  -0.555  -6.3233]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.14    -0.975   -9.7841]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -47.49852355727253, time: 51.621
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.795  -0.67   -6.8097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.79    -0.96    -9.3703]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -50.043844829989084, time: 51.602
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.535   -0.645   -7.8722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.325   -0.975  -10.6206]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -44.650308271024, time: 52.292
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.565  -0.49   -6.6309]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.275   -0.965   -9.7132]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -44.60722456132051, time: 51.882
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.21   -0.62   -6.5842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.06    -0.995   -9.1279]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -50.29604425438709, time: 52.066
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.045  -0.665  -7.1514]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.905   -0.95   -10.1087]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -57.975941862958095, time: 51.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.215   -0.675   -8.3256]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.58    -0.99    -9.9601]
[-43.57    -0.57   -21.3054]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.445   -0.815  -25.0151]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -60.010175983278785, time: 52.448
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.19    -0.545  -19.7828]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.24    -0.86   -24.9071]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -61.143152269946356, time: 52.72
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.4     -0.535  -19.3153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.585   -0.925  -25.1614]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -58.1324760917224, time: 52.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.62   -0.455 -18.307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.705   -0.93   -25.2568]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -59.9832466749959, time: 52.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.785   -0.66   -21.1212]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.625   -0.915  -25.1815]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -58.6360843083462, time: 54.941
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.765   -0.645  -19.6899]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.38    -0.89   -24.9939]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -55.88695340746289, time: 52.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.915   -0.605  -18.5026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -0.9    -25.2034]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -52.296206166180674, time: 52.255
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.215   -0.64   -19.2295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.66    -0.91   -25.2128]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -56.57202872911493, time: 53.729
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.5     -0.635  -18.9075]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.38    -0.845  -24.9557]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -56.93162928357902, time: 52.503
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.235   -0.685  -18.6219]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.425   -0.915  -25.0352]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -56.72821628593506, time: 51.367
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.395   -0.755  -18.4086]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.625   -0.94   -25.2087]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -54.11632345951774, time: 51.688
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.      -0.815  -18.5517]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.645   -0.895  -25.2145]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -54.5474606925998, time: 51.979
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.79    -0.745  -16.9381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.31    -0.885  -24.9804]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -60.64970046823007, time: 53.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.735   -0.865  -17.3808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.41    -0.87   -25.0521]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -52.192491602205784, time: 52.832
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.93    -0.83   -17.3648]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.205   -0.885  -24.8474]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -50.63523205115969, time: 52.517
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.975   -0.8    -16.3033]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.78    -0.97   -25.3219]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -50.19412373709422, time: 52.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.4     -0.835  -16.3636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.705   -0.95   -25.2573]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -56.777446306506945, time: 52.764
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.66    -0.825  -17.1159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.515   -0.945  -25.1393]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -50.28389617608027, time: 52.688
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.095   -0.875  -16.4199]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.42    -0.875  -24.9983]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -51.380665973490906, time: 52.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.9     -0.88   -16.8414]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.54    -0.925  -25.1389]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -51.91993572775687, time: 52.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.1     -0.875  -17.0499]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.525   -0.955  -25.1191]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -55.27056870867373, time: 52.006
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.02   -0.955 -18.823]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.425   -0.98   -25.0754]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -57.44350328399051, time: 51.963
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.53    -0.91   -18.9627]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.39    -0.935  -25.0277]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -56.19927864991274, time: 52.836
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.3     -0.905  -19.5445]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.415   -0.94   -25.0238]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -55.283957237041875, time: 52.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.67    -0.85   -19.1698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.49    -0.985  -25.1241]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -55.17472318208148, time: 52.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.705  -0.735 -18.104]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -0.93   -25.0964]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -54.84085298204137, time: 52.495
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.19    -0.795  -16.8866]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.19    -0.92   -24.8281]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -54.87654970857746, time: 52.707
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.125  -0.84  -18.335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.365   -0.97   -25.0185]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -52.33577987645075, time: 53.221
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.44    -0.75   -18.0408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.42   -0.965 -25.053]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -59.17581681769384, time: 53.409
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.38   -0.835 -17.627]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.16   -0.985 -24.904]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -62.022239499939126, time: 53.074
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.595   -0.645  -15.5483]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.26    -0.995  -25.0327]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -61.662459033106096, time: 52.909
agent0_energy_min, agent0_energy_max, agent0_energy_avg
12600 50
steps: 629950, episodes: 12600, mean episode reward: -46.89655139734031, time: 52.962
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.12    -0.965  -21.2428]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.28    -1.     -17.1504]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -47.27431605756603, time: 52.689
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.41    -0.99   -21.7295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.955   -1.     -16.5751]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -49.83662378643556, time: 53.46
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.99   -0.99  -23.099]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.205   -0.99   -16.8424]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -42.76532860074547, time: 53.884
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.95    -1.     -23.6544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.785   -1.     -15.4701]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -44.61754280928994, time: 52.215
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.605  -0.995 -22.736]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.83    -1.     -16.1052]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -46.45862753589852, time: 52.63
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.86    -0.97   -22.7636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.105   -1.     -16.7884]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -48.897958872079016, time: 53.199
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.77    -0.985  -21.7042]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.52   -0.96  -22.124]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -47.11355427280958, time: 53.196
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.305   -0.945  -21.6195]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.22    -0.965  -19.9928]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -45.630038530137824, time: 53.245
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.175   -0.985  -23.3792]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.18    -1.     -20.6218]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -45.07787607382599, time: 52.479
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.395  -0.975 -21.294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.28    -1.     -20.2447]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -45.902787174355105, time: 52.744
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.085  -0.97  -21.558]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.475   -0.975  -23.3333]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -45.91637767123393, time: 52.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.905   -0.955  -21.9798]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.335   -0.965  -22.1838]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -43.58282191339432, time: 53.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.5     -0.98   -20.1403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.06    -1.     -22.3615]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -45.263562489438065, time: 53.597
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.2     -0.965  -22.6289]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.105   -1.     -21.6145]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -49.306694881752065, time: 52.365
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.225   -0.97   -20.5118]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.955   -0.98   -19.1532]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -45.72342456042474, time: 52.616
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.195   -0.985  -21.7592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.225   -0.995  -14.5485]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -38.331285797212466, time: 53.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.735   -0.985  -22.3016]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.96    -1.     -22.4803]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -43.8097111861633, time: 52.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.43    -0.995  -22.6108]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.235   -1.     -22.7648]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -44.06965817251011, time: 52.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.295   -0.975  -22.3778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.68    -1.     -22.4953]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -43.38140473158981, time: 52.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.9     -0.99   -21.4586]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.715   -0.995  -23.4076]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -52.76934298876784, time: 52.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.685   -1.     -20.1322]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.475   -1.     -23.9289]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -43.6667363365745, time: 53.669
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.625   -0.99   -19.9056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.825   -1.     -23.8349]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -40.5863274174459, time: 54.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.535   -0.995  -20.6444]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.73    -1.     -24.1495]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -40.93032770141876, time: 52.696
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.985   -0.99   -21.9016]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.205   -0.995  -20.5262]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -37.76355707225144, time: 53.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.535   -0.99   -20.6905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.455   -1.     -18.1799]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -41.912286234876916, time: 53.531
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.03    -0.995  -21.7817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.21    -0.995  -17.5024]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -48.15288784430192, time: 52.383
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.235   -0.995  -21.7351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.14    -1.     -15.5037]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -43.38100440537717, time: 52.734
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.925   -0.99   -22.3668]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.17    -1.     -17.1832]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -36.71955644289312, time: 54.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.805   -0.995  -20.4532]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.5     -1.     -12.7581]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -38.44790220629125, time: 53.678
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.18    -0.97   -23.0986]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.685   -1.     -14.2112]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -39.85755172935137, time: 52.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.18    -1.     -23.4962]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.35    -1.     -19.3278]50
steps: 629950, episodes: 12600, mean episode reward: -76.3680270951087, time: 51.786
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.19    -0.78   -24.0701]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.89    -0.465   -7.1613]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -80.85627652455676, time: 51.366
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.66    -0.855  -24.4904]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.4    -0.535  -8.8  ]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -73.34444103699195, time: 51.693
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.93    -0.83   -24.6367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.5     -0.435   -8.0555]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -82.15515631601622, time: 52.533
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.74    -0.84   -24.5343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.29    -0.465   -7.9294]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -85.73292648604682, time: 50.697
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.98   -0.86  -24.711]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.545   -0.475   -8.1532]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -80.89953592506399, time: 51.035
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.185   -0.855  -24.1791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.68    -0.485   -9.3934]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -82.94115710008573, time: 51.052
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.415   -0.87   -24.3705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.435   -0.415   -7.0682]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -77.80493850498539, time: 50.96
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.315   -0.88   -24.2528]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.645   -0.48    -9.2571]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -70.1161750085687, time: 51.677
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.53   -0.865 -24.359]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.805  -0.615  -9.665]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -70.82616862432982, time: 50.145
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.62    -0.84   -24.3892]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.1     -0.5     -8.3978]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -75.28004968711971, time: 50.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.045   -0.875  -24.7436]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.175   -0.53    -9.5912]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -80.83229624033073, time: 50.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.58    -0.92   -24.5075]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.84    -0.505  -10.9726]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -70.13600842122122, time: 51.962
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.085  -0.865 -24.802]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.06    -0.49    -9.7657]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -76.14509905061418, time: 52.552
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.98    -0.865  -24.6989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.82    -0.41    -8.0525]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -74.92742713667204, time: 50.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.      -0.895  -24.7432]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.445   -0.61   -10.2245]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -79.58312296599685, time: 50.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.19    -0.895  -24.8815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.6     -0.51    -8.0985]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -66.29304615217738, time: 51.128
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.33    -0.83   -24.9284]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.09    -0.46    -8.4958]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -76.17519750048638, time: 50.731
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.12    -0.94   -24.8459]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.69    -0.465   -8.5869]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -70.99861954838536, time: 51.392
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.92    -0.93   -24.6521]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.285   -0.46    -7.1026]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -74.96987178196838, time: 51.199
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.15    -0.875  -24.1026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.855  -0.385  -5.6334]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -70.93904548533986, time: 50.468
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.8    -0.92  -24.584]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.215   -0.475   -7.0724]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -71.25037675896984, time: 50.58
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.74    -0.91   -24.5021]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.895   -0.43    -6.3896]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -73.07043770025854, time: 52.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.785   -0.895  -24.5285]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.295   -0.485   -9.5221]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -74.82328182197932, time: 52.089
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.56    -0.9    -24.3567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.35    -0.43    -7.6959]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -68.03151427067304, time: 52.124
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.23    -0.885  -24.0993]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.515   -0.48    -7.2641]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -65.29367830827809, time: 51.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.62   -0.865 -24.401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.675   -0.475   -7.4236]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -68.55370915519995, time: 51.907
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.855   -0.9    -23.8435]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.62    -0.43    -6.5153]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -69.07232799494153, time: 51.725
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.14   -0.83  -24.058]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.515   -0.525   -9.2503]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -71.64154067587054, time: 51.826
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.335   -0.905  -24.2545]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.165   -0.525   -7.9353]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -72.35828730838608, time: 52.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.71    -0.905  -24.4841]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.25    -0.53   -10.3161]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -71.79810074328168, time: 51.132
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.96    -0.875  -23.9471]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.625   -0.6     -9.1496]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -75.38614503530891, time: 52.092
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.77    -0.945  -24.6347]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.105   -0.935  -13.7614]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -48.785020543713635, time: 52.839
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.295   -0.96   -24.9829]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.055   -0.985  -17.2432]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -53.645739409478566, time: 52.985
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.285  -0.955 -25.023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.88    -0.955  -17.2951]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -51.956337814587926, time: 53.663
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.845   -0.88   -24.6455]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.335   -0.92   -16.1785]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -57.05332402007483, time: 54.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.795   -0.905  -24.7072]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.235   -0.885  -16.2785]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -58.34891868560331, time: 52.475
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.84    -0.87   -23.7247]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.81    -0.865  -15.4793]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -46.9887488009108, time: 52.414
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.32   -0.91  -23.955]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.82  -0.94 -16.35]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -44.27285540220464, time: 53.283
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.02    -0.925  -24.3768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.675   -0.685  -13.4853]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -59.71316635058227, time: 52.595
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.725   -0.97   -24.3264]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.04    -0.675  -15.0717]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -67.1667762143091, time: 51.664
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.105   -0.97   -22.6005]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.61    -0.885  -17.4894]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -57.433035471604015, time: 51.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.01    -0.925  -21.8706]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.87    -0.87   -15.7367]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -49.64276449380304, time: 51.802
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.83    -0.95   -23.0678]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.275   -0.915  -15.7042]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -47.87332645966073, time: 51.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.165  -0.99  -23.361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.85   -0.965 -18.673]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -51.79960398286009, time: 53.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.085   -0.95   -22.7029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.715   -0.93   -17.3438]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -48.729626549264864, time: 51.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.255  -0.97  -23.031]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.775   -0.87   -15.4417]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -50.15097192010888, time: 51.562
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.745   -0.86   -22.8177]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.095   -0.885  -15.7725]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -53.0533796063211, time: 52.043
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.38    -0.93   -23.2464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.625   -0.92   -16.9507]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -50.565259550394074, time: 51.384
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.7     -0.9    -22.8261]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.12    -0.98   -19.5527]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -56.08607757426491, time: 52.365
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.59    -0.885  -22.0816]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.12    -0.92   -16.5288]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -50.878040491516096, time: 50.86
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.445   -0.91   -23.0464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.875   -0.91   -18.5181]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -50.219677460135024, time: 51.827
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.7     -0.895  -23.6669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.485   -0.95   -19.6833]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -52.555235662735925, time: 51.409
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.505   -0.88   -23.5806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.855   -0.975  -18.1797]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -46.345874953727105, time: 51.34
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.46    -0.91   -23.1779]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.9     -0.95   -18.5827]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -48.933990792554724, time: 50.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.83    -0.925  -24.0188]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.165   -0.94   -20.8626]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -44.486080491444675, time: 50.96
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.885   -0.96   -24.4523]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.44   -0.96  -21.638]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -43.24487219859566, time: 51.029
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.74    -0.98   -22.7813]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.405   -0.99   -22.8142]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -47.14029780123083, time: 51.503
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.485   -0.945  -23.4226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.625   -0.925  -19.8931]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -48.85482479362762, time: 50.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.335   -0.99   -22.7498]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.035   -0.7    -19.8187]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -49.31208868505851, time: 51.343
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.55    -0.94   -23.0884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.525   -0.695  -17.1251]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -47.90401552307259, time: 51.374
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.005   -0.98   -22.7315]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.35    -0.995  -22.5636]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -50.82443295980274, time: 50.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.485   -0.97   -23.5746]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.24    -0.955  -21.1053]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -45.43534602983265, time: 53.519
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.275   -0.875  -23.5848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.2     -0.92   -24.8836]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -52.702478417779545, time: 53.032
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.45    -0.935  -23.6504]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.24    -0.88   -24.9272]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -52.92399275949532, time: 53.761
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.085   -0.945  -23.2771]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.4     -0.92   -25.0026]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -71.3506709141646, time: 53.261
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.355   -0.9    -21.7555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.375   -0.95   -25.0524]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -58.52672102596139, time: 52.888
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.165   -0.935  -24.0102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.29    -0.9    -24.9459]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -62.32571663299687, time: 53.098
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.755   -0.92   -22.3031]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.87    -0.915  -24.7261]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -53.961804085975345, time: 52.99
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.165   -0.925  -23.5163]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.335   -0.92   -24.9991]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -59.75554037340362, time: 53.129
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.56    -0.855  -22.6559]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.32    -0.92   -24.9231]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -60.47969918871777, time: 52.487
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.365   -0.91   -22.1026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.23    -0.93   -24.8908]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -56.147839698676506, time: 52.239
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.81    -0.945  -23.4652]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.24    -0.925  -24.9365]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -55.87843719705854, time: 52.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.      -0.97   -21.3351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.11    -0.92   -24.8327]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -52.60473277225, time: 53.071
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.49    -0.965  -22.5915]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.015   -0.905  -24.7623]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -46.72290105921872, time: 53.746
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.035   -0.985  -24.1505]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.31    -0.95   -25.0015]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -60.65675382492707, time: 53.384
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.     -0.98  -23.968]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.93    -0.955  -24.7254]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -46.930532657709115, time: 52.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.29    -0.95   -24.2627]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.99    -0.935  -24.7465]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -43.734702194139565, time: 52.749
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.39    -0.975  -24.0189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.56   -0.97  -25.163]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -50.68055299952579, time: 53.211
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.035   -0.96   -23.4786]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.64    -0.94   -25.2093]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -50.09549297695377, time: 53.363
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.395   -0.945  -23.3628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.52   -0.94  -25.111]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -54.25914913489247, time: 54.103
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.295   -0.935  -23.4954]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.39    -0.985  -25.1395]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -47.120604859309516, time: 52.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.185   -0.93   -22.8853]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.72    -0.96   -25.3089]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -50.8873215822956, time: 52.566
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.215   -0.925  -24.3643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.17    -0.96   -24.9689]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -45.512481178853776, time: 53.616
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.045   -0.895  -24.2276]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.625   -0.995  -25.2249]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -40.872234830243585, time: 53.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.38    -0.96   -23.9523]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -0.995  -25.2524]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -50.89223540687286, time: 54.091
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.88    -0.89   -21.8063]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.54    -0.955  -25.1868]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -46.13761185148955, time: 53.092
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.385   -0.955  -21.3508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.56    -0.975  -25.1804]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -56.91049045678841, time: 52.916
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.865   -0.905  -22.5302]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.44    -0.995  -25.1413]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -47.91588258074058, time: 52.973
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.74    -0.925  -21.2042]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.525  -0.98  -25.185]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -47.38564078942454, time: 52.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.1    -0.93  -21.179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.525   -0.985  -25.2097]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -49.26460131698159, time: 54.07
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.56  -0.93 -21.77]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -0.985  -25.1325]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -45.868001766062186, time: 51.883
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.75    -0.95   -22.2825]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.7     -0.985  -25.3177]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -55.36946400157516, time: 52.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.885   -0.955  -22.4027]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.3     -0.995  -25.0608]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -49.609152396538214, time: 52.372
[-14.955   -0.915  -10.7928]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -43.3722860115698, time: 52.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.505   -0.995  -12.4035]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.585   -0.9    -11.1283]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -51.93570667079429, time: 51.069
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.62    -0.99   -12.0116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.685   -0.925  -11.4267]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -47.096572385890724, time: 50.703
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.74    -0.905  -11.4759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.505   -0.88   -10.9079]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -39.80072984596772, time: 51.656
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.25    -0.965  -10.3951]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.8    -0.935 -10.669]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -43.20498588638004, time: 50.962
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.325   -0.905  -10.4457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.335   -0.92   -10.3498]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -48.38278086018154, time: 51.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.29    -0.925  -11.9643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.24    -0.86   -10.5955]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -52.26534643211318, time: 52.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.865   -0.925  -10.5895]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.755  -0.91  -10.95 ]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -46.15886157435203, time: 53.041
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.475   -0.955  -12.2345]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.505   -0.875  -12.0703]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -47.82205443704433, time: 51.376
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.775   -0.905  -10.4253]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.89   -0.9   -12.154]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -45.443065485445025, time: 51.437
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.755  -0.925 -10.691]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.185   -0.915  -12.7801]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -44.53857370511606, time: 51.519
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.865   -0.93   -10.5909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.21    -0.87   -10.9443]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -46.065324120290896, time: 52.06
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.455   -0.945  -11.5933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.815   -0.885  -12.8162]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -39.64279287449297, time: 52.247
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.025   -0.95   -11.2321]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.35    -0.915  -11.8561]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -42.21558397458917, time: 51.631
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.05    -0.895  -11.4703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.5     -0.96   -12.5441]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -49.50582014766341, time: 50.578
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.72    -0.835  -11.9888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.695   -0.925  -12.2245]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -51.58137019426305, time: 51.804
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.645   -0.77   -11.5246]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.64    -0.855  -12.9763]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -46.664409598213155, time: 52.245
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.08    -0.87   -12.2504]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.285   -0.835  -10.7115]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -52.46501519951469, time: 51.736
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.18    -0.94   -12.6083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.945   -0.815  -10.0103]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -42.41700440455848, time: 51.042
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.015   -0.915  -10.6558]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.14    -0.8     -9.3471]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -71.55939449818389, time: 52.236
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.26    -0.885  -11.7815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.39   -0.87  -11.072]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -46.03118703747534, time: 52.794
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.635   -0.965  -11.8849]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.455   -0.89   -10.7869]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -40.65384839444709, time: 52.129
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.59    -0.905  -10.7961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.04    -0.84   -11.7315]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -60.818706598427255, time: 51.707
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.36    -0.915  -11.7869]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.295   -0.87   -12.1834]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -48.473593047033646, time: 52.13
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.87    -0.975  -10.4299]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.77    -0.89   -13.7117]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -42.37187712903115, time: 52.058
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.105   -0.925  -10.5635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.01    -0.9    -12.0605]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -42.288544373522456, time: 51.531
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.08    -0.98   -11.3669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.77    -0.92   -12.0915]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -40.72344208277185, time: 51.695
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.07    -0.975   -9.9675]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.79    -0.885  -12.0319]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -57.03668936067463, time: 50.466
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.905   -0.75   -10.3625]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.405   -0.88   -14.2512]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -42.76906885523482, time: 50.646
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.94    -0.935  -10.3738]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.14    -0.855  -11.8303]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -45.99116282294511, time: 51.378
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.77    -0.955  -10.3721]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.365   -0.85   -13.1967]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -43.03067368520286, time: 51.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.98    -0.895   -9.7516]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.755  -0.76   -6.7749]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -55.76286635157115, time: 52.138
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.33    -1.     -12.8088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.11   -0.55   -5.3862]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -55.31307135827037, time: 52.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.1     -0.985  -11.8253]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.445  -0.675  -5.2984]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -60.46036801470008, time: 52.664
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.085   -1.     -12.9266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.485   -0.76    -7.7169]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -61.915469557419954, time: 51.671
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.425   -1.     -12.8428]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.375   -0.74    -7.3836]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -55.05048737037073, time: 50.738
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.385   -0.985  -14.8287]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.265   -0.69    -7.0367]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -61.93520815665144, time: 51.655
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.205   -1.     -14.6337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.625  -0.585  -5.4662]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -56.672646763100985, time: 52.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.055   -0.975  -13.9465]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.995   -0.695   -8.0413]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -58.660001617208636, time: 51.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.155   -0.99   -13.9157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.645   -0.695   -7.6848]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -61.8179534471383, time: 51.573
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.3     -0.985  -14.9491]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.905   -0.765   -8.8343]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -60.12596817773487, time: 51.038
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.      -0.985  -13.8669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.91    -0.72    -9.2952]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -54.521197151547355, time: 50.88
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.52    -0.975  -12.1176]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.12   -0.765  -8.431]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -58.775067506210014, time: 52.281
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.19    -0.995  -12.0941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.78    -0.755   -7.6479]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -54.642873867878116, time: 52.399
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.975   -0.965  -12.9135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.91    -0.86   -10.6049]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -56.72667182167464, time: 52.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.885   -1.     -12.7101]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.785   -0.8     -9.7351]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -66.85155217035073, time: 51.929
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.975   -0.995  -14.0441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.635   -0.82    -9.2429]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -48.91382200231196, time: 51.571
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.28    -1.     -12.2051]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.39    -0.865   -8.2307]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -54.606889750574155, time: 51.974
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.535   -0.995  -12.4667]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.695   -0.87    -9.7636]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -53.969367547860095, time: 51.257
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.8     -1.     -11.3682]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.67    -0.925  -11.8195]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -54.33865275514349, time: 51.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.685   -0.995  -11.8165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.465   -0.99   -14.7765]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -52.75854949736126, time: 52.821
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.185   -0.995  -11.1029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.45    -0.975  -12.1733]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -53.861468366620244, time: 51.574
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.93    -0.98   -10.9146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.905   -0.985  -12.3297]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -54.255498931602105, time: 51.841
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.21    -0.995  -11.3823]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.155   -0.94   -11.3622]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -55.31566711184729, time: 51.795
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.91    -0.995  -11.3643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.97    -0.955  -11.8138]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -58.47050585745338, time: 51.588
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.475   -0.99   -11.3221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.74    -0.95   -10.9405]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -52.52475975417207, time: 52.205
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.395   -0.99   -11.1583]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.025   -0.955  -10.5764]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -53.39262986548413, time: 51.644
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.95    -0.96   -10.4316]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.715  -0.925 -10.1  ]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -54.578748771324165, time: 53.543
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.28    -0.965  -10.9093]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.69   -0.96  -11.511]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -57.400098303785455, time: 51.625
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.975   -0.995  -12.5779]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.885   -0.925  -11.3101]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -59.553898794132095, time: 52.057
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.315   -0.975  -12.9732]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.54   -0.94  -14.031]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -53.8033148695073, time: 52.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.015   -0.915  -11.3807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.44    -0.895  -11.3482]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -52.4522385484505, time: 53.064
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.1     -0.985  -14.0222]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.405   -1.     -25.2687]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.775   -0.935  -25.3178]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -61.12204220028039, time: 52.106
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.24    -0.99   -24.8065]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.66    -0.92   -25.2142]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -57.28235300097897, time: 52.878
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.84    -0.995  -23.6343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.805   -0.94   -25.3602]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -54.12664662483763, time: 51.367
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.295   -0.995  -21.8541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.685   -0.9    -25.2653]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -54.16681889579395, time: 52.143
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.355   -1.     -23.3457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.825   -0.97   -25.3697]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -60.54963325267299, time: 51.863
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.02    -1.     -23.3815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.725   -0.95   -25.2982]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -60.63160866868617, time: 52.883
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.725   -1.     -24.6289]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.79    -0.96   -25.3435]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -61.401438216352055, time: 53.667
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.475   -0.915  -24.8103]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.015   -0.97   -24.8728]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -54.8940450455269, time: 51.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.175  -0.925 -22.335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.56    -0.98   -25.2039]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -50.62954384929917, time: 51.855
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.93   -0.98  -23.839]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.825   -0.975  -23.8434]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -59.57713504995308, time: 52.117
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.945   -0.97   -23.7945]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.18    -0.985  -24.7233]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -60.11586786084396, time: 51.638
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.95    -0.745  -22.4266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.67    -0.95   -24.5656]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -67.84997715080416, time: 52.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.67    -0.89   -23.8327]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.09    -0.95   -25.0301]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -57.93776676820925, time: 52.984
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.655   -0.97   -23.8221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.7     -0.985  -25.3206]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -53.12030048586017, time: 51.974
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.27    -0.935  -23.5285]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.815   -0.985  -25.3683]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -53.26711447042158, time: 52.24
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.725   -0.96   -23.7908]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.86    -0.995  -25.4254]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -63.8906311115167, time: 51.893
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.165   -0.975  -21.7162]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.855   -1.     -25.4322]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -59.86675855988586, time: 52.67
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.67    -0.995  -21.1133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.58    -0.995  -25.2811]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -60.26639752705543, time: 52.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.62   -0.995 -20.723]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.485   -0.995  -25.1914]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -62.88986480049763, time: 52.668
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.555   -1.     -21.5691]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.63    -1.     -25.2701]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -55.00189485836012, time: 53.688
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.09    -1.     -22.4393]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.75    -1.     -24.8223]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -51.98636087360841, time: 52.911
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.975   -0.935  -19.6199]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.935  -1.    -24.762]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -58.724070534034496, time: 52.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.93    -1.     -21.0707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.14    -0.99   -24.8985]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -52.91521725303017, time: 52.527
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.655   -0.995  -21.1957]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.74    -0.995  -25.3056]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -58.66038562765302, time: 51.852
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.96    -0.995  -21.8943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.505   -0.985  -25.1229]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -77.431637824178, time: 52.199
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.28    -0.845  -18.3041]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.605   -0.815  -22.3686]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -59.021505853087284, time: 52.239
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.94    -0.915  -17.4856]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.495   -0.975  -25.1611]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -55.32299832259323, time: 51.849
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.895  -0.98  -22.385]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.75    -0.995  -25.3657]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -58.305370716146314, time: 51.881
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.97    -0.995  -20.4941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.665   -0.96   -23.2105]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -51.30176538209846, time: 51.871
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.77    -0.985  -21.3352]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.235  -0.995 -18.161]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -54.96559960361999, time: 52.391
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.7     -0.995  -20.4591]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.315   -0.96   -20.8052]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -57.613315190566155, time: 52.007
19000 50
steps: 949950, episodes: 19000, mean episode reward: -46.12265445448404, time: 51.636
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.255  -0.655  -7.4174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.535   -0.985  -10.1643]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -42.437205190820535, time: 52.49
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.43   -0.585  -6.4592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.495   -0.99    -9.5079]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -53.68063127170269, time: 52.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.94   -0.665  -7.6591]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.715   -0.985  -10.1378]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -48.00663954521804, time: 51.377
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.68   -0.685  -6.8146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.685   -0.965  -10.2547]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -50.73560507004501, time: 51.116
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.53   -0.695  -7.5001]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.395   -0.98   -10.5977]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -47.82058004426124, time: 51.741
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.855   -0.685   -8.0242]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.945   -0.97   -10.3644]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -50.8264701090655, time: 52.378
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.795   -0.75    -8.2729]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.84    -0.97   -10.8259]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -55.49916737943294, time: 52.335
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.73    -0.81    -8.0191]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.245   -0.97   -10.7185]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -50.56497734553086, time: 51.82
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.565   -0.715   -8.9269]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.915   -0.96   -11.2902]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -53.17141004110662, time: 51.629
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.97    -0.76    -9.4566]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.54    -0.94   -10.9934]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -46.376065176222966, time: 52.034
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.935   -0.795   -9.4721]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.97   -0.935 -10.264]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -51.97538753466744, time: 52.708
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.89    -0.82    -8.9175]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.015   -0.945  -10.8298]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -49.733542170519534, time: 52.117
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.58    -0.815   -8.6652]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.38    -0.985  -10.3339]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -52.914029385322465, time: 51.753
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.66    -0.86   -10.0036]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.87    -0.97   -10.1846]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -47.397972286436655, time: 52.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.845   -0.89   -13.2114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.525   -0.985  -10.0394]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -51.41857149523031, time: 51.642
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.03    -0.885  -11.3274]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.92    -0.985  -10.0816]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -58.47240479514084, time: 52.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.955   -0.915  -13.2259]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.25    -0.97   -10.7579]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -57.161689662503086, time: 52.399
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.105   -0.935  -13.3707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.89    -0.915   -9.9909]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -53.29039388095705, time: 51.942
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.845   -0.905  -13.1877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.98    -0.99   -10.2185]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -53.350944932488254, time: 52.499
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.545  -0.91  -13.73 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.075   -0.97   -11.0188]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -54.187263486872034, time: 51.386
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.75    -0.85   -11.6883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.785   -0.945  -10.2986]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -55.8759787007182, time: 52.723
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.47    -0.94   -13.3849]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.92    -0.935  -10.1772]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -58.42374902447847, time: 51.716
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.245  -0.875 -12.395]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.505   -0.965  -11.3644]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -48.488807438737346, time: 51.624
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.35    -0.765   -9.4255]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.455  -0.94   -8.364]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -46.583424992350245, time: 52.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.1     -0.84   -10.2078]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.535  -0.945  -9.88 ]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -50.80514130098683, time: 52.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.92    -0.72    -8.9023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.595   -0.89    -9.4426]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -46.82217824353225, time: 51.712
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.675  -0.84   -9.7  ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.79    -0.97    -9.7353]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -49.87173358097939, time: 52.082
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.675   -0.815   -8.2102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.825   -0.97    -9.5881]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -52.41780507701411, time: 52.26
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.54    -0.725   -8.7732]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.2     -0.975   -9.1481]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -52.59413683946148, time: 51.416
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.395   -0.88   -10.2181]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.86    -0.91    -9.5364]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -48.19341368279982, time: 52.122
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.03    -0.85   -10.3105]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.46    -0.875   -9.9435]
[-49.51    -0.995  -25.1524]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.74    -1.     -25.3123]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -39.978745138608694, time: 53.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.41    -0.995  -25.1126]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.37    -1.     -25.0676]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -42.5182024523029, time: 52.57
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.55    -1.     -25.2338]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.48    -1.     -25.1406]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -37.72463827235866, time: 53.515
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.77    -1.     -24.8253]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.685   -1.     -25.2845]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -39.9527947823035, time: 52.959
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.11    -1.     -25.0485]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.76    -1.     -25.3234]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -36.23510936309824, time: 52.804
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.16    -1.     -24.2942]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.765   -1.     -25.3333]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -30.1770644276002, time: 52.714
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.135   -1.     -25.0591]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.92    -1.     -25.4399]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -37.7117587963132, time: 53.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.06    -1.     -24.3702]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.545   -1.     -25.1743]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -44.8142169739776, time: 52.662
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.65    -1.     -25.2905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.635   -1.     -25.2417]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -36.02573046979919, time: 52.636
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.815   -1.     -25.3906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.69    -1.     -25.2663]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -42.5862638819305, time: 51.673
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.06    -0.995  -24.9134]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.585   -1.     -25.1955]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -38.14294704203823, time: 52.221
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.64    -1.     -25.2617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.77    -1.     -25.3432]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -40.820729933501774, time: 53.087
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.57    -0.995  -24.4411]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.885   -1.     -25.4184]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -43.95180731406801, time: 52.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.36    -1.     -24.5499]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.665  -1.    -25.265]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -30.704165230387144, time: 52.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.26    -1.     -25.0002]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.83    -1.     -25.3734]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -33.72799023649683, time: 52.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.715   -1.     -25.3304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.89    -0.995  -25.4204]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -41.58845070602696, time: 52.153
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.48    -1.     -25.1683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.665   -1.     -25.3098]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -37.112472880701645, time: 52.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.12    -0.995  -23.3783]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -1.     -25.2624]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -35.94742556840145, time: 53.587
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.975   -0.995  -24.8353]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.85    -1.     -25.3788]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -40.272199358086, time: 52.16
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.445   -0.995  -24.6508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.715   -1.     -25.2829]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -37.70379622065774, time: 53.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.715   -0.995  -23.1764]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.4     -1.     -25.0663]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -35.10269696209388, time: 52.678
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.045   -1.     -22.3184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.105   -0.99   -24.7835]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -35.329881916063634, time: 53.074
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.135   -1.     -25.1337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.495   -1.     -25.1187]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -39.316269608963125, time: 52.432
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.13    -1.     -25.0068]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.045  -1.    -24.74 ]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -31.96950592467615, time: 52.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.92    -1.     -24.0933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.685   -1.     -25.2675]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -38.518444783750915, time: 52.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.3     -1.     -25.0877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595   -1.     -25.2118]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -34.729190510382615, time: 52.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.94    -1.     -24.9642]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.73    -1.     -25.3003]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -33.22645055428998, time: 52.723
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.845   -1.     -25.4215]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.67    -0.995  -25.2962]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -30.20546368086727, time: 52.003
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.685   -1.     -25.3356]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.78    -1.     -25.3643]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -29.330705530695095, time: 51.978
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.465   -0.99   -24.5264]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.625   -0.99   -25.2468]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -31.63470237672506, time: 52.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.195   -1.     -24.4599]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.795   -0.995  -25.3635]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -36.23730335537701, time: 52.169
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.275   -0.98   -22.5484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.345   -0.9    -19.9441]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -54.22969895999719, time: 50.652
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.75    -0.93   -22.5555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.895   -0.69   -17.8671]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -50.99304606188335, time: 52.277
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.365   -0.94   -20.6301]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.065   -0.755  -21.6396]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -49.202110268805136, time: 50.549
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.395   -0.92   -20.3466]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.57    -0.785  -19.9928]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -48.55281293625053, time: 50.725
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.12    -0.935  -19.8169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.155  -0.78  -20.275]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -47.3600287406766, time: 50.63
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.85    -0.995  -21.4176]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.23    -0.915  -21.2628]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -51.24274332170571, time: 51.887
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.84    -0.975  -22.2331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.      -0.785  -17.2886]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -49.11465079647156, time: 50.174
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.14    -0.995  -22.5723]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.83    -0.705  -19.8102]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -46.83833193196403, time: 50.928
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.1   -0.97 -22.29]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.985   -0.79   -21.7092]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -44.74745233854767, time: 51.188
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.665   -0.905  -21.2731]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.12    -0.775  -19.1301]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -50.041068626277855, time: 50.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.255   -0.94   -22.2173]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.805   -0.79   -18.3554]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -45.47345001235547, time: 51.928
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.26    -0.96   -23.3082]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.02    -0.775  -19.7319]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -48.693185020073216, time: 50.869
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.685   -0.915  -21.4023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.19    -0.885  -19.4075]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -43.12133992834524, time: 50.689
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.825   -0.92   -22.3259]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.89    -0.975  -18.3485]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -49.0773269683732, time: 50.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.275   -0.955  -22.5156]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.595   -0.97   -18.8972]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -50.319305023667056, time: 51.613
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.165   -0.82   -22.8308]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.27    -0.925  -20.1003]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -45.39695998034604, time: 50.631
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.405   -0.99   -22.9795]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.625   -0.605  -14.1386]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -60.34386833873444, time: 51.518
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.345   -0.915  -22.1807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.26    -0.755  -15.7023]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -50.35484325672159, time: 51.883
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.855   -0.945  -23.1369]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.66    -0.885  -16.5645]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -41.93598228496807, time: 50.27
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.375   -0.84   -21.8269]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.77    -0.985  -19.9018]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -59.82404745680651, time: 51.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.925   -0.965  -22.6225]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.03    -0.99   -18.6357]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -50.677448471550306, time: 50.926
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.765   -0.985  -22.0974]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.33    -0.915  -17.3375]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -56.70229534900094, time: 52.018
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.215   -0.985  -21.5779]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.88   -0.795 -14.727]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -52.44641730759824, time: 51.151
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.09    -0.995  -22.2052]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.9     -0.885  -18.3124]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -42.88162991413408, time: 50.961
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.415   -0.995  -22.9401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.17    -0.94   -18.1165]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -44.92889357227754, time: 51.103
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.16    -0.95   -22.4488]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.235   -0.98   -17.3865]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -44.17868592838214, time: 50.153
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.67    -0.93   -22.6313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.785   -0.995  -16.2848]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -57.34281990224304, time: 50.937
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.29    -0.865  -21.8316]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.89    -0.845  -17.3619]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -55.518485935714764, time: 51.246
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.665   -0.87   -22.5698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.51    -0.925  -17.7889]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -43.47510597220379, time: 50.946
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.055   -0.895  -22.9211]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.035   -1.     -18.5633]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -53.92178062093744, time: 50.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.29    -0.725  -21.0742]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.79    -0.985  -18.9397]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -48.954791942165556, time: 51.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.045  -0.875 -24.038]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.865   -0.485   -7.1545]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -74.89486600952313, time: 51.984
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.055   -0.895  -24.0555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.625   -0.455   -6.7682]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -64.47319142604378, time: 51.868
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.63    -0.88   -24.3951]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.255   -0.5     -6.4511]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -65.44362576566644, time: 51.721
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.575  -0.9   -24.335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.42  -0.48  -6.6 ]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -68.00883568716185, time: 51.685
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.65    -0.895  -24.4379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.515  -0.53   -7.293]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -67.67527502232802, time: 51.721
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.19    -0.93   -24.1073]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.495   -0.535   -6.9945]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -72.08965134994305, time: 52.708
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.585   -0.89   -24.4061]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.86    -0.55    -6.6246]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -74.2136684385898, time: 51.666
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.87    -0.925  -24.6408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.435  -0.565  -7.405]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -69.05494750687413, time: 52.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.06    -0.87   -24.7167]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.71    -0.46    -6.6239]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -76.63919633918907, time: 51.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.035   -0.9    -24.8323]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.185   -0.42    -7.1726]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -75.61072165991033, time: 50.476
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.915   -0.895  -24.6555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.955  -0.5    -5.7023]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -73.77736066910892, time: 52.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.92    -0.92   -24.7539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.635  -0.47   -6.0531]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -77.36148422660125, time: 52.56
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.885  -0.91  -24.693]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.12   -0.495  -6.152]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -72.64239309720551, time: 51.6
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.63    -0.885  -24.3765]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.535  -0.465  -6.98 ]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -66.07181120416699, time: 51.626
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.585   -0.935  -24.3714]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.165   -0.535   -7.8288]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -77.46112092563752, time: 51.711
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.04    -0.945  -24.7761]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.155   -0.55    -9.0388]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -70.98364440050773, time: 51.785
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.92    -0.91   -24.6376]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.02    -0.6     -9.4905]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -72.4451864468642, time: 52.191
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.25    -0.92   -24.1444]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.38    -0.535   -7.7152]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -66.11500514043607, time: 51.702
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.79    -0.92   -24.5473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.675   -0.64    -9.3855]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -68.55611662084085, time: 51.798
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.975   -0.92   -24.7034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.62    -0.675   -8.2095]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -61.80599825182145, time: 51.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.5     -0.995  -25.1588]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.57    -0.63    -7.1261]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -63.67222316041826, time: 51.777
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.675  -1.    -25.266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.49   -0.57   -6.3433]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -69.71707838378317, time: 52.123
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.92    -0.995  -25.4512]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.295   -0.685   -9.5268]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -67.38433978029957, time: 51.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.51    -0.99   -25.1599]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.79    -0.66    -8.7991]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -66.88231884907894, time: 51.748
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.615   -0.995  -25.2159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.035   -0.65    -9.4492]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -68.71782404505372, time: 51.013
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.245   -0.995  -24.9252]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.12   -0.615  -8.156]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -65.5547522579486, time: 51.04
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.62    -0.99   -23.5366]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.555   -0.595   -7.5258]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -62.80648162255804, time: 52.83
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.895   -1.     -24.0142]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.96    -0.64    -8.4165]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -63.415237812858834, time: 51.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.525   -0.995  -24.1464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.81   -0.65   -8.101]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -59.73946427432732, time: 51.759
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.565   -1.     -23.5501]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.105   -0.745   -7.4136]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -88.06073690327204, time: 51.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.39    -1.     -24.5486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.57    -0.805   -9.6108]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -63.979595108785524, time: 51.196
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.72    -0.765  -15.9948]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.62    -0.955  -25.2155]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -51.58481897543302, time: 52.863
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.28    -0.69   -15.2124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.72  -0.95 -25.3 ]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -55.018732236442794, time: 53.163
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.695   -0.79   -16.6719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.625  -0.9   -25.182]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -55.68521817427385, time: 52.558
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.735   -0.865  -17.8086]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.455   -0.88   -25.0546]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -57.32633131759492, time: 53.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.82    -0.815  -16.5042]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.525   -0.93   -25.1366]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -56.95009414479483, time: 53.169
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.175   -0.925  -18.8476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.665   -0.915  -25.2192]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -54.47957851608883, time: 53.718
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.13    -0.81   -15.9054]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.63    -0.895  -25.1682]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -54.26299755781989, time: 52.502
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.605   -0.835  -16.9138]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.545   -0.935  -25.1431]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -50.67894088729941, time: 52.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.86    -0.845  -16.4372]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.375   -0.97   -25.0185]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -56.25851924264942, time: 52.628
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.795   -0.72   -15.5062]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.39    -0.9    -25.0481]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -51.979902730318734, time: 52.219
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.19    -0.745  -15.2901]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.53   -0.915 -25.118]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -50.673144478920534, time: 53.316
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.42    -0.785  -16.1615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.925   -0.91   -24.6884]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -56.43196153975622, time: 53.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.285  -0.745 -15.507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.325   -0.9    -25.0119]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -55.553118524558556, time: 53.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.76    -0.815  -15.5848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.295   -0.94   -25.0284]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -60.147009361145194, time: 52.544
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.66   -0.875 -15.705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.515   -0.935  -25.1334]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -48.59461870298866, time: 52.428
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.3     -0.89   -16.8192]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.46    -0.93   -25.0719]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -53.55078960270131, time: 52.861
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.39   -0.855 -16.772]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.525   -0.965  -25.1225]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -50.480048879194584, time: 53.192
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.945   -0.91   -17.5093]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.505   -0.965  -23.9222]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -50.326934072407575, time: 53.714
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.475   -0.965  -17.2121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.79    -0.925  -23.5495]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -57.171252564518, time: 53.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.95    -0.955  -17.3527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.89    -0.76   -21.6189]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -63.64811345998894, time: 52.526
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.83   -0.95  -17.984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.255   -0.765  -21.4061]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -53.53547114191893, time: 52.503
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.975  -0.99  -18.732]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.035   -0.84   -23.1971]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -51.34435679955621, time: 53.386
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.33    -0.975  -17.5545]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.225   -0.825  -15.2286]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -52.43719618915533, time: 52.474
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.58    -0.92   -15.6782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.615   -0.79   -18.5483]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -48.79398030288099, time: 52.361
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.545  -0.91  -15.01 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.42    -0.955  -16.7784]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -47.537786498177326, time: 51.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.52    -0.985  -17.0896]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.695   -1.     -14.9456]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -49.546018018547585, time: 51.974
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.28    -0.97   -15.7338]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.645   -0.95   -14.2352]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -55.17235635028933, time: 52.969
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.535   -0.89   -13.9977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.58    -0.94   -10.1622]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -44.545439788720905, time: 52.082
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.47    -0.995  -15.4237]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.755   -0.97   -10.3249]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -49.06786992883629, time: 53.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.925   -0.96   -16.4184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.145   -0.98   -11.9449]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -50.45244137851982, time: 52.733
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.605   -0.91   -16.3214]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.5     -0.955  -10.4955]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -51.24543892922409, time: 53.205
agent0_energy_min, agent0_energy_max, agent0_energy_avg
18800 50
steps: 939950, episodes: 18800, mean episode reward: -37.9056056910455, time: 53.508
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.81    -0.98   -22.5109]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.735   -1.     -14.3351]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -39.80286313405074, time: 54.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.27    -1.     -20.7356]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.1     -1.     -15.3484]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -39.9646097287675, time: 53.639
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.335   -1.     -21.1363]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.535   -1.     -17.3515]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -42.22770744796648, time: 52.223
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.08    -0.99   -21.6092]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.025   -1.     -19.1143]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -47.256314783500194, time: 54.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.71    -0.98   -22.2271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.8     -1.     -18.5174]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -34.38743977189417, time: 53.85
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.195   -0.98   -23.5237]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.78   -1.    -16.034]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -38.75629784516676, time: 52.969
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.78    -0.995  -21.9065]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.45    -1.     -14.7644]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -41.11531848423059, time: 53.413
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.41   -0.99  -21.529]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.415   -0.985  -13.7731]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -39.984121708057536, time: 52.479
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.835   -0.995  -19.0506]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.17    -0.935  -13.9421]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -38.027848045580406, time: 52.45
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.02    -0.985  -19.7808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.665   -1.     -13.2353]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -40.094981591393186, time: 53.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.835   -0.975  -19.6462]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.33    -1.     -14.1429]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -38.50802005973391, time: 53.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.3    -0.98  -21.746]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.155   -0.995  -13.0949]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -43.329502082977655, time: 54.721
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.09    -1.     -21.3044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.79    -0.995  -13.6583]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -39.40969053621723, time: 53.237
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.905   -0.985  -21.1528]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.32    -0.975  -11.9544]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -47.80597238001724, time: 52.936
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.83    -0.89   -20.1973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.005  -0.995 -11.518]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -38.51218476148408, time: 53.078
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.92    -0.925  -20.7041]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.29   -0.82  -12.742]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -42.80535048169776, time: 53.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.045   -0.96   -20.7603]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.57    -0.86   -11.5369]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -51.15853276980922, time: 54.564
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.28    -0.925  -19.4399]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.635   -0.93   -12.4311]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -49.452467278312234, time: 53.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.66   -1.    -21.025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.025   -0.76   -10.6523]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -59.25300050409015, time: 53.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.265   -0.81   -18.2683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.585   -0.84   -11.6451]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -62.90249747136204, time: 53.562
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.915   -0.875  -20.6239]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.73    -0.755   -8.8821]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -42.94170507495677, time: 52.599
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.425   -1.     -22.5004]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.205   -0.85    -8.8058]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -56.52591878692494, time: 52.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.57    -0.995  -20.8477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.065   -0.835   -8.7114]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -59.09829384151136, time: 52.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.465   -0.795  -18.3332]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.305   -0.93   -10.8401]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -44.03331735447293, time: 53.249
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.49    -0.925  -19.6006]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.615   -0.775  -15.5917]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -48.32351501881689, time: 52.148
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.12    -0.95   -21.5484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.69    -0.86   -12.5161]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -44.30481746888603, time: 51.676
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.995  -0.995 -22.453]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.69    -0.885  -11.8748]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -48.465949763644474, time: 52.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.795  -1.    -22.087]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.56    -0.925  -10.7679]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -54.97600473904436, time: 53.401
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.09    -1.     -21.8852]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.905   -0.925  -11.2195]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -43.12134045162403, time: 51.723
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.94   -1.    -21.838]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.225   -0.92   -10.8339]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -41.3133445963529, time: 51.555
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.815   -1.     -22.4722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.565   -0.95   -12.4937]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.835   -0.92   -21.9366]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.28    -0.99   -25.0056]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -47.835078763826814, time: 53.374
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.135   -0.945  -20.1424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.44    -0.98   -25.1546]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -44.55522215636488, time: 52.48
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.      -0.955  -20.0035]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.58    -0.98   -25.2151]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -46.16337525471783, time: 53.428
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.19    -0.985  -19.9551]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.705  -1.    -25.314]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -50.89501131049517, time: 52.711
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.84    -0.98   -20.4169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.695   -1.     -25.3033]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -48.89921080141334, time: 52.177
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.61    -0.975  -19.9885]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.635   -0.98   -25.2802]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -46.319152910848885, time: 51.62
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.485   -0.985  -20.0638]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.075   -0.94   -24.3067]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -50.026854112173424, time: 52.184
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.63    -0.955  -18.0708]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.38    -0.97   -20.5669]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -47.782538828350226, time: 51.315
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.38    -0.96   -18.9009]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.395   -0.975  -23.1433]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -57.89360012519487, time: 51.791
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.91    -0.93   -17.5908]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.45    -0.995  -22.3597]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -45.95875528654722, time: 52.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.685   -0.895  -18.1434]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.42    -0.975  -21.8586]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -49.857557150687086, time: 52.523
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.065   -0.935  -18.3955]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.76    -0.96   -22.1392]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -46.73073806481556, time: 52.842
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.26    -0.955  -18.8577]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.2    -0.985 -22.399]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -50.041758151788684, time: 51.663
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.175   -0.985  -18.9107]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.855   -0.955  -21.4725]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -43.513204514285235, time: 51.316
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.58    -0.975  -18.4026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.38    -0.995  -21.5952]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -53.565010319428126, time: 51.283
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.88    -1.     -19.9844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.655   -1.     -21.1759]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -46.874073917258954, time: 51.306
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.545   -0.96   -18.9712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.605   -1.     -20.6375]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -37.81673033704795, time: 52.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.775   -0.985  -17.8046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.035  -0.98  -21.054]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -46.92050498132175, time: 51.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.49    -0.99   -18.5165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.175   -0.965  -21.8667]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -46.76248009490575, time: 51.883
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.745   -0.965  -18.2813]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.025   -0.985  -21.4148]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -47.858873990922554, time: 50.811
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.645   -0.955  -17.2953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.045   -0.96   -21.7405]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -44.48304295873937, time: 51.645
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.375   -0.855  -16.6005]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.515   -0.975  -21.7824]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -46.667323330662775, time: 51.921
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.565   -0.96   -19.8653]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.915   -0.94   -20.7311]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -56.525565877954435, time: 51.566
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.69    -0.905  -18.0701]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.55    -0.95   -22.0373]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -46.196780123751, time: 50.504
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.965   -0.93   -18.3083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.88    -0.995  -22.1106]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -46.82420403977898, time: 49.309
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.42    -0.985  -18.6665]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.905   -1.     -21.9502]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -46.18411294021107, time: 50.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.3     -0.955  -17.1929]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.975   -0.98   -20.8655]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -41.619104096884115, time: 51.458
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.58    -0.985  -19.0937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.26    -0.99   -20.9595]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -40.33616176795232, time: 50.4
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.645   -0.98   -19.7174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.83    -0.98   -21.4483]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -44.13607356385188, time: 50.771
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.565   -0.99   -19.4663]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.55    -0.925  -21.8791]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -45.95685216496746, time: 51.49
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.08    -0.99   -19.9807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.995   -0.975  -22.6113]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -44.516976040206266, time: 52.857
[-17.735   -0.82   -12.4879]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -57.65096811064576, time: 52.131
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.075   -0.805  -10.1862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.205   -0.825  -10.3931]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -52.93495750373455, time: 51.953
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.605   -0.74    -9.6386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.73    -0.905  -12.1369]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -59.49337737323702, time: 52.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.85    -0.965  -11.4581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.44    -0.86   -10.6226]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -50.45764625656541, time: 51.853
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.98    -0.995  -11.6544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.015   -0.91   -10.6029]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -59.29713823392322, time: 51.526
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.695   -0.995  -12.2467]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.405   -0.805   -9.9749]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -48.16740372425026, time: 52.693
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.415   -0.94   -11.2586]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.595   -0.765   -9.3475]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -47.49869165773905, time: 51.801
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.485   -0.95   -10.8778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.345   -0.82    -9.9827]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -52.6152176161989, time: 52.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.6     -0.915  -10.0955]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.98    -0.825   -9.9399]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -60.69765396507974, time: 52.627
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.485   -0.885  -11.2551]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.68    -0.85   -13.1097]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -57.28804304780032, time: 52.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.975   -0.805  -11.4611]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.405   -0.83   -12.5109]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -43.82726173849833, time: 52.435
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.77    -0.96   -11.6507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.595   -0.875  -10.8275]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -44.01938940807152, time: 51.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.895   -0.885   -9.7008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.43    -0.9    -11.6764]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -50.81935828586071, time: 51.59
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.96    -0.875  -11.0473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.895   -0.905  -12.6605]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -51.96104889571468, time: 52.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.61    -0.825  -10.6815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.465  -0.875 -11.499]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -42.568489607700485, time: 51.711
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.06    -0.935  -11.4705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.47   -0.89  -12.089]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -54.39205535724621, time: 52.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.7     -0.915  -10.8252]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.765   -0.885  -14.1987]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -45.41576843373698, time: 51.763
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.19    -0.955  -11.1788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.865   -0.92   -13.0017]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -44.219945421288614, time: 53.486
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.34    -0.915  -11.0652]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.93    -0.885  -11.2171]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -60.63291847271371, time: 51.986
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.405   -0.88   -11.1589]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.02    -0.765  -11.3127]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -45.65450681582181, time: 52.096
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.115   -0.935  -11.6292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.95    -0.93   -11.7058]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -47.47550385596738, time: 52.626
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.52    -0.96   -11.0381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.405   -0.965  -11.8497]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -51.500938294846236, time: 52.218
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.69    -0.935  -11.0287]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.875   -0.89   -11.7662]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -42.35685102649193, time: 52.273
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.43    -0.91   -10.7898]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.865   -0.825  -11.3148]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -44.847240313321045, time: 51.86
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.38    -0.945  -11.7498]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.41    -0.89   -12.6337]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -45.10983631612598, time: 52.325
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.98    -0.935  -11.1683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.865   -0.89   -11.5314]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -51.11125261985724, time: 51.818
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.66    -0.96   -12.5617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.83    -0.83   -11.6356]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -42.003485063321826, time: 51.204
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.66    -0.97   -12.0102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.825   -0.9    -11.7775]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -43.32540612523884, time: 52.36
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.99    -0.945  -11.5915]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.245   -0.9    -13.2324]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -48.87843190983218, time: 51.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.325   -0.98   -10.0645]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.92    -0.935  -11.5596]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -42.88614644598909, time: 51.29
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.365   -0.995  -10.1088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.01    -0.915  -11.6718]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -54.442347146071334, time: 52.202
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.695   -1.     -12.2757]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.695   -0.945  -11.8599]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -57.27045057053202, time: 52.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.      -0.97   -13.4398]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.235   -0.945  -12.9325]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -61.6077710155353, time: 53.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.51    -0.99   -13.6452]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.495  -0.825 -11.758]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -63.96172698202608, time: 51.633
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.84    -0.97   -16.1198]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.98    -0.925  -12.4966]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -73.12253332655311, time: 52.455
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.5     -0.965  -15.7752]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.845   -0.79   -12.1014]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -58.47733823832612, time: 51.97
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.58    -0.94   -13.1753]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.55   -0.96  -16.287]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -62.627246643713676, time: 52.943
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.82    -0.99   -16.6851]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.73    -0.93   -14.6198]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -58.8563803485117, time: 51.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.67    -0.98   -13.4505]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.84    -0.88   -16.6418]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -59.43327795541197, time: 52.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.155   -0.995  -12.7975]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.82    -0.86   -13.2776]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -62.114217124707245, time: 52.818
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.705   -0.995  -14.4063]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.44    -0.865  -13.4695]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -56.0689502222394, time: 51.737
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.05    -0.995  -13.9159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.325   -0.875  -13.8261]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -55.502528626906354, time: 52.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.815   -0.99   -12.9921]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.45    -0.875  -12.8184]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -53.68472657297853, time: 51.366
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.03    -0.985  -11.6169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.155   -0.87   -12.4472]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -63.27925507721618, time: 52.392
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.335   -1.     -14.9273]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.325   -0.86   -10.2124]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -56.910576291073575, time: 51.856
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.42    -1.     -18.0818]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.81    -0.92   -10.9662]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -54.51827340559063, time: 52.173
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.86    -1.     -15.4413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.36    -0.875  -11.2183]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -54.74505693943136, time: 52.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.245   -0.995  -14.4788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.5     -0.92   -12.7081]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -56.283178573728485, time: 52.53
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.355   -1.     -15.8537]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.315   -0.905  -11.1787]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -52.44306857875807, time: 53.329
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.13    -1.     -13.5671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.42    -0.875   -9.2306]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -49.344738793823346, time: 52.32
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.25    -0.99   -12.9559]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.03    -0.86    -9.3912]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -56.25326793667155, time: 52.567
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.665  -0.97  -13.819]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.805   -0.89    -9.6909]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -74.03125386849513, time: 52.83
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.545   -0.82   -12.6626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.8     -0.905  -10.1445]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -68.72551342208706, time: 52.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.36    -0.96   -16.1438]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.755  -0.825 -10.097]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -83.24532324537476, time: 50.8
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.155   -1.     -18.1218]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.24    -0.87   -11.7341]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -63.23957010377898, time: 51.753
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.805   -1.     -22.1647]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.66    -0.775   -9.6321]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -50.84839400586216, time: 53.041
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.56    -0.995  -16.2481]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.96   -0.835  -9.055]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -64.46310570579566, time: 52.724
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.575   -1.     -17.3778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.235   -0.865  -10.2056]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -54.61941650386643, time: 51.848
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.23    -0.995  -15.1602]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.305   -0.845   -8.9277]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -50.901164748406856, time: 51.923
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.99    -1.     -13.8402]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.495   -0.855   -8.1267]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -59.995198902201786, time: 52.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.445  -1.    -14.037]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.83    -0.85    -8.9568]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -50.99100315000067, time: 51.923
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.925   -1.     -13.2253]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.22    -0.815   -9.7713]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -60.47120061818931, time: 52.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.925   -0.89   -10.8302]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.49    -1.     -21.5517]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.545   -0.925  -20.3958]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -56.61621622596694, time: 52.779
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.76    -1.     -20.3131]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.98   -0.89  -19.214]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -54.195001866357416, time: 53.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.05    -0.995  -21.8864]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.315   -0.83   -17.3872]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -50.66226969774439, time: 51.845
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.22    -0.96   -18.2568]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.195   -0.97   -20.0056]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -54.74219079501674, time: 52.668
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.185   -0.995  -17.8611]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.67    -1.     -17.2951]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -52.22467010518139, time: 52.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.34   -1.    -20.406]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.93    -0.89   -17.0876]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -53.93229964869153, time: 53.966
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.605   -0.99   -20.3961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.2     -1.     -18.3135]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -59.46081137662296, time: 52.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.195   -0.995  -17.1149]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.245   -1.     -15.7863]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -55.39879863651679, time: 52.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.2     -0.775  -13.6506]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.23    -1.     -15.8872]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -59.17243397542043, time: 51.963
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.19   -0.915 -21.342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.375   -0.99   -15.7727]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -67.7886154167054, time: 52.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.205   -0.995  -21.3052]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.41    -0.96   -17.3642]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -92.83050994030852, time: 52.41
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.      -0.98   -21.4657]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.43   -1.    -18.366]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -62.60004248464095, time: 52.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.3     -0.76   -15.0973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.455   -0.995  -15.9092]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -55.113342438736154, time: 52.966
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.065   -1.     -21.3105]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.15    -0.975  -14.7594]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -69.77117327428667, time: 52.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.36    -0.995  -19.5939]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.63    -0.94   -14.6616]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -55.74253751536705, time: 53.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.925   -0.96   -19.7601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.295   -0.96   -14.9995]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -55.312849424395274, time: 52.679
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.665   -1.     -22.0446]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.92    -0.97   -15.2461]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -68.07620791698616, time: 52.22
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.63    -0.99   -21.0189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.715   -0.88   -11.4413]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -48.68524089723405, time: 52.625
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.605   -0.985  -20.6612]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.07    -0.96   -12.1303]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -49.90016265305034, time: 51.632
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.48    -1.     -22.0741]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.25   -0.975 -12.193]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -53.847314408046174, time: 52.111
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.51    -1.     -22.4391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.025   -0.965  -10.0954]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -66.36204044014717, time: 53.859
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.07    -1.     -21.5018]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.01    -0.975  -10.2292]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -52.166820325108695, time: 52.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.115   -0.995  -22.7442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.675   -1.     -10.6828]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -48.9752375936592, time: 51.623
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.805   -1.     -22.8515]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.555   -0.945   -9.9694]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -42.97494846328197, time: 52.434
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.13   -1.    -23.258]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.18    -0.985   -9.6557]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -54.72803834067267, time: 52.996
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.64    -0.985  -23.1244]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.94    -0.935   -9.6688]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -49.15111392606677, time: 52.865
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.385   -0.995  -23.2484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.9     -0.99    -9.6707]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -74.71983292091977, time: 52.713
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.18    -1.     -22.7548]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.9    -0.65   -6.7274]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -68.31188943748994, time: 51.939
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.24    -1.     -22.2463]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.395   -0.915   -9.7753]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -52.27932825765589, time: 52.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.795   -0.955  -23.4506]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.2     -1.     -10.5862]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -44.67531170797431, time: 52.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.185   -0.99   -21.9883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.66    -0.99   -10.3651]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -49.959672771236946, time: 53.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.15  -0.98 -24.91]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.455   -0.92   -17.4261]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -45.92794773396095, time: 51.414
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.82    -0.93   -22.9012]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.9    -0.955 -16.059]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -55.85666768261164, time: 52.304
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.62    -0.93   -22.8702]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.32    -0.895  -15.1478]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -55.8388031913872, time: 51.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.6     -0.99   -22.4932]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.945   -1.     -16.6578]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -59.834432036187664, time: 51.713
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.82    -0.895  -21.8866]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.765   -0.995  -15.4523]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -45.632041461084874, time: 50.638
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.53    -0.755  -20.9758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.795   -0.995  -14.5541]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -45.287037010480944, time: 52.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.59    -0.985  -22.7251]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.305   -0.995  -14.8987]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -43.10423994166791, time: 51.6
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.615   -0.98   -22.2377]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.805  -0.955 -15.543]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -43.12035136688732, time: 51.729
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.475   -0.995  -22.0354]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.87    -1.     -15.5234]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -44.18516905770126, time: 50.971
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.725   -1.     -22.8083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.42    -1.     -17.9888]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -44.56150599779046, time: 51.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.865   -1.     -24.0181]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.295   -0.99   -15.8323]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -42.477404163474695, time: 52.089
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.365   -0.975  -23.2881]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.015   -0.925  -15.6665]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -40.003764589880305, time: 51.881
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.97    -0.88   -22.4955]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.69    -0.925  -14.6499]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -44.99888628526833, time: 51.898
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.3     -0.85   -22.2683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.305   -0.955  -15.3194]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -48.701550327734715, time: 51.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.345   -0.94   -22.8668]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.875  -0.98  -17.386]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -47.577859075690434, time: 52.05
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.785  -0.995 -23.351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.635   -0.975  -17.4037]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -42.18852844091449, time: 51.868
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.77    -0.96   -22.7385]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.35    -0.985  -15.4172]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -44.7008240227414, time: 51.548
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.185   -0.985  -23.8048]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.6     -0.985  -16.9148]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -48.110397260045865, time: 51.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.675   -0.975  -22.5905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.225   -0.97   -17.6326]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -43.7796807006841, time: 51.363
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.755   -1.     -23.9157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.71    -0.97   -17.4464]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -43.98450312031767, time: 50.934
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.865   -0.95   -22.4504]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.33    -0.98   -17.0379]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -47.448127037633455, time: 52.601
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.945   -0.95   -23.1189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.86    -0.97   -17.4368]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -47.69065963873254, time: 51.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.465   -0.8    -19.8197]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.62    -0.925  -16.4781]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -48.299176930694465, time: 50.843
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.37    -0.98   -22.0538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.025   -0.96   -14.8887]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -43.22361341815001, time: 51.034
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.05    -0.99   -23.0112]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.695   -1.     -18.8243]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -40.29882670518678, time: 51.986
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.51    -0.96   -23.9764]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.185   -0.995  -19.2154]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -55.930949497942585, time: 51.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.755   -0.805  -21.6804]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.155   -0.99   -18.0306]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -53.80705219459576, time: 51.676
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.095  -0.865 -22.599]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.195   -0.975  -15.8679]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -44.2050413830931, time: 51.118
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.865   -0.895  -20.7318]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.9     -1.     -16.6587]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -41.778548310350935, time: 51.329
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.445   -0.995  -24.6114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.29    -1.     -18.1104]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -41.49912827122658, time: 50.62
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.37   -0.995 -22.393]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.185  -0.985 -18.86 ]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -41.529571095077465, time: 52.707
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -49.17520349009483, time: 53.773
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.395   -0.92   -10.2888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.14    -0.93   -10.5717]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -62.54797851086226, time: 52.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.095   -0.935  -12.3723]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.29    -0.88    -9.2468]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -60.68665649343971, time: 51.028
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.23    -0.89   -10.0263]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.63    -0.965  -11.1587]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -67.16581241297135, time: 52.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.685  -0.725  -7.3289]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.45    -0.95   -10.3034]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -65.33743034296236, time: 51.782
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.025   -0.84    -9.0053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.73    -0.915  -10.4532]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -60.763948195793745, time: 52.157
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.74    -0.75    -8.0128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.805   -0.96    -9.4383]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -43.13099346463203, time: 52.18
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.34    -0.845   -9.5724]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.825   -0.965   -8.9481]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -59.765988234157994, time: 52.691
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.62    -0.865   -8.7927]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.76   -0.945 -10.138]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -59.58542837256742, time: 51.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.655   -0.755  -10.2608]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.92    -0.965  -10.2505]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -45.105559900765954, time: 51.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.5     -0.895   -9.0184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.43    -0.97    -9.3004]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -51.21601511250821, time: 52.06
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.345   -0.895   -9.5432]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.055   -0.98    -9.0037]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -60.51041717500111, time: 52.165
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.86    -0.76    -8.6978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.835   -0.965   -8.7508]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -56.946654484926874, time: 51.771
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.51    -0.845   -9.1659]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.69    -0.995   -9.6287]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -65.55249834782543, time: 52.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.78    -0.78    -7.9245]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.36    -0.85    -8.3441]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -48.0858132765725, time: 52.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.615   -0.87    -8.3078]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.885   -0.935   -8.9678]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -62.24253816838404, time: 54.061
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.66    -0.875  -11.4237]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.885   -0.945  -10.8572]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -45.607951615857786, time: 52.279
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.86    -0.84   -10.0031]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.14   -0.915  -9.724]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -50.05229139879442, time: 52.514
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.325   -0.625   -8.9394]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.405   -0.92    -9.9027]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -42.53596313456043, time: 52.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.32    -0.805   -9.5837]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.26    -0.8     -8.7174]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -59.935967749255326, time: 51.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.245   -0.77    -9.4694]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.65    -0.88    -9.4546]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -56.51334969081801, time: 53.224
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.57    -0.805  -11.0654]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.01    -0.94    -9.6808]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -55.79522255711673, time: 52.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.57    -0.84   -12.3204]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.635   -0.94    -8.5768]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -47.74538310660025, time: 51.92
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.275  -0.855  -9.88 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.05    -0.92    -9.7034]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -59.87662955041314, time: 51.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.185   -0.725  -10.6445]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.795   -0.885   -9.9381]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -55.05186764095451, time: 52.575
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.135   -0.86   -10.7221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.675   -0.74    -8.3274]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -61.215389420142166, time: 52.911
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.705   -0.815   -9.3888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.08    -0.89   -10.1625]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -48.306227791964105, time: 52.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.41    -0.87   -10.4082]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.67    -0.865   -8.3257]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -41.94505084939075, time: 51.777
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.025  -0.88  -10.179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.8     -0.82    -8.8449]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -66.42936987437969, time: 51.862
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.875   -0.93   -11.5989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.44    -0.895  -11.6721]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -48.305894169791465, time: 52.1
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.95    -0.68    -8.2956]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.235   -0.96   -11.5758]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -54.41443433520359, time: 52.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.18    -0.55    -9.7823]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.41    -0.995  -24.5131]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.68    -0.775   -8.3703]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -64.76951707945372, time: 53.63
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.605   -1.     -25.2294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.18    -0.755   -8.7142]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -70.32638039933366, time: 51.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.06    -1.     -24.8393]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.515  -0.71   -7.0076]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -75.16593858590996, time: 51.941
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.76    -0.79   -22.4859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.87    -0.72    -7.5447]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -64.95773612350095, time: 51.52
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.6     -0.935  -23.8301]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.01    -0.75    -7.3585]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -60.43467830221303, time: 52.198
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.155   -0.975  -24.1781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.83    -0.77    -8.8085]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -71.5715351504486, time: 53.01
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.89    -0.985  -24.7146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.655   -0.68    -7.9299]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -70.2055955002095, time: 52.629
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.29    -0.96   -24.9173]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.03    -0.825   -9.0226]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -71.86470048402704, time: 52.406
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.885   -0.84   -23.7033]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.04    -0.8     -8.8832]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -76.62082285937655, time: 51.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.115   -0.615  -18.6459]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.82    -0.73    -8.6656]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -68.682358264645, time: 51.042
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.935   -0.935  -22.7199]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.115  -0.8    -9.552]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -60.625929497318005, time: 52.548
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.73    -0.93   -24.7764]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.265   -0.925   -9.8502]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -63.95906385582802, time: 51.81
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.695   -0.925  -25.2365]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.535   -0.885  -10.2891]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -79.06607929244616, time: 51.806
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.89    -0.99   -25.4387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.48    -0.875  -11.6465]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -59.08855017902393, time: 52.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.61  -0.88 -23.36]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.015   -0.79    -8.9636]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -64.46989650248538, time: 52.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.395   -0.805  -21.0355]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.595   -0.885  -13.1515]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -62.69736763329429, time: 52.468
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.355   -0.945  -23.3298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.84    -0.845  -11.1825]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -66.15225729115792, time: 52.117
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.415   -0.995  -25.1836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.34    -0.745   -8.8713]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -62.33161382103599, time: 52.501
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.46    -0.785  -24.0816]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.51    -0.815   -9.0985]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -56.90896862664429, time: 51.929
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.745   -0.96   -21.1656]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.11    -0.805  -11.3915]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -59.71574418196717, time: 51.973
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.795   -0.875  -18.4555]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.76    -0.695   -9.4293]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -61.548224106565414, time: 52.738
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.24    -0.705  -15.4894]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.085   -0.805  -10.4119]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -79.23268798075932, time: 51.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.54   -0.845 -17.598]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.38    -0.72    -8.1982]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -56.17101199735393, time: 51.57
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.95    -1.     -19.2531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.665   -0.84   -10.1865]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -52.89790022966602, time: 51.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.8     -1.     -16.8795]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.31    -0.875  -10.4365]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -56.03857209302117, time: 51.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.64    -0.985  -15.6509]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.93    -0.945  -12.4323]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -69.1359017843284, time: 52.419
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.45    -1.     -15.6925]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.27    -0.94   -14.6836]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -62.98992532531477, time: 51.975
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.12    -1.     -14.9496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.8     -0.955  -13.7813]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -68.88895713877666, time: 52.717
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.805   -1.     -14.9405]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.625   -0.925  -11.6653]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -44.6237617440619, time: 51.906
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.735   -0.99   -14.6258]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.22    -0.895  -10.7632]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -54.5220751215748, time: 51.766
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.285   -0.985  -14.6112]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.045   -0.885  -11.4256]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -54.41908410244369, time: 52.644
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.785   -0.995  -20.1537]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.87    -1.     -25.4305]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -32.72698628467223, time: 53.118
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.465   -1.     -24.2449]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.915   -1.     -25.4605]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -33.571513241408155, time: 53.501
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.645   -1.     -25.2805]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.855  -1.    -25.415]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -36.97008520582907, time: 52.561
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.99    -0.875  -23.3047]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.82   -1.    -25.396]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -59.18115377972812, time: 52.679
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.39    -0.705  -19.6705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.85   -1.    -25.408]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -56.25072504140631, time: 52.382
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.995  -0.915 -23.578]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.3     -1.     -25.1302]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -39.27396974732601, time: 54.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.03    -1.     -24.0858]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.8     -1.     -25.3797]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -47.117226837554824, time: 53.097
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.06    -0.98   -16.5527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.76    -1.     -25.3512]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -34.149616351822736, time: 54.376
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.21    -0.98   -20.0521]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.325   -0.985  -25.1148]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -58.043308604290836, time: 52.991
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.88    -0.735  -19.7569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.11    -0.995  -25.0565]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -41.55347970991075, time: 52.759
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.12    -0.78   -20.5806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.095   -1.     -24.5541]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -35.45289034197935, time: 52.726
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.175   -0.96   -21.6534]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.055   -1.     -22.5055]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -29.94753199249252, time: 53.191
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.065   -0.695  -16.5663]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.185   -0.99   -25.0654]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -33.8466472622092, time: 53.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.22    -0.805  -17.7857]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.42    -0.955  -24.7843]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -41.967414111474355, time: 53.071
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.465   -0.8    -15.6804]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.83    -1.     -25.4142]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -40.79923046941628, time: 52.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.215   -0.76   -18.9028]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.66    -1.     -25.3108]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -29.843986770684133, time: 52.917
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.545   -0.89   -19.9607]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.9     -0.955  -24.8758]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -53.035803587298226, time: 52.535
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.37    -0.835  -18.4645]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.375  -0.56  -20.403]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -38.671082617438216, time: 53.079
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.375   -0.96   -20.1645]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.315   -0.925  -23.8626]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -46.76459907000707, time: 52.614
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.      -0.97   -24.3326]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.08    -0.955  -23.3317]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -47.326938612125915, time: 52.806
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.5     -1.     -24.6905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.28    -0.925  -21.1413]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -35.83218998741661, time: 54.009
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.055   -0.975  -22.9447]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.595   -0.99   -22.9502]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -29.874140944268675, time: 52.058
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.185   -1.     -24.6553]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.875   -0.97   -23.4848]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -32.13099615579501, time: 52.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.11    -1.     -25.0613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.83    -0.92   -23.3092]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -46.72781808947915, time: 52.45
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.295   -1.     -23.4626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.53    -0.94   -22.9036]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -39.642066853728835, time: 52.544
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.78    -0.995  -24.3941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.48    -0.89   -21.9406]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -32.33031123404081, time: 53.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.475   -1.     -21.7123]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.605   -0.97   -22.2533]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -35.45928403590481, time: 52.929
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.235   -0.815  -20.7337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.74    -0.96   -23.7112]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -34.27466182417632, time: 52.008
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.42   -0.96  -21.295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.705   -0.955  -24.7848]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -31.129249056991643, time: 52.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.795   -0.935  -23.2223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.29    -0.935  -25.0036]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -32.872003856612956, time: 52.37
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.205   -0.92   -24.6666]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.21    -0.93   -24.9885]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -29.65765438427421, time: 53.552
agent0_energy_min, agent0_energy_max, agent0_energy_avg
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -50.649969812318, time: 52.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.58    -0.935  -18.6089]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.02    -0.88   -10.6437]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -47.73251374331334, time: 52.377
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.41    -1.     -20.7548]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.955   -0.855  -12.3479]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -45.56438355404729, time: 51.942
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.6     -1.     -19.2301]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.935   -0.875  -10.0126]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -43.8092435558917, time: 52.199
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.625   -1.     -18.7464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.175   -0.86   -10.0991]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -43.245493989226254, time: 51.512
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.665   -1.     -20.0322]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.06    -0.9    -10.1789]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -39.933225039114205, time: 51.685
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.29    -0.985  -17.6509]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.7     -0.935  -10.0345]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -48.201531951078216, time: 53.221
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.815   -0.935  -17.1193]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.415   -0.81   -10.7611]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -44.48498829003324, time: 51.988
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.775   -0.945  -17.3489]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.18    -0.93   -10.7826]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -41.20622701991069, time: 51.354
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.35    -0.995  -20.0387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.615   -0.965  -11.1146]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -44.03017206661224, time: 51.34
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.48    -1.     -22.0417]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.52    -0.87   -13.0461]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -43.4024638679304, time: 51.786
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.68   -1.    -20.996]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.35   -0.865 -10.805]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -40.16989931658431, time: 52.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.965   -1.     -20.7301]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.555   -0.94   -11.1543]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -52.72174077233653, time: 51.353
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.375   -1.     -24.3023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.28    -0.99   -11.0531]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -44.7744508036916, time: 51.651
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.21    -0.995  -21.8626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.955   -0.965   -9.3411]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -39.211569303593954, time: 51.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.97    -1.     -20.9996]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.76    -0.9    -11.7508]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -46.57378041937646, time: 52.353
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.18    -0.995  -18.7826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.835   -0.93   -11.2722]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -37.293637980749956, time: 51.303
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.735   -1.     -19.8659]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.7     -0.93    -9.8938]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -50.14611216102793, time: 50.978
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.415   -1.     -19.6705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.93    -1.     -11.9518]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -33.697170987672905, time: 50.768
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.35   -1.    -19.132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.085   -1.     -12.4588]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -40.57425451058159, time: 52.141
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.745   -0.995  -20.1953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.305   -0.915  -11.5932]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -38.856774659668716, time: 51.338
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.055  -1.    -19.422]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.38    -1.     -11.7249]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -42.1711070293335, time: 50.535
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.515   -1.     -18.4467]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.235   -0.975  -10.4554]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -38.16223944490049, time: 51.194
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.795   -0.99   -18.0348]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.39    -0.83   -10.3254]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -50.5994057295084, time: 51.602
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.14    -0.92   -17.6071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.925   -0.9    -11.3504]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -40.96473853375479, time: 51.291
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.31    -0.89   -18.3931]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.84   -0.995 -13.874]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -43.418179355626656, time: 51.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.725   -0.975  -17.6217]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.85    -0.975  -12.9749]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -38.49105753156689, time: 51.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.805   -0.92   -18.0608]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.76    -0.825  -10.0631]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -62.02809675240114, time: 51.051
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.845   -0.975  -18.8927]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.72    -0.845  -10.9361]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -49.61626553088526, time: 51.269
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.93    -0.995  -24.2881]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.035  -0.895  -9.654]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -40.1892846593053, time: 51.451
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.06    -0.94   -20.9508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.07    -0.965  -11.1064]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -38.24440745097079, time: 51.155
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.66    -0.945  -19.1818]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.03    -0.82   -15.1016]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.14    -0.965  -10.1076]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -49.60883995473521, time: 54.509
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.005  -0.815 -15.944]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.495   -0.995  -10.5009]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -77.267683496066, time: 53.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.045   -0.85   -16.2398]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.445   -0.995  -10.3231]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -49.07102080645948, time: 53.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.94    -0.88   -15.8987]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.345  -0.85  -10.268]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -44.34473195962468, time: 52.81
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.8     -0.86   -15.1966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.47    -0.93    -9.9102]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -47.495303287920635, time: 52.416
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.995   -1.     -15.9638]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.075   -1.     -10.3116]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -44.272875342897315, time: 53.129
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.88    -1.     -17.8348]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.49    -1.     -10.2684]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -41.28492044910337, time: 52.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.47    -1.     -16.1883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.985   -0.995  -10.0873]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -44.624565903446324, time: 52.778
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.335   -0.975  -14.8948]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.015   -0.995  -10.2996]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -45.56318131619608, time: 53.15
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.665   -0.985  -15.7464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.505  -1.    -10.485]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -45.7196512242714, time: 52.856
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.39    -0.985  -16.5765]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.825   -1.     -10.7184]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -57.210584990791894, time: 53.445
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.825   -1.     -19.0744]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.62    -0.995  -11.1994]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -54.19566187209044, time: 53.264
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.01    -0.83   -17.7461]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.17    -1.     -11.2144]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -46.109291140874596, time: 52.695
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.57    -0.995  -18.7592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.56    -0.86    -9.4867]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -41.87435620681142, time: 53.181
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.9     -0.98   -17.8606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.355   -0.955  -10.4296]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -44.06309586663981, time: 53.418
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.76    -0.965  -18.0104]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.865   -0.985  -10.9124]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -45.68682114727202, time: 53.655
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.56    -0.935  -18.1144]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.095   -0.995  -11.5438]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -49.57227962505319, time: 52.488
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.95    -0.955  -17.6959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.06    -0.985  -10.3761]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -43.77920528813347, time: 52.289
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.935   -0.95   -18.2114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.315   -1.     -10.2904]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -51.88347802571525, time: 52.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.495  -0.96  -18.707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.      -0.965  -10.3926]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -46.74133166211326, time: 53.841
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.64   -1.    -18.141]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.79    -1.     -10.2186]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -49.47434243224523, time: 53.774
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.335   -0.985  -17.3842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.125   -0.995  -10.3312]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -53.67838126358014, time: 52.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.14    -0.995  -17.6085]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.795   -1.     -10.3614]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -47.5198256819804, time: 53.114
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.775   -0.975  -16.5918]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.715   -0.995  -10.5183]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -47.87495250123846, time: 52.217
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.91    -0.975  -17.2772]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.89    -0.985  -10.5521]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -62.56721945625896, time: 53.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.04   -0.985 -16.648]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.59    -0.955  -10.1551]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -54.85939977498012, time: 54.045
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.15    -1.     -17.8409]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.02    -1.     -11.2691]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -44.78851608219715, time: 52.889
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.505   -0.985  -16.0752]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.5     -0.99   -10.2193]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -52.41081788958377, time: 52.291
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.065   -0.995  -16.0044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.935   -0.995  -10.0388]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -53.059758663179764, time: 52.606
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.105   -0.995  -16.3442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.4     -0.615   -7.4953]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -46.22432779904706, time: 52.892
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.93    -0.99   -15.0525]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.55    -0.98   -10.4491]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -46.492270631581924, time: 53.407
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.095   -0.98   -19.3563]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.7     -1.     -22.8564]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -51.618242513676286, time: 52.237
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.155  -0.98  -19.815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.635   -0.995  -23.8939]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -51.81148878933501, time: 51.77
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.68    -0.985  -20.1901]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.605   -0.955  -22.6929]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -50.32135050719454, time: 51.53
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.38    -1.     -20.6163]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.835   -0.93   -22.6552]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -46.92567046958928, time: 51.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.85    -0.97   -18.6486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.695   -0.985  -22.3926]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -42.629157550025226, time: 52.246
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.71    -0.98   -20.9758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.49    -0.98   -23.2106]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -43.44363170027627, time: 52.579
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.045   -0.995  -18.9159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.685   -0.995  -23.0973]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -43.12875378409166, time: 50.609
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.51    -0.97   -19.1343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.795   -1.     -22.8495]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -48.68643685132117, time: 51.736
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.965   -0.95   -17.7013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.72   -0.985 -23.601]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -53.16020638824921, time: 51.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.815  -0.96  -17.991]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.28    -0.985  -24.1298]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -50.84165562529312, time: 51.502
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.075   -0.975  -17.2456]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.08    -0.86   -22.6822]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -58.28651076484019, time: 52.515
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.97    -0.99   -16.3414]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.15    -0.74   -22.0835]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -41.15878837736096, time: 51.533
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.665   -1.     -14.5125]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.      -0.755  -21.8836]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -44.40026123599233, time: 52.25
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.875   -0.965  -15.3989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.745   -0.85   -22.1856]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -41.39188974684736, time: 52.827
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.955   -0.93   -17.7649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.635   -1.     -22.1946]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -48.031110039826906, time: 52.3
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.06    -1.     -14.5642]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.625   -0.99   -23.5809]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -62.98504933006581, time: 52.262
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.315   -0.995  -16.9034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.765   -1.     -21.7784]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -49.25547556805013, time: 51.598
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.695   -0.995  -14.5775]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.615   -0.99   -22.7319]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -41.43939653628372, time: 51.371
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.125   -0.98   -15.3984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.165   -1.     -21.0909]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -41.905992350630875, time: 52.156
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.3     -0.995  -14.4435]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.39    -0.995  -22.6053]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -41.90366631673461, time: 52.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.28    -0.975  -16.0752]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.445   -1.     -21.4403]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -38.72056127927021, time: 51.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.255   -0.99   -16.9175]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.675   -1.     -20.9519]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -58.32027760879606, time: 52.001
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.76    -1.     -16.1097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.76   -1.    -20.062]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -44.75649369590366, time: 51.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.51    -0.99   -16.6847]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.555   -0.985  -18.1673]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -48.45087720242795, time: 51.247
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.095   -0.985  -18.2221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.775   -0.99   -18.5361]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -39.484453038639174, time: 52.001
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.155   -0.995  -16.6587]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.21    -0.955  -17.8897]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -52.055938705087236, time: 52.795
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.68    -0.995  -18.4242]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.285   -0.92   -18.8983]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -42.46490443470975, time: 51.875
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.91    -0.995  -17.4188]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.4     -0.975  -16.9041]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -40.83346601945705, time: 52.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.115   -0.995  -16.5203]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.23    -1.     -17.6704]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -42.32402172458221, time: 51.522
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.65    -1.     -17.2634]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.375   -0.98   -19.4653]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -48.39171143044186, time: 52.276
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.105   -0.935  -15.5135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.9     -0.97   -19.8441]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -57.49911790482849, time: 52.114
[-17.385   -0.92   -12.8639]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -55.673258680288036, time: 52.257
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.39    -0.995  -12.4585]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.115   -0.935  -14.6951]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -46.512901631607676, time: 52.11
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.67    -0.995  -10.9753]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.795   -0.92   -12.8254]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -47.45759362477813, time: 51.937
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.28    -0.975  -11.3404]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.04    -0.925  -13.5893]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -45.03805215474325, time: 52.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.82    -1.      -9.7631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.355  -0.935 -12.594]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -50.566693287415184, time: 53.158
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.925  -0.985 -10.32 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.04    -0.89   -12.4782]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -48.84895143531356, time: 51.875
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.07    -0.96   -11.7551]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.87    -0.905  -12.9734]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -50.95580007086373, time: 52.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.785   -0.875  -10.9986]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.08    -0.98   -13.7499]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -41.28414872537076, time: 52.53
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.655   -0.975  -10.7583]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.77    -0.93   -12.1042]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -43.39250390096574, time: 51.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.39    -0.92   -11.4244]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.58    -0.96   -12.1589]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -45.312489212973624, time: 52.694
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.425   -0.93   -10.7013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.13    -0.94   -12.9854]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -44.415187477241965, time: 52.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.64    -0.92   -12.4873]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.23    -0.98   -12.3132]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -47.18319663692754, time: 51.413
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.02    -0.9    -11.9128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.915   -0.915  -12.3926]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -45.38794620069362, time: 52.03
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.51    -0.915  -12.0426]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.125   -0.84   -12.3563]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -49.0447891816769, time: 51.86
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.225   -0.95   -12.0471]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.175  -0.87  -12.013]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -52.83746567557068, time: 52.997
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.805   -0.985  -13.1391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.545   -0.9    -14.0789]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -47.56812023952155, time: 52.056
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.515   -0.98   -11.8593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.6     -0.795  -12.2808]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -48.62283364601792, time: 51.609
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.83    -1.     -16.6581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.995   -0.945  -13.8099]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -43.192433327787164, time: 52.276
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.765   -1.     -13.6643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.535   -0.835  -12.2965]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -49.34436802189698, time: 51.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.915   -0.995  -14.3477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.05    -0.785  -12.6668]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -55.41093349684266, time: 52.571
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.86    -0.99   -12.9774]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.15    -0.755  -12.6169]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -42.09777365052031, time: 51.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.46  -0.98 -12.93]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.015   -0.675  -11.1105]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -48.491816112080514, time: 50.97
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.175   -0.88   -12.9881]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.805   -0.83   -14.0575]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -47.15184131031878, time: 51.846
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.26    -0.985  -13.3182]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.22    -0.94   -13.5024]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -45.98485744772987, time: 51.93
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.7     -0.995  -12.4259]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.54    -0.875  -13.4954]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -44.189327668347666, time: 53.076
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.135   -1.     -12.2205]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.02    -0.97   -13.0059]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -39.856978423439905, time: 51.766
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.91    -0.995  -12.9663]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.145   -0.945  -11.4207]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -49.22520718344493, time: 52.349
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.63   -0.995 -13.614]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.6     -0.8    -12.1638]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -44.57672197569114, time: 52.3
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.275   -0.985  -13.4179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.115   -0.72   -10.9832]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -45.51547328717714, time: 51.218
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.86    -0.995  -12.2905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.63    -0.775  -12.4243]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -47.211401691290476, time: 52.749
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.5     -0.96   -13.3172]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.24    -0.725  -12.1322]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -44.03828878365707, time: 52.193
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.99    -0.975  -12.0159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.525  -0.71   -7.0351]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -52.495237915393815, time: 52.429
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.865   -0.935  -13.4536]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.095   -0.85    -8.5859]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -55.97753306711356, time: 52.62
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.655   -0.965  -10.2992]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.975   -0.795   -9.2768]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -56.89188543067696, time: 52.428
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.165  -1.    -12.53 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.37    -0.835  -10.9468]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -52.979318219835356, time: 52.869
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.135  -0.995 -12.63 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.295  -0.81  -10.728]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -62.35166518036174, time: 52.798
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.945   -1.     -18.9776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.065   -0.78   -10.4827]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -60.529513291886616, time: 52.838
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.755   -0.995  -14.5966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.41    -0.805  -10.5805]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -49.6285433477852, time: 52.171
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.82    -1.     -13.5132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.22    -0.825  -10.3003]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -55.89535373401044, time: 52.481
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.97    -1.     -12.1553]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.04    -0.92   -12.0784]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -53.980795701353806, time: 51.932
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.695   -0.94   -10.2369]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.075   -0.825  -13.5347]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -65.21148550586861, time: 53.39
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.44   -0.995 -11.056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.935  -0.74  -10.798]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -68.66731172426195, time: 52.235
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.95    -1.     -13.0098]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.37    -0.655  -10.8359]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -63.31933787147168, time: 52.331
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.535   -1.      -9.2787]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.445   -0.755  -11.3392]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -54.19191918681383, time: 52.204
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.415  -0.995 -10.376]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.795  -0.935 -15.095]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -57.103573795225, time: 52.667
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.99    -1.     -11.4762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.895   -0.89   -14.1653]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -50.31161299872174, time: 53.147
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.295   -1.     -13.0753]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.885   -0.9    -12.3054]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -51.326737264080606, time: 52.126
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.495   -1.      -9.9426]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.72    -0.985  -13.3952]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -68.98583601328295, time: 52.912
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.05    -1.     -14.0879]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.465  -0.995 -13.746]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -47.1107955453945, time: 52.078
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.65    -1.     -11.9711]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.06    -0.945  -11.5594]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -53.98933882064057, time: 52.186
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.785   -1.     -10.1026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.03    -0.975  -12.1259]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -46.76014795297802, time: 52.129
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.19    -0.985   -9.2278]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.87    -0.935  -11.6972]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -59.76809534632778, time: 52.736
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.59    -1.     -11.4679]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.67    -0.95   -12.6719]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -88.62541017696003, time: 53.013
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.23    -0.97   -12.3227]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.35    -1.     -13.1183]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -48.924773295101495, time: 52.317
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.475   -0.915  -10.1184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.25    -0.995  -11.2342]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -46.15192973458303, time: 53.081
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.55    -0.975  -13.2748]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.945   -0.995  -11.0345]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -49.8635991825604, time: 52.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.49    -1.     -10.7556]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.04   -0.965  -9.573]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -53.59630598822026, time: 52.455
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.81    -0.99   -11.0581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.3     -0.795   -8.6986]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -48.15163833895933, time: 52.87
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.675   -1.     -10.8088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.835   -1.     -10.4706]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -48.95239130552351, time: 52.353
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.755   -1.     -10.3505]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.18    -0.975   -9.8116]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -46.2700759810497, time: 52.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.275   -0.885   -9.3003]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.435   -0.88   -10.0006]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -50.523077945949055, time: 53.205
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.565   -0.985  -10.7471]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.76    -0.875  -10.3409]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -52.61312208308011, time: 52.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.375   -0.845   -8.9078]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.2     -0.99   -21.7011]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.52    -0.98   -18.9887]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -37.05598275779025, time: 51.351
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.73    -0.995  -20.8491]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.01    -0.985  -18.1187]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -52.54293126973329, time: 51.791
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.865   -1.     -19.9133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.575   -0.985  -16.4423]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -40.24057806473743, time: 51.899
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.27    -0.96   -20.3679]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.33    -0.97   -14.5682]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -40.95804056815879, time: 51.62
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.99    -0.98   -20.3304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.245   -0.95   -16.5933]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -45.935593480149635, time: 51.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.595   -0.995  -22.6985]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.315   -0.965  -22.2107]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -47.47028893637463, time: 52.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.575  -0.995 -18.788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.59    -0.985  -18.5478]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -40.38959045570886, time: 51.545
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.5     -1.     -20.2913]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.615   -0.925  -16.9614]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -40.57538004634917, time: 51.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.55    -1.     -19.8182]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.95    -1.     -19.3189]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -50.14032238742888, time: 51.99
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.96    -1.     -20.0788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.835   -1.     -17.8236]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -40.54244807540209, time: 52.569
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.165   -1.     -17.4811]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.935   -0.93   -15.3504]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -38.195633534671316, time: 51.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.435   -0.995  -17.8865]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.675   -0.975  -17.3414]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -45.022968604945675, time: 51.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.605   -1.     -18.6958]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.055   -0.995  -18.4684]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -40.85756184951365, time: 52.0
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.28    -1.     -18.9886]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.275   -0.995  -19.0937]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -44.45992696640114, time: 52.753
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.775   -0.995  -19.1312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.97    -0.795  -14.0153]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -44.211820194176525, time: 52.732
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.785   -1.     -18.7665]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.43    -0.95   -18.7407]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -45.45582214931032, time: 52.28
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.415   -1.     -19.5675]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.575   -0.94   -17.1253]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -36.26622418021148, time: 53.305
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.09    -1.     -19.1393]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.715   -0.885  -17.6855]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -40.85415206494923, time: 52.237
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.525   -0.995  -19.5702]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.28    -0.65   -14.7307]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -43.15727733823454, time: 51.522
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.965   -1.     -18.5117]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.455   -0.795  -14.1119]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -43.385428510204655, time: 51.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.215   -0.915  -17.7908]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.2     -0.89   -13.6189]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -41.8344247994129, time: 52.065
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.425   -0.97   -19.3594]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.135   -0.92   -16.3345]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -53.30772308164815, time: 52.032
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.7     -0.975  -17.7111]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.34    -0.645  -12.6683]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -49.331851374103174, time: 51.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.87    -1.     -21.0932]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.16    -0.985  -14.2989]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -40.10110936731235, time: 52.079
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.71    -0.965  -20.5297]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.265  -0.98  -15.927]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -41.26440113230317, time: 52.684
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.335   -1.     -21.4455]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.99    -0.975  -13.5435]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -42.84825351075845, time: 51.664
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.18    -0.99   -20.3288]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.415   -1.     -18.4116]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -42.096468921094704, time: 52.049
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.43    -0.955  -18.1041]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.77    -1.     -15.7156]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -60.488479790294186, time: 51.82
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.97    -0.97   -16.6169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.215   -0.99   -17.3257]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -42.97823081719684, time: 51.994
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.08    -0.995  -17.4764]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.83    -1.     -12.1717]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -49.58576480272915, time: 52.078
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.64    -0.96   -15.8265]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.055   -0.91   -12.5141]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -56.51581643671863, time: 52.559
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.935   -0.86   -20.5482]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.81    -0.995  -10.3859]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -52.96531428135811, time: 52.401
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.2     -1.     -21.9771]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.115   -1.     -10.7557]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -54.46862451644685, time: 51.908
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.085   -0.985  -23.1484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.395   -1.     -10.8829]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -52.130866565968034, time: 52.569
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.545   -0.99   -21.3947]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.4     -1.     -11.5587]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -49.63823523345846, time: 51.95
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.595   -1.     -24.3571]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.36    -0.92    -9.9636]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -54.11436845209373, time: 52.732
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.015   -1.     -24.3645]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.      -0.95    -9.7241]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -49.5731802101015, time: 52.836
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.055   -1.     -23.7529]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.43    -0.985   -9.9894]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -52.86981218142717, time: 51.835
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.385  -1.    -23.416]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.455   -1.     -11.2874]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -46.0792084484165, time: 52.185
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.72    -0.99   -23.5266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.115   -0.98   -10.3392]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -50.07804778824635, time: 51.827
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.59    -0.95   -24.6912]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.275   -0.985  -11.0167]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -61.57014394656695, time: 53.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.435  -0.97  -23.852]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.365   -0.985  -10.9016]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -53.93668786519528, time: 52.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.405   -1.     -24.2224]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.09    -0.96   -10.3137]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -54.99984497717219, time: 51.7
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.985   -0.99   -22.0126]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.05    -0.985   -8.9372]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -49.76046957906143, time: 52.101
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.86    -1.     -24.0226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.34    -0.99    -9.2516]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -58.841948971384326, time: 52.882
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.925   -1.     -24.2069]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.465   -1.     -10.7629]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -52.27873070911801, time: 52.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.64    -0.99   -23.4538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.71    -0.995   -9.6247]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -47.61358950872465, time: 52.558
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.18    -1.     -23.3098]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.815   -0.965   -9.5296]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -48.588190676083514, time: 54.392
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.745   -0.915  -20.0597]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.265   -0.99    -9.1103]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -59.193164041512446, time: 51.996
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.94    -0.915  -23.0589]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.035   -0.985   -9.7084]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -46.6321042152181, time: 52.19
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.275   -0.975  -20.2822]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.54    -0.99    -9.3652]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -52.68813689730821, time: 52.865
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.995   -1.     -21.9803]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.52    -0.99    -9.4012]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -44.92406707052883, time: 51.962
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.81   -0.995 -21.384]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.12    -0.99    -8.4397]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -54.55186567497801, time: 52.926
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.675  -0.99  -20.637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.86   -0.995  -8.85 ]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -55.8207801739425, time: 53.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.275   -0.97   -22.6772]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.2     -1.     -10.2385]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -46.57610994719835, time: 52.196
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.84    -0.9    -23.2183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.93    -0.99    -9.9009]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -48.451071751502724, time: 52.512
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.955   -0.895  -23.6622]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.85    -1.      -8.8228]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -82.69324801846089, time: 52.558
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.465   -0.765  -22.5776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.905   -1.     -11.6448]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -47.08807549069783, time: 53.126
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.085   -0.965  -24.2374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.405   -1.     -10.6653]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -48.262221369604724, time: 52.462
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.135   -0.99   -24.2336]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.095   -1.     -10.3473]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -62.91236764824, time: 52.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.69    -1.     -23.6371]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.19    -0.985  -10.2281]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -64.01763674430495, time: 53.904
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.455   -1.     -23.9221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.48    -1.      -9.8543]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -53.01382567637621, time: 53.042
[-13.695   -0.965  -10.7948]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -38.62323688007782, time: 51.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.79    -0.875  -18.4097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.555   -0.985  -10.6952]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -39.19578782479593, time: 51.203
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.77    -0.955  -19.2086]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.92    -0.945  -11.5329]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -42.02379208454873, time: 50.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.195   -0.9    -17.8223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.13    -0.835   -9.5202]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -50.54208406997882, time: 50.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.095   -0.885  -17.9284]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.055  -0.965 -11.503]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -45.77703984595668, time: 51.61
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.4     -0.89   -19.4241]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.25    -0.845  -11.0805]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -45.39070654724262, time: 51.378
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.24    -0.99   -21.0467]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.225   -0.885  -11.8301]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -43.548113885149306, time: 50.897
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.22    -0.975  -21.6906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.185   -0.945  -14.3529]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -39.85471344566608, time: 51.596
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.08    -0.995  -20.7354]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.315   -0.895  -11.2517]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -49.489997460654564, time: 52.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.475   -0.97   -22.8424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.125   -0.89   -12.7259]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -39.836514692644776, time: 50.558
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.64    -0.985  -19.9815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.92    -0.94   -11.6286]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -46.68027323544389, time: 51.832
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.77    -0.985  -20.9644]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.355   -0.925  -11.7804]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -42.50037972326151, time: 51.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.04    -1.     -20.1673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.1     -0.935  -13.0364]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -44.057926490241854, time: 51.809
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.345   -0.835  -19.0792]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.795   -0.935  -11.6816]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -46.18566920772051, time: 51.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.06    -1.     -19.1135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.945   -0.925  -12.4408]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -41.535366941847826, time: 50.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.06   -0.99  -18.457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.2     -0.91   -13.7949]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -44.81085152092314, time: 51.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.145   -1.     -21.7994]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.29    -1.     -15.3276]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -38.5881277527962, time: 50.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.79    -1.     -23.6097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.48    -0.995  -12.2405]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -39.8934395486832, time: 51.149
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.58    -1.     -22.8119]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.975   -0.93   -12.1466]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -39.227247554805565, time: 51.893
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.34    -0.995  -21.4355]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.005   -0.935  -11.4487]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -38.66361338926273, time: 50.499
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.11    -1.     -23.9579]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.925   -0.99   -11.8655]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -42.64243594349802, time: 51.632
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.895  -1.    -23.766]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.83    -0.955  -11.3052]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -47.11257615944205, time: 52.381
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.43    -1.     -21.8707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.14    -0.95   -11.0092]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -61.46823537457581, time: 51.889
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.095   -1.     -21.7538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.115   -0.91   -10.1051]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -45.02101610559156, time: 51.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.985   -1.     -20.5152]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.93    -0.965  -12.3776]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -38.88436788427731, time: 50.878
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.13    -1.     -19.1184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.71    -0.985  -13.3241]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -38.12500185124321, time: 52.281
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.59    -1.     -21.4966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.48    -0.97   -14.9484]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -42.40821347061333, time: 51.149
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.79    -0.995  -21.1961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.22    -0.98   -15.1482]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -40.97694003561523, time: 51.015
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.68    -1.     -22.1777]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.58    -0.925  -12.5698]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -47.999904243009006, time: 51.165
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.935  -0.995 -19.497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.46    -0.93   -13.2408]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -35.96586633329867, time: 51.476
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.96    -0.995  -20.8305]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.7     -0.91   -11.8606]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -43.70747279544688, time: 52.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.8     -1.     -19.5177]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.995   -0.905  -10.0728]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -47.84851692264883, time: 52.755
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.71    -0.68   -10.0288]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.655   -0.865   -9.5618]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -75.86212631211724, time: 52.373
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.005   -0.965  -11.2062]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.12    -0.86   -10.1367]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -55.44304364837773, time: 52.137
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.825  -0.925 -10.962]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.395   -0.865  -10.0732]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -50.245278584270835, time: 52.291
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.6     -0.855   -9.3483]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.07    -0.735  -10.0264]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -56.72905403487939, time: 53.0
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.885   -0.935  -11.8657]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.145  -0.76   -9.613]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -57.66433654787953, time: 52.225
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.805   -0.965  -10.3929]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.42    -0.88    -9.8004]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -54.34635700427183, time: 52.717
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.49    -0.88    -8.9324]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.725   -0.95   -10.1014]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -50.24625397517677, time: 52.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.925   -0.89    -9.8081]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.78    -0.895   -9.2904]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -42.24681180777458, time: 52.419
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.24    -0.83    -9.2155]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.22    -0.895  -10.1826]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -50.54938724419276, time: 53.527
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.2     -0.725   -7.7271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.33    -0.91   -10.8683]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -60.238490932646265, time: 52.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.99    -0.755   -9.3702]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.56    -0.91   -11.7357]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -53.13997159365175, time: 52.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.14    -0.8     -9.3859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.535   -0.84   -10.9635]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -51.37466652785437, time: 52.585
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.09    -0.895  -11.3023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.255   -0.695  -10.0991]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -54.870799841121915, time: 52.779
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.485  -0.865 -12.373]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.085  -0.62   -9.328]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -51.33351140095701, time: 53.02
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.555   -0.925  -11.8693]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.26    -0.825  -10.5103]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -67.71986173530766, time: 52.637
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.18    -0.86   -13.3041]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.13    -0.775  -10.0176]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -54.56899003882032, time: 54.908
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.99    -0.94   -12.3981]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.585   -0.81   -10.0039]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -53.91106405764854, time: 51.794
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.61    -0.94   -11.4779]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.57    -0.865  -10.0438]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -50.549178595806424, time: 52.637
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.63   -0.945 -12.954]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.44    -0.865  -10.6278]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -52.28932401014003, time: 52.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.24    -0.92   -15.0122]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.1     -0.825   -9.5457]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -56.14106514570296, time: 52.123
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.765   -0.86   -11.3791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.265  -0.845 -10.695]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -49.58457220883902, time: 54.122
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.1     -0.92   -10.3529]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.39    -0.895  -11.4449]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -59.12857520828094, time: 51.562
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.675   -0.94   -13.7781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.625   -0.905  -10.7629]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -57.17020669034187, time: 51.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.91    -0.94   -15.2077]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.915   -0.895  -10.6226]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -56.15120254130774, time: 52.787
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.495  -0.955 -16.084]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.515   -0.97   -10.8399]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -52.35347322848466, time: 52.981
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.36    -0.935  -13.7677]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.575   -0.945  -11.6968]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -60.4821766508605, time: 52.459
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.25    -0.94   -17.2194]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.705   -0.925  -12.1507]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -57.7648746542643, time: 51.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.955   -0.89   -12.1378]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.22    -0.945  -12.3656]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -57.88781116821017, time: 53.147
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.205   -0.955  -13.0457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.25    -0.945  -11.3007]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -61.683588223465975, time: 52.271
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.92    -0.71   -11.0924]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.47    -0.93   -11.4192]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -48.25584109966432, time: 52.089
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.955   -0.83   -10.7712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.875   -0.99   -13.5349]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.155   -0.945  -11.2229]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -49.78731932324317, time: 51.492
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.975   -0.69   -10.6698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.16    -0.89   -10.0648]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -64.44276910799528, time: 52.301
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.365   -0.865  -12.3862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.345   -0.905  -10.0321]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -58.518299633302796, time: 51.917
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.785   -0.965  -12.5991]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.07    -0.9    -10.6993]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -49.241345273798885, time: 51.644
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.925   -0.985  -10.8853]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.62    -0.855  -10.9723]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -53.47196548085384, time: 52.007
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.2     -0.97   -11.0014]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.285   -0.88   -11.4864]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -61.48664014249002, time: 51.918
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.095   -0.975  -11.4623]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.73    -0.865  -12.7932]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -56.83146002853631, time: 52.474
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.89    -0.965  -10.7464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.41    -0.905  -13.9222]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -50.50344860726718, time: 51.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.265   -0.96   -11.1773]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.925   -0.895  -11.0975]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -54.52687599141047, time: 52.437
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.42    -0.975  -10.7933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.475   -0.865  -11.6786]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -52.17700509205204, time: 52.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.475   -0.955  -10.5566]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.84    -0.91   -11.9139]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -52.034796586892426, time: 52.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.575   -0.895  -10.0153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.115   -0.885  -10.3659]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -46.2829489000829, time: 52.103
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.195   -0.97   -10.5989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.31    -0.915  -10.1565]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -46.86897756942688, time: 51.994
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.85   -1.     -9.746]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.3     -0.87   -10.3626]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -47.15992448653845, time: 52.59
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.695   -0.995   -9.7601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.815   -0.98   -11.2339]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -52.179780633589274, time: 52.046
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.27  -0.99 -10.14]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.685   -0.93   -11.1269]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -48.273582599525305, time: 51.78
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.405   -0.94    -9.4257]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.73    -0.835   -9.4789]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -51.75669577678198, time: 51.545
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.98    -0.95    -9.9747]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.735  -0.98  -15.482]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -46.75902961421112, time: 52.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.325   -0.99    -9.5781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.745  -0.985 -11.251]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -55.0549963811437, time: 51.513
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.345  -0.87  -10.159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.24    -0.79   -10.3494]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -62.226007791223694, time: 52.516
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.06   -0.705  -9.606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.59    -0.905  -13.3787]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -56.09118538621315, time: 51.798
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.085  -0.935  -9.171]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.845   -0.915  -13.9369]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -56.79121575828629, time: 53.133
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.62    -0.82    -9.4601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.095   -0.935  -13.7827]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -51.18945603682387, time: 52.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.47    -0.81    -9.3013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.985   -0.93   -12.6216]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -50.921470128538004, time: 51.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.775   -0.85    -9.0581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.615   -0.99   -13.1792]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -48.19727509927273, time: 52.563
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.025   -0.87    -9.2513]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.24    -0.94   -12.5116]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -47.52469193144839, time: 52.67
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.925   -0.86    -9.1512]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.305   -0.9    -10.9015]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -51.605011460120906, time: 52.378
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.135   -0.975  -10.0463]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.04    -0.925  -11.1931]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -52.51850718318831, time: 52.432
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.56    -0.935  -10.3416]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.665   -0.94   -12.7531]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -58.110549193214354, time: 52.431
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.915   -0.885  -10.4728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.985  -0.93  -10.215]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -64.13910515229571, time: 52.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.355   -0.855   -9.2272]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.24    -0.94   -13.4491]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -55.911516407615466, time: 52.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.21    -0.985  -23.1029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.175   -0.905  -24.5014]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -33.316659462961326, time: 52.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.015   -0.995  -22.5612]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.05    -0.935  -24.8199]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -31.04275571932041, time: 53.331
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.755   -0.995  -21.8911]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.56    -0.86   -24.5581]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -38.80689699153033, time: 52.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.875   -0.99   -21.4083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.525   -0.925  -24.6875]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -36.65397107736416, time: 52.325
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.935   -0.98   -21.4569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.46    -0.91   -21.7553]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -33.38058534160742, time: 52.396
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.185   -0.97   -20.7698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.595   -0.915  -23.0984]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -38.48925548723105, time: 52.236
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.31    -1.     -24.0557]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.28    -0.955  -23.4636]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -36.79919250086172, time: 52.696
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.85    -0.94   -23.5884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.57    -0.93   -23.8046]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -37.77318617697969, time: 53.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.72    -1.     -24.9891]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.465   -0.94   -23.3522]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -32.53054494646169, time: 52.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.59    -1.     -25.3242]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.995   -0.99   -24.0598]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -32.10935068404579, time: 52.607
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.6    -1.    -25.321]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.125   -0.99   -24.6258]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -27.830867013064005, time: 51.228
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.21    -0.975  -23.9364]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.66    -0.995  -23.5962]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -30.616835111570904, time: 51.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.58   -0.995 -25.326]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.645   -0.975  -22.3491]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -28.647083219046145, time: 52.193
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.98    -0.975  -24.2105]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.035   -0.945  -24.0773]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -38.13007332993596, time: 51.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.67    -0.955  -24.8768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.205  -0.95  -24.615]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -30.893372540672008, time: 51.8
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.025   -0.995  -25.0794]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.915  -0.91  -23.749]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -27.23568850844219, time: 51.734
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.34    -1.     -20.6786]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.05    -1.     -24.3508]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -27.31630220597273, time: 50.853
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.005   -1.     -21.0978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.45  -1.   -23.63]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -31.36026418218694, time: 51.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.55    -1.     -23.1807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.52    -1.     -24.3724]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -31.10120312789995, time: 51.302
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.535   -0.655  -18.8858]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.275   -0.995  -23.5019]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -35.41853283472724, time: 51.789
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.545   -0.97   -21.9538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.375   -1.     -21.0459]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -33.992757032269154, time: 51.076
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.77    -0.955  -20.3842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.1     -0.995  -22.7236]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -29.424517328455423, time: 51.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.255   -0.975  -19.9148]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.135   -1.     -21.6666]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -40.87776916630576, time: 51.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.875   -1.     -23.8547]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.015   -1.     -21.7083]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -32.879776659836146, time: 51.668
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.26    -0.995  -22.8071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.97    -0.995  -20.3696]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -37.578535483703696, time: 51.497
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.58   -0.96  -19.147]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.175   -1.     -22.6685]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -38.59912817281129, time: 52.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.49    -0.98   -20.2629]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.94   -0.99  -20.447]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -34.5392043780812, time: 51.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.72    -1.     -21.9034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.455  -0.985 -22.282]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -35.09212774246859, time: 52.144
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.335   -0.99   -24.2972]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.89    -1.     -21.1279]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -37.30348550245749, time: 51.868
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.595   -0.98   -22.9483]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.975   -0.995  -17.3346]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -35.05048506321953, time: 52.753
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.755  -0.995 -21.522]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.29   -0.995 -21.199]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -32.39524730873755, time: 52.209
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.28    -1.     -16.4241]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.19    -0.895  -19.0345]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -62.357944166137656, time: 52.749
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.34    -0.935  -13.6211]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.04    -0.905  -18.7969]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -45.562869474171805, time: 52.066
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.955   -0.97   -14.5531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.765   -0.98   -20.0698]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -39.59442592459341, time: 51.158
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.45    -0.99   -15.4329]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.63    -0.935  -20.0652]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -43.126969064094304, time: 52.623
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.98    -0.98   -12.9309]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.935   -0.985  -20.2867]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -37.93453251686115, time: 52.747
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.75    -0.99   -12.7008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.74    -0.995  -20.4609]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -39.112661586118726, time: 51.964
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.68    -0.99   -13.4468]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.12    -0.985  -19.7742]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -47.844720592878026, time: 52.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.505   -0.98   -11.9222]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.815   -0.97   -18.6327]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -38.135873826570474, time: 52.28
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.6     -0.945  -11.3421]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.025  -0.99  -18.982]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -43.28537213887909, time: 51.362
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.005   -0.95   -11.0911]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.425   -0.97   -20.1006]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -42.832921240490066, time: 52.627
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.95    -0.98   -12.1806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.645   -0.98   -21.0768]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -42.35308399121257, time: 52.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.11   -0.975 -13.507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.425   -0.98   -19.9908]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -37.75858356714459, time: 52.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.86    -0.99   -11.7303]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.62    -0.98   -19.2142]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -44.14718210412759, time: 51.424
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.595   -0.985  -13.4194]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.385   -0.98   -20.1715]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -45.38354630041029, time: 52.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.845   -0.97   -13.9802]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.93    -0.965  -20.6095]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -46.070000979179696, time: 52.257
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.18    -0.985  -13.2592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.795   -0.945  -18.6901]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -43.482140640381466, time: 51.315
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.835   -0.985  -13.1503]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.39    -0.96   -19.2302]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -41.037536705141655, time: 51.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.165  -0.975 -12.414]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.725   -0.945  -17.0223]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -38.266725235219525, time: 52.052
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.685   -1.     -13.0782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.06    -0.925  -18.5722]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -55.38479185335116, time: 53.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.2     -0.985  -12.9773]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.715   -0.94   -19.9725]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -37.551325035099474, time: 52.034
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.87    -0.98   -12.0975]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.95    -0.95   -20.6524]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -42.417279665460384, time: 52.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.595   -0.97   -11.9748]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.14    -0.79   -18.3953]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -43.775149111333356, time: 51.932
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.325  -0.985 -14.551]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.315   -0.745  -19.5915]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -36.863418448274764, time: 52.206
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.475   -0.965  -13.7373]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.02    -0.69   -19.6954]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -45.36109194089, time: 51.843
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.735   -0.97   -14.1024]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.265   -0.77   -19.1938]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -39.236335116119974, time: 56.011
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.43    -0.985  -12.8665]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.415   -0.775  -17.8954]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -47.39719161708349, time: 51.848
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.05    -0.985  -13.3158]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.425   -0.86   -18.5296]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -39.25973232496753, time: 51.746
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.325   -0.97   -14.8893]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.17    -0.805  -18.8278]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -48.45148387388836, time: 51.948
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.875  -0.94  -13.399]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.55    -0.79   -18.7022]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -40.27036304114898, time: 52.672
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.395   -0.975  -11.7256]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.125   -0.85   -20.7781]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -44.119694869269736, time: 52.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.985   -0.955  -12.7738]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.735   -0.91   -21.1024]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -40.71732765847318, time: 52.307
[-25.    -1.   -15.76]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.14    -0.99   -10.0784]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -49.298729374734734, time: 54.071
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.275   -0.995  -14.7204]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.805  -0.965  -9.898]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -53.08451986367008, time: 52.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.56    -0.995  -17.0175]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.56   -0.95  -10.175]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -49.24116890420991, time: 53.206
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.575   -0.99   -16.0971]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.76    -0.985  -10.0009]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -46.514071312038666, time: 54.186
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.245   -0.995  -14.5646]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.875   -0.94    -9.7155]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -46.893906561557415, time: 53.665
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.41    -0.99   -14.9635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.12    -0.99    -9.9322]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -51.02848598991632, time: 52.758
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.38    -1.     -14.0216]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.03    -0.865   -9.5805]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -48.414372054039426, time: 53.662
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.1     -1.     -14.2432]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.255   -0.975  -10.4411]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -43.29975755563457, time: 53.57
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.35    -1.     -13.9552]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.77    -1.     -10.2244]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -42.57030785419422, time: 52.931
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.755   -1.     -14.4824]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.815   -0.985  -10.9138]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -43.519241506297675, time: 53.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.725   -0.985  -12.8076]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.28    -1.      -9.6506]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -49.4929380447473, time: 52.725
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.84    -0.995  -13.3397]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.22    -0.915   -8.7437]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -46.889525526033815, time: 52.271
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.725   -0.965  -13.9864]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.22    -0.96    -9.4977]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -44.616406458520544, time: 52.912
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.965   -0.99   -13.5901]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.255   -0.96    -9.1781]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -47.25954814071927, time: 52.591
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.45    -1.     -13.3622]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.185   -0.99   -10.3393]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -44.84952582848692, time: 56.415
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.      -1.     -13.0858]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.05    -0.995   -9.7331]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -45.96210898556853, time: 52.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.19    -1.     -14.3444]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.55    -0.995   -9.8772]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -45.18127375730167, time: 52.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.395   -1.     -15.1239]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.96    -0.95    -9.5693]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -46.162118543048365, time: 52.981
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.675   -1.     -14.1535]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.755   -0.935   -9.6719]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -43.96616507387018, time: 53.554
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.185   -0.88   -13.2102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.505   -0.95    -9.8617]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -48.26592141880932, time: 53.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.4     -0.945  -13.8148]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.675   -0.995  -10.9244]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -54.12341066282507, time: 53.788
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.645   -0.94   -13.3383]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.79    -0.945   -9.8722]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -57.73868127548403, time: 54.096
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.925   -0.91   -12.9759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.625   -0.945  -10.3777]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -43.16963271608195, time: 53.392
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.765   -0.995  -14.3506]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.345   -0.84    -9.8778]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -67.94690990151663, time: 52.899
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.18    -0.99   -14.8913]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.075   -0.985  -10.4743]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -48.30975956113938, time: 53.588
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.695   -0.96   -12.9398]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.95    -0.975  -10.0256]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -43.83672275209519, time: 53.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.2     -0.99   -14.3311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.115   -0.97    -9.9842]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -44.288394801197036, time: 52.923
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.3     -1.     -13.9145]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.935   -0.76    -9.6698]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -44.91404285499386, time: 53.084
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.82    -1.     -14.1246]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.535   -0.98    -9.9332]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -40.28582676306557, time: 53.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.805   -0.99   -13.5696]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.475   -1.      -9.7942]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -41.04694514278368, time: 53.615
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.025   -0.995  -12.9734]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.375   -0.985   -9.3307]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -41.525427456829036, time: 52.758
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.995  -0.855 -12.157]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -45.72475466178856, time: 51.876
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.74    -1.     -11.4285]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.615   -0.875  -12.3887]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -44.21650537662313, time: 52.659
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.995   -1.     -10.4359]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.185   -0.78   -12.3212]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -43.75236188434903, time: 52.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.045   -0.99    -8.4129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.38    -0.73   -11.6259]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -42.85578604598712, time: 52.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.34    -0.985  -12.5284]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.955   -0.635  -10.5854]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -43.779879949931974, time: 52.487
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.745   -0.995  -11.0503]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.295   -0.735  -11.4826]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -42.42397853850507, time: 52.205
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.08    -0.985  -10.0739]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.085   -0.825  -11.7788]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -45.56426412703645, time: 52.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.3     -0.995  -11.2919]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.33   -0.655 -11.572]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -59.0780944620859, time: 52.904
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.5     -0.975  -10.3551]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.695   -0.74   -12.0301]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -43.40133779680267, time: 53.204
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.64    -0.995  -10.2271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.2     -0.87   -11.9106]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -42.47010836303548, time: 52.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.79    -1.     -10.4213]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.555   -0.965  -12.1638]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -57.92847551222452, time: 52.225
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.085   -0.995  -12.7851]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.855   -0.79   -12.3228]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -52.047717967065616, time: 51.686
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.755   -0.98   -17.0628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.15    -0.745  -11.4846]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -58.65524818801392, time: 52.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.865   -0.935  -12.6063]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.41    -0.775  -12.1863]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.87 hr

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.54    -0.985  -15.6903]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.29   -0.995 -20.139]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -45.578478439355656, time: 51.626
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.905  -0.915 -15.747]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.91    -0.97   -19.6202]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -45.880236175861704, time: 51.945
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.135   -1.     -18.8828]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.95    -0.96   -19.3106]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -52.616574936751, time: 51.345
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.215   -0.955  -17.4534]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.11    -0.885  -14.6338]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -50.69874183411097, time: 52.029
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.615   -0.995  -16.9221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.205   -0.89   -14.2553]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -43.52306455829368, time: 51.522
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.54    -0.99   -17.8692]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.845   -0.89   -15.8741]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -39.50799371727355, time: 52.064
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.355   -1.     -18.3758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.635   -0.985  -18.3809]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -44.6935436137217, time: 52.24
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.6     -0.955  -15.0184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.425   -0.98   -16.3489]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -46.54045922232386, time: 51.651
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.51    -1.     -16.7677]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.52    -1.     -18.2546]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -36.70720587834971, time: 52.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.255   -0.975  -17.2621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.51    -0.99   -16.6155]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -36.84998184323375, time: 51.95
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.94    -1.     -18.5131]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.145   -0.98   -14.8162]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -44.26993461137645, time: 52.32
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.335  -1.    -16.247]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.12    -0.9    -18.3954]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -40.431422068032965, time: 51.985
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.865   -1.     -17.0378]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.065  -0.915 -19.982]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -53.24802833958325, time: 48.039
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.755   -1.     -17.0826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.505   -0.925  -20.4527]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.88 hr

[-15.76    -0.97   -11.2945]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -46.202455047152895, time: 52.748
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.4     -0.995  -10.7938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.985  -0.93  -11.054]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -69.46358485963039, time: 52.373
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.32    -0.995  -15.4635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.505   -0.83   -10.1319]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -53.062414431836324, time: 52.659
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.15    -1.      -9.7849]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.025   -0.89   -10.1531]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -48.935777685948466, time: 53.24
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.06    -1.      -8.4443]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.485   -0.96   -10.7361]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -45.90965933374884, time: 53.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.06    -0.995   -8.4189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.315   -0.92   -10.5787]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -44.327824255422414, time: 52.521
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.455   -1.      -9.9508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.425   -0.995  -10.4761]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -40.913868106458, time: 53.755
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.085   -0.86    -8.7023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.37    -1.      -9.4532]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -49.099450627687, time: 51.991
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.35    -0.995   -9.3089]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.46    -1.     -10.8528]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -46.29605524456896, time: 54.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.27    -0.995   -9.4061]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.945   -0.995  -10.4242]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -53.524715253969646, time: 52.221
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.975   -1.     -10.8418]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.28    -0.915   -9.8197]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -41.47792818713267, time: 52.06
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.555   -0.995   -9.5208]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.665  -0.885  -9.675]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -51.46548845570191, time: 51.636
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.81    -0.97    -9.6019]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.195   -0.88    -9.6814]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -53.735090564873644, time: 47.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.95    -0.775   -8.8913]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.425   -0.92    -9.6877]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.88 hr

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.75    -0.98   -22.7363]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.155   -1.      -8.5273]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -45.41264053681161, time: 52.609
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.155   -0.99   -23.2651]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.595   -0.99    -9.3206]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -53.19776862866751, time: 52.376
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.31    -0.96   -24.2424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.095   -0.995  -10.8403]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -54.20325709727219, time: 53.047
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.73    -1.     -23.8918]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.025   -0.995  -10.8611]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -55.483303121282894, time: 52.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.63    -1.     -23.4827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.21    -1.      -9.6809]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -46.78104040056344, time: 52.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.64    -0.97   -22.8451]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.53    -1.      -9.2022]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -52.211565600803524, time: 52.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.975   -0.985  -22.5503]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.975   -1.     -11.4092]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -67.80745158291715, time: 52.9
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.515   -1.     -22.7204]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.92   -1.    -10.963]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -60.651604787912945, time: 53.487
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.62    -1.     -20.8247]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.495   -0.975  -10.2037]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -50.05043419393466, time: 54.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.975   -1.     -21.8862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.005   -0.975   -8.8162]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -57.910891229670234, time: 51.736
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.305   -0.995  -21.0174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.685   -0.885   -9.8724]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -60.305938791707376, time: 52.569
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.84    -1.     -21.7095]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.975   -0.975  -11.0916]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -60.50026235585443, time: 50.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.505   -1.     -21.5801]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.855   -0.99    -9.9636]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -57.576916234613776, time: 47.926
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.64    -1.     -23.1685]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.26    -0.99    -9.1917]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.89 hr

[-12.995   -0.93    -9.7928]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -52.31541309922239, time: 53.249
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.46    -0.93   -10.8288]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.6     -0.935  -12.2522]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -54.83636453079625, time: 51.653
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.935   -0.94   -11.9921]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.6     -0.955  -12.7546]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -51.18827291929161, time: 52.362
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.575   -0.935  -11.6464]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.25    -0.97   -11.9804]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -48.99328094708511, time: 53.159
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.58    -0.98   -11.7699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.905   -0.99   -11.2901]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -47.84910942184508, time: 52.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.985   -0.845  -10.2389]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.235   -0.965  -11.3428]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -47.7500553631955, time: 52.396
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.67    -0.835  -13.2587]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.47    -0.955  -11.9663]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -50.91285894620209, time: 51.846
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.615  -0.91  -11.165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.73    -0.93   -11.9105]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -44.02898572136444, time: 52.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.17    -0.97   -12.5722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.73    -0.895  -11.0495]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -62.106659546358294, time: 53.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.59    -0.975  -11.4616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.935   -0.92   -10.2834]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -45.61796993798575, time: 51.5
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.145   -1.     -10.7609]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.75    -0.86   -10.1484]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -57.31453861133914, time: 52.393
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.32    -1.     -12.0875]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.5    -0.85  -10.645]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -53.27808774424254, time: 49.49
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.85    -1.     -10.9308]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.235   -0.955  -10.8887]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -56.12442645644114, time: 45.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.925   -0.99   -12.7496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.09    -0.97   -10.9699]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.89 hr

[-36.26    -0.99   -20.0414]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.56   -1.    -22.875]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -35.08303823482833, time: 51.818
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.665   -0.995  -24.1804]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.84    -0.945  -23.4186]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -31.004917830764146, time: 51.038
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.715   -1.     -24.9742]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.725   -0.985  -21.7648]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -33.59332922861365, time: 52.217
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.19    -1.     -24.6671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.105   -1.     -19.2182]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -36.00681581627272, time: 52.644
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.785   -0.88   -23.8973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.76    -1.     -18.5518]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -30.46018057823647, time: 51.983
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.69    -0.975  -23.2005]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.44    -1.     -17.3319]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -31.651572008439352, time: 51.283
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.01    -1.     -24.2921]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.935   -1.     -18.7631]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -37.47297342531101, time: 51.681
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.19    -1.     -24.3412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.11    -1.     -22.2689]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -29.189654909599586, time: 52.013
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.825   -0.995  -24.9144]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.44    -1.     -18.9905]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -29.577251656038086, time: 52.764
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.465   -1.     -23.4649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.825   -1.     -18.8654]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -36.083644213201, time: 51.355
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.39    -1.     -21.8125]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.2     -1.     -23.4981]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -29.280767151290757, time: 51.929
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.155   -1.     -22.3124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.975   -1.     -19.7357]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -40.15305838101935, time: 49.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.715   -0.995  -21.4408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.905   -1.     -20.3531]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -49.27391135497004, time: 45.166
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.62    -1.     -21.7441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.715   -0.94   -21.7358]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.89 hr

[-12.235   -0.93    -9.8895]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.12    -0.76    -9.6722]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -46.62568603623313, time: 52.704
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.81    -0.895   -8.9066]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.165   -0.93   -12.1553]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -55.34490039230581, time: 52.398
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.99    -0.99   -10.5819]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.88   -0.835 -10.824]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -53.22665045831942, time: 53.283
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.185   -0.885   -9.3115]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.12    -0.91   -11.2882]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -51.74200354233917, time: 51.999
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.365   -0.905  -10.1151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.35    -0.795   -9.5316]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -50.74461441235658, time: 52.627
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.865   -0.965   -9.9258]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.205   -0.87   -11.0921]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -43.079689414791, time: 52.145
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.465   -0.885   -8.8071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.55    -0.895   -9.8305]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -54.263282064614515, time: 52.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.995   -0.83    -9.0052]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.82    -0.715  -10.1505]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -49.85339411496417, time: 52.469
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.335   -0.9     -9.9959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.415   -0.71    -8.8866]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -61.81898187146132, time: 53.542
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.72    -0.935  -10.0061]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.565   -0.725   -9.5318]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -50.55398313749627, time: 51.935
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.07    -0.845   -9.2671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.68    -0.89   -11.3158]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -50.5264348750663, time: 52.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.125   -0.93    -9.2547]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.715  -0.92  -11.216]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -45.94510434222362, time: 49.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.61    -0.945   -8.8834]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.32    -0.93   -10.5846]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -46.70565742912124, time: 45.114
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.745   -1.      -9.0665]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.845   -0.96   -10.4997]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.89 hr

[-15.57    -0.92   -11.4228]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -39.04885618470874, time: 51.882
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.74    -0.995  -16.3968]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.98    -0.955  -12.5691]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -50.4337879028528, time: 51.338
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.695  -1.    -17.238]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.715   -0.94   -11.9596]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -39.54894590406251, time: 51.793
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.025   -0.995  -16.3476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.775   -0.98   -11.7928]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -55.63234539052192, time: 51.946
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.375  -1.    -18.775]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.465  -0.98  -13.776]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -36.016032419207775, time: 52.038
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.035   -1.     -17.3581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.205   -0.975  -10.9398]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -42.39084771691577, time: 51.657
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.305   -0.995  -18.2244]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.49    -0.955  -11.4539]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -43.5665227967255, time: 50.502
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.235   -1.     -18.6076]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.55    -0.9    -12.1115]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -41.176881765858944, time: 51.026
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.76    -0.995  -18.0506]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.025   -0.95   -13.6799]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -39.80092731513713, time: 51.388
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.995   -0.96   -21.6619]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.93    -0.905  -12.3667]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -44.674993757586705, time: 52.507
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.42    -0.98   -17.9922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.17    -0.895  -11.3446]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -40.874957937618994, time: 51.758
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.09    -0.98   -16.6247]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.46    -0.76   -12.4311]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -43.287207678425005, time: 49.937
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.645   -1.     -18.6496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.35    -0.795  -11.3126]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -44.90357218845906, time: 46.815
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.925   -1.     -17.5538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.715   -1.     -13.3259]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -39.39654314821199, time: 35.348
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.24    -0.935  -17.2403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.695   -0.985  -13.6897]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.89 hr

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.03    -0.965  -12.2913]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.785   -0.88   -20.2676]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -45.09011913409342, time: 52.496
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.875   -0.98   -12.5831]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.715   -0.875  -19.6251]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -42.04235141204309, time: 52.719
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.51    -0.965  -10.4375]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.78   -0.79  -19.444]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -45.11164978351304, time: 51.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.59    -0.985  -12.0935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.11    -0.935  -19.2493]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -36.64403339644432, time: 52.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.275  -0.98  -12.753]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.09    -0.94   -19.3822]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -43.129175465731244, time: 51.687
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.945   -0.995  -11.2869]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.85    -0.86   -19.2612]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -59.62709783995722, time: 52.286
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.82    -0.97   -12.0957]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.015   -0.795  -18.5245]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -46.92200125538226, time: 52.242
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.96    -0.97   -13.0315]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.615   -0.775  -18.7699]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -46.87585451768882, time: 52.275
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.36    -0.945  -11.8601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.61    -0.735  -16.4637]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -39.72666001945819, time: 54.178
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.64    -0.99   -10.0931]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.69    -0.765  -16.7095]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -42.92923740862205, time: 53.194
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.215   -0.99   -10.5649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.56   -0.83  -18.414]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -40.84516253400823, time: 48.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.61    -0.98    -9.6358]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.51    -0.935  -21.2501]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -38.98408693931985, time: 42.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.75    -0.98   -10.9581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.72    -0.955  -19.5368]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -42.9296161264783, time: 31.631
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.91    -0.97   -11.9692]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.36    -0.94   -18.9875]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.90 hr

[-17.545   -1.     -12.5955]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.965  -1.     -9.019]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -47.520848516352196, time: 53.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.24    -0.99   -13.9651]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.89    -0.965  -10.2255]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -52.674370080614246, time: 52.928
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.025   -0.96   -12.5621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.02   -0.855  -8.655]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -41.18230530766821, time: 52.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.425   -0.875  -12.4637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.05    -0.925   -8.8837]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -44.13908266428864, time: 52.803
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.52    -0.905  -12.7951]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.355   -0.93    -9.9838]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -54.88360373703205, time: 53.755
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.51    -0.925  -14.6726]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.225   -0.995  -10.5307]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -50.04805776795722, time: 52.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.95    -0.915  -15.3704]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.455   -0.995  -10.0899]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -38.68069771739418, time: 52.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.925   -0.985  -14.7081]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.95    -1.      -8.9191]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -44.52118040327072, time: 52.799
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.93    -1.     -14.2903]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.635   -0.995   -9.4457]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -45.678088437106254, time: 53.292
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.125   -1.     -14.9385]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.365   -0.99   -10.0373]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -39.41222813174948, time: 50.284
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.54    -1.     -12.8223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.2     -0.99    -9.2584]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -44.829483823223335, time: 47.417
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.91    -0.995  -13.3314]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.675   -0.905   -8.7999]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -53.82863937324211, time: 35.782
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.795   -0.995  -14.9183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.9     -0.96   -10.3312]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -47.43742162991486, time: 29.887
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.715   -0.995  -14.5279]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.815   -0.975   -9.4285]
...Finished!
Trained episodes: 1 -> 40000
Total time: 2.91 hr
