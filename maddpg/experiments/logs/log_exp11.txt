python train.py --scenario wanderer2_4agents-1 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_4agents-1 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_4agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_4agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_4agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_4agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_4agents-1__2018-07-15_17-41-08...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -297.5299086581784, time: 461.393
agent0_energy_min, agent0_attention_min
[-16.4974975  -16.32532533]
agent1_energy_min, agent1_attention_min
[-16.70970971 -16.47347347]
agent2_energy_min, agent2_attention_min
[-14.48148148 -16.91991992]
agent3_energy_min, agent3_attention_min
[-17.25725726 -16.4994995 ]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -701.3182108205498, time: 708.645
agent0_energy_min, agent0_attention_min
[-17.667 -21.365]
agent1_energy_min, agent1_attention_min
[ -8.442 -30.673]
agent2_energy_min, agent2_attention_min
[ -7.659 -29.53 ]
agent3_energy_min, agent3_attention_min
[-18.836 -18.532]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -236.09273754700666, time: 714.072
agent0_energy_min, agent0_attention_min
[-15.137 -32.772]
agent1_energy_min, agent1_attention_min
[ -6.631 -40.086]
agent2_energy_min, agent2_attention_min
[ -5.856 -42.633]
agent3_energy_min, agent3_attention_min
[-11.301 -35.699]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -216.34208592898122, time: 706.589
agent0_energy_min, agent0_attention_min
[-18.685 -30.05 ]
agent1_energy_min, agent1_attention_min
[ -8.153 -40.629]
agent2_energy_min, agent2_attention_min
[ -4.268 -44.988]
agent3_energy_min, agent3_attention_min
[-23.397 -23.96 ]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -198.27891293002105, time: 674.946
agent0_energy_min, agent0_attention_min
[-30.183 -18.859]
agent1_energy_min, agent1_attention_min
[ -9.437 -38.843]
agent2_energy_min, agent2_attention_min
[ -4.408 -44.356]
agent3_energy_min, agent3_attention_min
[-25.023 -22.97 ]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -187.36077525780158, time: 679.697
agent0_energy_min, agent0_attention_min
[-29.235 -20.234]
agent1_energy_min, agent1_attention_min
[-11.122 -38.171]
agent2_energy_min, agent2_attention_min
[ -6.754 -42.837]
agent3_energy_min, agent3_attention_min
[-22.767 -23.925]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -172.87936400355312, time: 680.525
agent0_energy_min, agent0_attention_min
[-36.044 -13.471]
agent1_energy_min, agent1_attention_min
[-14.666 -34.99 ]
agent2_energy_min, agent2_attention_min
[ -6.191 -43.291]
agent3_energy_min, agent3_attention_min
[-23.892 -23.172]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -160.2740209624502, time: 691.57
agent0_energy_min, agent0_attention_min
[-47.412  -2.492]
agent1_energy_min, agent1_attention_min
[-13.428 -36.104]
agent2_energy_min, agent2_attention_min
[ -5.493 -43.559]
agent3_energy_min, agent3_attention_min
[-24.746 -23.982]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -152.3286950053103, time: 687.847
agent0_energy_min, agent0_attention_min
[-49.022  -0.959]
agent1_energy_min, agent1_attention_min
[-15.46  -33.846]
agent2_energy_min, agent2_attention_min
[ -8.812 -40.496]
agent3_energy_min, agent3_attention_min
[-20.846 -26.95 ]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -144.74649926326506, time: 687.479
agent0_energy_min, agent0_attention_min
[-48.645  -1.11 ]
agent1_energy_min, agent1_attention_min
[-20.075 -29.196]
agent2_energy_min, agent2_attention_min
[-10.54  -38.763]
agent3_energy_min, agent3_attention_min
[-16.819 -30.808]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -147.64374857304043, time: 684.126
agent0_energy_min, agent0_attention_min
[-4.9952e+01 -2.6000e-02]
agent1_energy_min, agent1_attention_min
[-23.428 -25.404]
agent2_energy_min, agent2_attention_min
[-13.132 -36.359]
agent3_energy_min, agent3_attention_min
[-14.125 -33.6  ]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -135.91809650287595, time: 685.838
agent0_energy_min, agent0_attention_min
[-49.822  -0.165]
agent1_energy_min, agent1_attention_min
[-26.374 -22.584]
agent2_energy_min, agent2_attention_min
[-13.182 -36.502]
agent3_energy_min, agent3_attention_min
[-16.173 -32.169]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -139.88658173059952, time: 688.264
agent0_energy_min, agent0_attention_min
[-47.748  -2.218]
agent1_energy_min, agent1_attention_min
[-26.846 -22.433]
agent2_energy_min, agent2_attention_min
[-12.253 -37.475]
agent3_energy_min, agent3_attention_min
[-15.853 -32.407]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -150.31793087011027, time: 685.303
agent0_energy_min, agent0_attention_min
[-48.117  -1.866]
agent1_energy_min, agent1_attention_min
[-33.112 -15.529]
agent2_energy_min, agent2_attention_min
[-16.322 -32.904]
agent3_energy_min, agent3_attention_min
[-18.989 -28.662]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -129.95238484011858, time: 693.581
agent0_energy_min, agent0_attention_min
[-49.696  -0.167]
agent1_energy_min, agent1_attention_min
[-34.017 -15.228]
agent2_energy_min, agent2_attention_min
[-22.027 -26.925]
agent3_energy_min, agent3_attention_min
[-16.915 -30.786]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -133.67133280730692, time: 692.949
agent0_energy_min, agent0_attention_min
[-47.333  -2.069]
agent1_energy_min, agent1_attention_min
[-35.495 -13.59 ]
agent2_energy_min, agent2_attention_min
[-16.671 -32.893]
agent3_energy_min, agent3_attention_min
[-16.627 -32.357]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -134.78732035796133, time: 695.968
agent0_energy_min, agent0_attention_min
[-47.955  -1.421]
agent1_energy_min, agent1_attention_min
[-37.123 -11.783]
agent2_energy_min, agent2_attention_min
[-18.568 -31.2  ]
agent3_energy_min, agent3_attention_min
[-23.592 -24.843]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -124.90843471751087, time: 693.061
agent0_energy_min, agent0_attention_min
[-4.9618e+01 -4.4000e-02]
agent1_energy_min, agent1_attention_min
[-37.912 -11.35 ]
agent2_energy_min, agent2_attention_min
[-33.769 -15.699]
agent3_energy_min, agent3_attention_min
[-32.385 -16.847]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -151.3935443099683, time: 491.083
agent0_energy_min, agent0_attention_min
[-49.175  -0.456]
agent1_energy_min, agent1_attention_min
[-35.919 -13.506]
agent2_energy_min, agent2_attention_min
[-19.608 -30.088]
agent3_energy_min, agent3_attention_min
[-31.967 -17.337]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -119.940311811039, time: 443.299
agent0_energy_min, agent0_attention_min
[-49.812  -0.07 ]
agent1_energy_min, agent1_attention_min
[-37.881 -11.684]
agent2_energy_min, agent2_attention_min
[-17.035 -32.789]
agent3_energy_min, agent3_attention_min
[-35.114 -14.363]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -126.96521050357748, time: 444.905
agent0_energy_min, agent0_attention_min
[-49.631  -0.199]
agent1_energy_min, agent1_attention_min
[-40.542  -8.938]
agent2_energy_min, agent2_attention_min
[-18.462 -31.353]
agent3_energy_min, agent3_attention_min
[-36.267 -12.402]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -116.69295401780352, time: 440.043
agent0_energy_min, agent0_attention_min
[-45.261  -0.393]
agent1_energy_min, agent1_attention_min
[-38.933 -10.402]
agent2_energy_min, agent2_attention_min
[-16.485 -33.358]
agent3_energy_min, agent3_attention_min
[-24.688 -24.768]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -109.62971173545586, time: 434.171
agent0_energy_min, agent0_attention_min
[-39.806  -0.175]
agent1_energy_min, agent1_attention_min
[-38.261 -11.103]
agent2_energy_min, agent2_attention_min
[-13.943 -35.796]
agent3_energy_min, agent3_attention_min
[-37.073 -12.554]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -115.71580500177829, time: 433.614
agent0_energy_min, agent0_attention_min
[-41.839  -0.498]
agent1_energy_min, agent1_attention_min
[-24.836 -24.459]
agent2_energy_min, agent2_attention_min
[-13.512 -36.17 ]
agent3_energy_min, agent3_attention_min
[-35.185 -14.626]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -122.56515395917522, time: 432.442
agent0_energy_min, agent0_attention_min
[-46.705  -0.538]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_4agents-1__2018-07-15_17-41-10...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -324.031267122598, time: 462.816
agent0_energy_min, agent0_attention_min
[-18.53453453 -16.48948949]
agent1_energy_min, agent1_attention_min
[-17.67867868 -16.24324324]
agent2_energy_min, agent2_attention_min
[-16.7017017 -16.6026026]
agent3_energy_min, agent3_attention_min
[-15.003003  -16.6006006]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -610.1828604855078, time: 718.912
agent0_energy_min, agent0_attention_min
[-15.888 -11.698]
agent1_energy_min, agent1_attention_min
[ -6.862 -11.088]
agent2_energy_min, agent2_attention_min
[-20.87  -19.762]
agent3_energy_min, agent3_attention_min
[-13.281 -21.507]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -260.5987442255315, time: 716.921
agent0_energy_min, agent0_attention_min
[-18.911 -22.654]
agent1_energy_min, agent1_attention_min
[-26.552 -16.535]
agent2_energy_min, agent2_attention_min
[-10.351 -34.447]
agent3_energy_min, agent3_attention_min
[ -9.039 -36.512]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -222.2040728407668, time: 719.944
agent0_energy_min, agent0_attention_min
[-24.289 -23.144]
agent1_energy_min, agent1_attention_min
[-35.454 -10.736]
agent2_energy_min, agent2_attention_min
[-22.059 -25.569]
agent3_energy_min, agent3_attention_min
[ -8.363 -40.723]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -189.33459176772035, time: 701.417
agent0_energy_min, agent0_attention_min
[-29.183 -19.266]
agent1_energy_min, agent1_attention_min
[-40.759  -8.086]
agent2_energy_min, agent2_attention_min
[-34.456 -13.206]
agent3_energy_min, agent3_attention_min
[ -7.561 -41.669]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -165.278136089619, time: 678.123
agent0_energy_min, agent0_attention_min
[-34.42  -14.689]
agent1_energy_min, agent1_attention_min
[-44.478  -5.083]
agent2_energy_min, agent2_attention_min
[-45.122  -1.458]
agent3_energy_min, agent3_attention_min
[ -7.606 -41.832]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -166.1432066193491, time: 680.682
agent0_energy_min, agent0_attention_min
[-41.029  -8.36 ]
agent1_energy_min, agent1_attention_min
[-45.503  -4.16 ]
agent2_energy_min, agent2_attention_min
[-47.101  -1.521]
agent3_energy_min, agent3_attention_min
[ -9.382 -39.974]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -157.0481054894689, time: 689.109
agent0_energy_min, agent0_attention_min
[-42.053  -6.509]
agent1_energy_min, agent1_attention_min
[-45.235  -3.386]
agent2_energy_min, agent2_attention_min
[-47.131  -2.037]
agent3_energy_min, agent3_attention_min
[-13.446 -35.799]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -153.5249613027319, time: 681.263
agent0_energy_min, agent0_attention_min
[-42.951  -5.042]
agent1_energy_min, agent1_attention_min
[-42.38   -5.382]
agent2_energy_min, agent2_attention_min
[-47.342  -2.022]
agent3_energy_min, agent3_attention_min
[-23.683 -25.338]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -142.33477394982154, time: 686.477
agent0_energy_min, agent0_attention_min
[-44.308  -3.33 ]
agent1_energy_min, agent1_attention_min
[-47.036  -1.794]
agent2_energy_min, agent2_attention_min
[-48.436  -0.767]
agent3_energy_min, agent3_attention_min
[-24.085 -25.202]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -128.96628609414088, time: 683.37
agent0_energy_min, agent0_attention_min
[-46.958  -2.392]
agent1_energy_min, agent1_attention_min
[-48.031  -1.1  ]
agent2_energy_min, agent2_attention_min
[-48.516  -0.901]
agent3_energy_min, agent3_attention_min
[-25.857 -23.006]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -121.50112721887385, time: 689.297
agent0_energy_min, agent0_attention_min
[-49.098  -0.672]
agent1_energy_min, agent1_attention_min
[-48.621  -0.966]
agent2_energy_min, agent2_attention_min
[-48.698  -0.929]
agent3_energy_min, agent3_attention_min
[-26.436 -22.689]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -133.72850898717846, time: 693.675
agent0_energy_min, agent0_attention_min
[-49.689  -0.167]
agent1_energy_min, agent1_attention_min
[-48.709  -0.746]
agent2_energy_min, agent2_attention_min
[-48.78   -0.772]
agent3_energy_min, agent3_attention_min
[-28.364 -20.471]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -113.11938900397512, time: 687.72
agent0_energy_min, agent0_attention_min
[-49.682  -0.097]
agent1_energy_min, agent1_attention_min
[-48.666  -0.447]
agent2_energy_min, agent2_attention_min
[-49.063  -0.582]
agent3_energy_min, agent3_attention_min
[-26.989 -22.041]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -108.65665104780629, time: 689.651
agent0_energy_min, agent0_attention_min
[-49.772  -0.105]
agent1_energy_min, agent1_attention_min
[-48.276  -0.744]
agent2_energy_min, agent2_attention_min
[-49.365  -0.413]
agent3_energy_min, agent3_attention_min
[-27.847 -21.464]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -114.14480407225072, time: 689.004
agent0_energy_min, agent0_attention_min
[-49.81   -0.069]
agent1_energy_min, agent1_attention_min
[-48.63   -0.208]
agent2_energy_min, agent2_attention_min
[-49.288  -0.315]
agent3_energy_min, agent3_attention_min
[-28.318 -20.41 ]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -115.27033495423466, time: 696.932
agent0_energy_min, agent0_attention_min
[-49.529  -0.178]
agent1_energy_min, agent1_attention_min
[-48.895  -0.341]
agent2_energy_min, agent2_attention_min
[-49.445  -0.434]
agent3_energy_min, agent3_attention_min
[-28.296 -20.066]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -104.5493205840213, time: 696.235
agent0_energy_min, agent0_attention_min
[-48.371  -0.175]
agent1_energy_min, agent1_attention_min
[-49.174  -0.111]
agent2_energy_min, agent2_attention_min
[-49.293  -0.667]
agent3_energy_min, agent3_attention_min
[-30.849 -17.619]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -103.27100773817033, time: 475.835
agent0_energy_min, agent0_attention_min
[-49.095  -0.073]
agent1_energy_min, agent1_attention_min
[-47.874  -0.137]
agent2_energy_min, agent2_attention_min
[-48.677  -1.11 ]
agent3_energy_min, agent3_attention_min
[-30.865 -18.571]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -97.5196457266282, time: 446.267
agent0_energy_min, agent0_attention_min
[-4.979e+01 -3.900e-02]
agent1_energy_min, agent1_attention_min
[-46.217  -0.071]
agent2_energy_min, agent2_attention_min
[-49.624  -0.176]
agent3_energy_min, agent3_attention_min
[-36.281 -12.742]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -109.77631065081977, time: 449.286
agent0_energy_min, agent0_attention_min
[-49.233  -0.133]
agent1_energy_min, agent1_attention_min
[-47.444  -0.053]
agent2_energy_min, agent2_attention_min
[-48.631  -1.113]
agent3_energy_min, agent3_attention_min
[-34.972 -14.229]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -109.17900307723603, time: 451.453
agent0_energy_min, agent0_attention_min
[-48.929  -0.196]
agent1_energy_min, agent1_attention_min
[-45.67   -0.134]
agent2_energy_min, agent2_attention_min
[-49.168  -0.613]
agent3_energy_min, agent3_attention_min
[-36.099 -13.186]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -137.4097723153783, time: 447.968
agent0_energy_min, agent0_attention_min
[-48.026  -0.825]
agent1_energy_min, agent1_attention_min
[-47.309  -0.442]
agent2_energy_min, agent2_attention_min
[-47.031  -0.651]
agent3_energy_min, agent3_attention_min
[-39.104  -9.765]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -110.84190776360535, time: 449.992
agent0_energy_min, agent0_attention_min
[-47.376  -1.703]
agent1_energy_min, agent1_attention_min
[-47.8    -0.171]
agent2_energy_min, agent2_attention_min
[-48.167  -0.363]
agent3_energy_min, agent3_attention_min
[-34.663 -11.213]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -112.93991218908913, time: 450.041
agent0_energy_min, agent0_attention_min
[-49.308  -0.21 ]
agent1_energy_min, agent1_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_4agents-3__2018-07-15_17-41-17...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -325.651434207208, time: 467.725
agent0_energy_min, agent0_attention_min
[-16.84884885 -17.24424424]
agent1_energy_min, agent1_attention_min
[-15.995996  -15.5975976]
agent2_energy_min, agent2_attention_min
[-14.80780781 -18.77177177]
agent3_energy_min, agent3_attention_min
[-17.63463463 -16.73273273]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -786.7777765049607, time: 718.62
agent0_energy_min, agent0_attention_min
[-15.31  -17.323]
agent1_energy_min, agent1_attention_min
[-10.192 -18.75 ]
agent2_energy_min, agent2_attention_min
[-10.268 -25.521]
agent3_energy_min, agent3_attention_min
[-11.718 -18.785]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -282.6178149307362, time: 723.043
agent0_energy_min, agent0_attention_min
[ -8.104 -35.508]
agent1_energy_min, agent1_attention_min
[ -9.903 -33.209]
agent2_energy_min, agent2_attention_min
[ -8.887 -34.949]
agent3_energy_min, agent3_attention_min
[ -4.705 -10.85 ]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -241.69001694764717, time: 716.145
agent0_energy_min, agent0_attention_min
[ -2.476 -45.947]
agent1_energy_min, agent1_attention_min
[ -6.516 -42.279]
agent2_energy_min, agent2_attention_min
[ -8.988 -38.822]
agent3_energy_min, agent3_attention_min
[-10.456 -24.155]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -231.13340363815422, time: 708.813
agent0_energy_min, agent0_attention_min
[ -2.596 -42.987]
agent1_energy_min, agent1_attention_min
[ -6.918 -42.352]
agent2_energy_min, agent2_attention_min
[ -8.861 -38.59 ]
agent3_energy_min, agent3_attention_min
[-12.041 -36.281]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -220.77536126930187, time: 707.412
agent0_energy_min, agent0_attention_min
[ -2.078 -43.428]
agent1_energy_min, agent1_attention_min
[ -7.557 -41.844]
agent2_energy_min, agent2_attention_min
[ -7.736 -40.276]
agent3_energy_min, agent3_attention_min
[-10.624 -38.906]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -215.91063818139753, time: 714.959
agent0_energy_min, agent0_attention_min
[ -3.388 -43.809]
agent1_energy_min, agent1_attention_min
[-10.047 -39.579]
agent2_energy_min, agent2_attention_min
[ -7.424 -41.085]
agent3_energy_min, agent3_attention_min
[-11.65 -36.87]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -209.9001441774393, time: 712.003
agent0_energy_min, agent0_attention_min
[ -2.36  -45.472]
agent1_energy_min, agent1_attention_min
[ -6.981 -42.504]
agent2_energy_min, agent2_attention_min
[ -4.92  -43.396]
agent3_energy_min, agent3_attention_min
[-10.955 -37.909]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -207.43424292669, time: 708.377
agent0_energy_min, agent0_attention_min
[ -2.892 -45.012]
agent1_energy_min, agent1_attention_min
[ -8.374 -41.199]
agent2_energy_min, agent2_attention_min
[ -4.543 -44.204]
agent3_energy_min, agent3_attention_min
[-13.737 -35.18 ]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -197.09057606604398, time: 694.552
agent0_energy_min, agent0_attention_min
[ -3.595 -45.124]
agent1_energy_min, agent1_attention_min
[ -9.917 -39.785]
agent2_energy_min, agent2_attention_min
[ -4.655 -44.788]
agent3_energy_min, agent3_attention_min
[-17.506 -30.609]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -192.42276691334771, time: 678.41
agent0_energy_min, agent0_attention_min
[ -4.831 -43.977]
agent1_energy_min, agent1_attention_min
[-19.299 -30.36 ]
agent2_energy_min, agent2_attention_min
[ -5.545 -44.046]
agent3_energy_min, agent3_attention_min
[-13.398 -35.583]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -176.1537948045982, time: 679.698
agent0_energy_min, agent0_attention_min
[ -4.302 -44.679]
agent1_energy_min, agent1_attention_min
[-23.929 -25.747]
agent2_energy_min, agent2_attention_min
[ -5.137 -44.542]
agent3_energy_min, agent3_attention_min
[-12.684 -36.458]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -164.99686239168986, time: 680.122
agent0_energy_min, agent0_attention_min
[ -3.353 -45.741]
agent1_energy_min, agent1_attention_min
[-31.907 -17.855]
agent2_energy_min, agent2_attention_min
[ -3.349 -46.44 ]
agent3_energy_min, agent3_attention_min
[-16.638 -32.452]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -176.59238739295722, time: 678.883
agent0_energy_min, agent0_attention_min
[ -3.204 -46.286]
agent1_energy_min, agent1_attention_min
[-31.623 -17.941]
agent2_energy_min, agent2_attention_min
[ -2.834 -46.871]
agent3_energy_min, agent3_attention_min
[-17.396 -31.396]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -161.98400128511625, time: 685.275
agent0_energy_min, agent0_attention_min
[ -2.161 -47.213]
agent1_energy_min, agent1_attention_min
[-31.924 -17.896]
agent2_energy_min, agent2_attention_min
[ -2.303 -47.54 ]
agent3_energy_min, agent3_attention_min
[-25.398 -23.126]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -155.9984358042441, time: 688.075
agent0_energy_min, agent0_attention_min
[ -3.105 -45.979]
agent1_energy_min, agent1_attention_min
[-35.714 -13.862]
agent2_energy_min, agent2_attention_min
[ -3.126 -46.556]
agent3_energy_min, agent3_attention_min
[-26.8   -21.598]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -155.726463241529, time: 690.469
agent0_energy_min, agent0_attention_min
[ -2.947 -46.453]
agent1_energy_min, agent1_attention_min
[-39.66   -9.996]
agent2_energy_min, agent2_attention_min
[ -2.12  -47.647]
agent3_energy_min, agent3_attention_min
[-24.785 -23.428]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -156.4951869723536, time: 682.386
agent0_energy_min, agent0_attention_min
[ -2.579 -46.834]
agent1_energy_min, agent1_attention_min
[-41.351  -8.171]
agent2_energy_min, agent2_attention_min
[ -2.957 -46.775]
agent3_energy_min, agent3_attention_min
[-23.301 -22.873]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -154.58511685567072, time: 444.889
agent0_energy_min, agent0_attention_min
[ -1.183 -47.883]
agent1_energy_min, agent1_attention_min
[-42.896  -6.852]
agent2_energy_min, agent2_attention_min
[ -2.908 -46.712]
agent3_energy_min, agent3_attention_min
[-28.091 -17.48 ]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -151.93776506342758, time: 442.406
agent0_energy_min, agent0_attention_min
[ -1.264 -48.225]
agent1_energy_min, agent1_attention_min
[-38.37  -11.435]
agent2_energy_min, agent2_attention_min
[ -3.245 -46.268]
agent3_energy_min, agent3_attention_min
[-25.704 -18.916]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -152.66328422726306, time: 442.646
agent0_energy_min, agent0_attention_min
[ -0.817 -48.832]
agent1_energy_min, agent1_attention_min
[-36.224 -13.581]
agent2_energy_min, agent2_attention_min
[ -2.586 -46.71 ]
agent3_energy_min, agent3_attention_min
[-32.184 -13.073]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -135.82902127870335, time: 443.841
agent0_energy_min, agent0_attention_min
[ -1.024 -48.676]
agent1_energy_min, agent1_attention_min
[-36.398 -13.47 ]
agent2_energy_min, agent2_attention_min
[ -2.533 -46.909]
agent3_energy_min, agent3_attention_min
[-37.973  -6.12 ]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -132.892940394503, time: 442.606
agent0_energy_min, agent0_attention_min
[ -1.151 -48.578]
agent1_energy_min, agent1_attention_min
[-31.105 -18.323]
agent2_energy_min, agent2_attention_min
[ -2.656 -46.61 ]
agent3_energy_min, agent3_attention_min
[-38.302  -3.691]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -155.46423927645415, time: 444.771
agent0_energy_min, agent0_attention_min
[ -1.616 -48.126]
agent1_energy_min, agent1_attention_min
[-20.568 -23.088]
agent2_energy_min, agent2_attention_min
[ -3.184 -46.24 ]
agent3_energy_min, agent3_attention_min
[-33.709  -6.546]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -131.19752482463664, time: 444.337
agent0_energy_min, agent0_attention_min
[ -1.41  -48.294]
agent1_energy_min, agent1_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_4agents-2__2018-07-15_17-41-14...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -332.9781973042185, time: 463.304
agent0_energy_min, agent0_attention_min
[-14.22622623 -18.4034034 ]
agent1_energy_min, agent1_attention_min
[-15.85785786 -17.08008008]
agent2_energy_min, agent2_attention_min
[-16.73073073 -16.43943944]
agent3_energy_min, agent3_attention_min
[-15.41841842 -19.52652653]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -781.0832995009079, time: 717.608
agent0_energy_min, agent0_attention_min
[-12.356 -25.738]
agent1_energy_min, agent1_attention_min
[ -8.955 -28.521]
agent2_energy_min, agent2_attention_min
[-16.466 -19.591]
agent3_energy_min, agent3_attention_min
[-12.942 -25.538]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -263.88780116258005, time: 718.438
agent0_energy_min, agent0_attention_min
[ -8.958 -39.255]
agent1_energy_min, agent1_attention_min
[ -5.45  -43.204]
agent2_energy_min, agent2_attention_min
[ -5.25  -41.805]
agent3_energy_min, agent3_attention_min
[ -7.942 -39.945]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -231.91403545483723, time: 721.235
agent0_energy_min, agent0_attention_min
[-11.118 -37.406]
agent1_energy_min, agent1_attention_min
[ -3.148 -45.258]
agent2_energy_min, agent2_attention_min
[ -2.566 -46.623]
agent3_energy_min, agent3_attention_min
[ -6.61  -42.489]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -218.67684713161086, time: 710.916
agent0_energy_min, agent0_attention_min
[-12.973 -35.719]
agent1_energy_min, agent1_attention_min
[ -2.32  -46.554]
agent2_energy_min, agent2_attention_min
[ -3.127 -46.116]
agent3_energy_min, agent3_attention_min
[ -6.372 -43.048]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -217.09409627825855, time: 716.79
agent0_energy_min, agent0_attention_min
[-14.09  -34.534]
agent1_energy_min, agent1_attention_min
[ -1.666 -46.337]
agent2_energy_min, agent2_attention_min
[ -3.843 -44.264]
agent3_energy_min, agent3_attention_min
[ -6.431 -42.525]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -203.4596815098764, time: 711.951
agent0_energy_min, agent0_attention_min
[-19.844 -28.137]
agent1_energy_min, agent1_attention_min
[ -1.222 -47.462]
agent2_energy_min, agent2_attention_min
[ -7.485 -39.272]
agent3_energy_min, agent3_attention_min
[ -6.995 -42.414]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -195.5102631307399, time: 698.43
agent0_energy_min, agent0_attention_min
[-22.771 -24.415]
agent1_energy_min, agent1_attention_min
[ -0.83  -48.335]
agent2_energy_min, agent2_attention_min
[ -9.877 -35.828]
agent3_energy_min, agent3_attention_min
[ -6.451 -42.917]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -187.79757083835412, time: 680.192
agent0_energy_min, agent0_attention_min
[-27.768 -18.669]
agent1_energy_min, agent1_attention_min
[ -1.112 -48.619]
agent2_energy_min, agent2_attention_min
[-13.658 -32.292]
agent3_energy_min, agent3_attention_min
[ -7.215 -42.398]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -186.88410100216825, time: 683.326
agent0_energy_min, agent0_attention_min
[-26.006 -19.107]
agent1_energy_min, agent1_attention_min
[ -0.835 -48.864]
agent2_energy_min, agent2_attention_min
[-14.826 -31.312]
agent3_energy_min, agent3_attention_min
[ -7.28  -42.171]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -169.96777012029233, time: 681.721
agent0_energy_min, agent0_attention_min
[-28.699 -15.956]
agent1_energy_min, agent1_attention_min
[ -0.686 -49.147]
agent2_energy_min, agent2_attention_min
[-21.748 -25.297]
agent3_energy_min, agent3_attention_min
[ -6.526 -42.766]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -154.99714213625256, time: 683.622
agent0_energy_min, agent0_attention_min
[-28.196 -14.265]
agent1_energy_min, agent1_attention_min
[ -0.494 -49.195]
agent2_energy_min, agent2_attention_min
[-34.448 -12.762]
agent3_energy_min, agent3_attention_min
[ -6.485 -43.14 ]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -143.42967763938046, time: 678.956
agent0_energy_min, agent0_attention_min
[-30.237 -10.208]
agent1_energy_min, agent1_attention_min
[ -0.694 -49.114]
agent2_energy_min, agent2_attention_min
[-33.238 -14.519]
agent3_energy_min, agent3_attention_min
[ -7.909 -41.885]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -141.5759918849474, time: 684.095
agent0_energy_min, agent0_attention_min
[-32.683  -7.788]
agent1_energy_min, agent1_attention_min
[ -1.111 -48.737]
agent2_energy_min, agent2_attention_min
[-31.679 -16.764]
agent3_energy_min, agent3_attention_min
[ -6.132 -43.783]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -143.79204287573833, time: 686.903
agent0_energy_min, agent0_attention_min
[-31.041  -9.557]
agent1_energy_min, agent1_attention_min
[ -1.289 -48.666]
agent2_energy_min, agent2_attention_min
[-32.29  -16.607]
agent3_energy_min, agent3_attention_min
[ -6.179 -43.69 ]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -138.80120766260904, time: 687.774
agent0_energy_min, agent0_attention_min
[-30.166  -9.278]
agent1_energy_min, agent1_attention_min
[ -1.023 -48.885]
agent2_energy_min, agent2_attention_min
[-28.834 -20.205]
agent3_energy_min, agent3_attention_min
[ -6.31  -43.518]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -140.63369128160073, time: 692.068
agent0_energy_min, agent0_attention_min
[-33.451  -8.271]
agent1_energy_min, agent1_attention_min
[ -1.474 -48.369]
agent2_energy_min, agent2_attention_min
[-32.566 -16.933]
agent3_energy_min, agent3_attention_min
[ -8.563 -41.192]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -132.15108178634446, time: 689.76
agent0_energy_min, agent0_attention_min
[-36.567  -6.124]
agent1_energy_min, agent1_attention_min
[ -1.176 -48.786]
agent2_energy_min, agent2_attention_min
[-30.481 -17.784]
agent3_energy_min, agent3_attention_min
[ -7.561 -42.241]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -134.6824078999846, time: 455.426
agent0_energy_min, agent0_attention_min
[-34.725  -5.137]
agent1_energy_min, agent1_attention_min
[ -1.176 -48.794]
agent2_energy_min, agent2_attention_min
[-33.962 -12.733]
agent3_energy_min, agent3_attention_min
[ -4.879 -44.63 ]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -141.34916716319114, time: 443.51
agent0_energy_min, agent0_attention_min
[-35.079  -3.663]
agent1_energy_min, agent1_attention_min
[ -1.188 -48.753]
agent2_energy_min, agent2_attention_min
[-31.685 -14.842]
agent3_energy_min, agent3_attention_min
[ -5.813 -43.683]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -127.66581507973653, time: 444.656
agent0_energy_min, agent0_attention_min
[-36.625  -2.666]
agent1_energy_min, agent1_attention_min
[ -0.859 -49.039]
agent2_energy_min, agent2_attention_min
[-26.33  -17.914]
agent3_energy_min, agent3_attention_min
[ -9.97  -39.225]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -143.5089986693626, time: 453.781
agent0_energy_min, agent0_attention_min
[-33.204  -4.783]
agent1_energy_min, agent1_attention_min
[ -0.822 -48.708]
agent2_energy_min, agent2_attention_min
[-24.716 -17.644]
agent3_energy_min, agent3_attention_min
[-13.115 -36.087]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -154.06415374111106, time: 445.769
agent0_energy_min, agent0_attention_min
[-33.924  -4.971]
agent1_energy_min, agent1_attention_min
[ -1.489 -48.013]
agent2_energy_min, agent2_attention_min
[-19.535 -21.558]
agent3_energy_min, agent3_attention_min
[-15.142 -34.767]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -140.92108039562982, time: 446.181
agent0_energy_min, agent0_attention_min
[-25.394  -6.161]
agent1_energy_min, agent1_attention_min
[ -2.756 -46.919]
agent2_energy_min, agent2_attention_min
[-22.102 -18.138]
agent3_energy_min, agent3_attention_min
[-16.196 -33.718]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -123.93940946380928, time: 450.051
agent0_energy_min, agent0_attention_min
[-25.935  -3.23 ]
agent1_energy_min, agent1_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_4agents-2__2018-07-15_17-41-13...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -340.7291365803202, time: 458.775
agent0_energy_min, agent0_attention_min
[-15.43943944 -15.36136136]
agent1_energy_min, agent1_attention_min
[-17.0950951  -16.56356356]
agent2_energy_min, agent2_attention_min
[-15.2042042  -17.61461461]
agent3_energy_min, agent3_attention_min
[-16.22122122 -18.18418418]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -881.2123384935353, time: 718.955
agent0_energy_min, agent0_attention_min
[-11.593 -23.58 ]
agent1_energy_min, agent1_attention_min
[-14.452 -25.726]
agent2_energy_min, agent2_attention_min
[-1.371 -9.22 ]
agent3_energy_min, agent3_attention_min
[-18.108 -20.536]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -319.8649065085991, time: 721.5
agent0_energy_min, agent0_attention_min
[ -0.626 -44.172]
agent1_energy_min, agent1_attention_min
[ -5.583 -39.563]
agent2_energy_min, agent2_attention_min
[ -5.71  -24.512]
agent3_energy_min, agent3_attention_min
[ -8.75  -33.017]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -239.13480707204516, time: 722.813
agent0_energy_min, agent0_attention_min
[ -0.262 -47.514]
agent1_energy_min, agent1_attention_min
[ -9.2   -34.121]
agent2_energy_min, agent2_attention_min
[ -5.244 -41.717]
agent3_energy_min, agent3_attention_min
[ -4.555 -42.307]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -234.6110586056155, time: 713.322
agent0_energy_min, agent0_attention_min
[ -0.26  -48.574]
agent1_energy_min, agent1_attention_min
[ -9.438 -35.117]
agent2_energy_min, agent2_attention_min
[ -1.348 -45.579]
agent3_energy_min, agent3_attention_min
[ -1.598 -44.921]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -229.75828727862338, time: 706.261
agent0_energy_min, agent0_attention_min
[ -0.187 -49.073]
agent1_energy_min, agent1_attention_min
[-20.016 -28.334]
agent2_energy_min, agent2_attention_min
[ -1.106 -46.525]
agent3_energy_min, agent3_attention_min
[ -2.229 -45.657]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -217.7603652112566, time: 682.199
agent0_energy_min, agent0_attention_min
[ -0.175 -48.809]
agent1_energy_min, agent1_attention_min
[-24.858 -22.885]
agent2_energy_min, agent2_attention_min
[ -1.352 -47.084]
agent3_energy_min, agent3_attention_min
[ -1.816 -46.563]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -197.32244675558928, time: 682.997
agent0_energy_min, agent0_attention_min
[ -0.094 -48.557]
agent1_energy_min, agent1_attention_min
[-31.454 -16.851]
agent2_energy_min, agent2_attention_min
[ -2.425 -44.928]
agent3_energy_min, agent3_attention_min
[ -1.284 -47.07 ]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -195.37296451274725, time: 679.449
agent0_energy_min, agent0_attention_min
[ -0.389 -48.08 ]
agent1_energy_min, agent1_attention_min
[-36.623 -10.967]
agent2_energy_min, agent2_attention_min
[ -1.572 -45.589]
agent3_energy_min, agent3_attention_min
[ -2.122 -47.185]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -196.51164843696233, time: 687.274
agent0_energy_min, agent0_attention_min
[ -1.124 -47.313]
agent1_energy_min, agent1_attention_min
[-39.516  -8.074]
agent2_energy_min, agent2_attention_min
[ -3.243 -45.358]
agent3_energy_min, agent3_attention_min
[ -3.151 -45.946]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -173.58882281405545, time: 685.956
agent0_energy_min, agent0_attention_min
[ -0.24  -48.266]
agent1_energy_min, agent1_attention_min
[-43.674  -5.173]
agent2_energy_min, agent2_attention_min
[ -5.078 -44.104]
agent3_energy_min, agent3_attention_min
[ -3.044 -46.328]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -181.38858377219958, time: 686.027
agent0_energy_min, agent0_attention_min
[ -0.648 -48.018]
agent1_energy_min, agent1_attention_min
[-45.964  -2.367]
agent2_energy_min, agent2_attention_min
[ -6.098 -43.672]
agent3_energy_min, agent3_attention_min
[ -3.507 -45.603]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -172.8735880457664, time: 689.291
agent0_energy_min, agent0_attention_min
[ -0.196 -48.72 ]
agent1_energy_min, agent1_attention_min
[-46.704  -1.918]
agent2_energy_min, agent2_attention_min
[ -3.217 -46.294]
agent3_energy_min, agent3_attention_min
[ -2.942 -46.452]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -158.8847273214038, time: 688.582
agent0_energy_min, agent0_attention_min
[ -0.568 -48.728]
agent1_energy_min, agent1_attention_min
[-46.097  -2.803]
agent2_energy_min, agent2_attention_min
[ -3.149 -46.684]
agent3_energy_min, agent3_attention_min
[ -3.715 -45.717]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -163.73283165932935, time: 690.64
agent0_energy_min, agent0_attention_min
[ -0.251 -48.536]
agent1_energy_min, agent1_attention_min
[-46.212  -2.646]
agent2_energy_min, agent2_attention_min
[ -2.25  -47.508]
agent3_energy_min, agent3_attention_min
[ -4.487 -45.13 ]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -192.4539238151807, time: 692.531
agent0_energy_min, agent0_attention_min
[ -0.709 -47.374]
agent1_energy_min, agent1_attention_min
[-47.262  -1.552]
agent2_energy_min, agent2_attention_min
[ -2.028 -47.363]
agent3_energy_min, agent3_attention_min
[ -6.064 -42.794]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -160.5084392887655, time: 694.247
agent0_energy_min, agent0_attention_min
[ -1.353 -48.078]
agent1_energy_min, agent1_attention_min
[-48.37   -0.956]
agent2_energy_min, agent2_attention_min
[ -2.093 -47.663]
agent3_energy_min, agent3_attention_min
[ -6.036 -43.09 ]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -140.34973762679587, time: 694.452
agent0_energy_min, agent0_attention_min
[ -0.636 -48.927]
agent1_energy_min, agent1_attention_min
[-48.104  -1.222]
agent2_energy_min, agent2_attention_min
[ -2.017 -47.778]
agent3_energy_min, agent3_attention_min
[ -8.191 -41.281]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -138.56931917888198, time: 461.351
agent0_energy_min, agent0_attention_min
[ -0.743 -48.768]
agent1_energy_min, agent1_attention_min
[-48.298  -1.037]
agent2_energy_min, agent2_attention_min
[ -1.506 -47.949]
agent3_energy_min, agent3_attention_min
[ -9.148 -40.172]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -140.88300568131805, time: 448.827
agent0_energy_min, agent0_attention_min
[ -0.277 -49.292]
agent1_energy_min, agent1_attention_min
[-47.921  -1.338]
agent2_energy_min, agent2_attention_min
[ -1.875 -47.875]
agent3_energy_min, agent3_attention_min
[ -9.862 -38.88 ]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -139.2844532037634, time: 448.255
agent0_energy_min, agent0_attention_min
[ -0.532 -49.014]
agent1_energy_min, agent1_attention_min
[-47.572  -1.618]
agent2_energy_min, agent2_attention_min
[ -1.901 -47.752]
agent3_energy_min, agent3_attention_min
[ -8.595 -39.94 ]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -131.43343780586434, time: 451.734
agent0_energy_min, agent0_attention_min
[ -0.626 -49.174]
agent1_energy_min, agent1_attention_min
[-46.609  -2.496]
agent2_energy_min, agent2_attention_min
[ -1.709 -48.156]
agent3_energy_min, agent3_attention_min
[ -9.068 -40.476]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -127.43035882878338, time: 447.98
agent0_energy_min, agent0_attention_min
[ -1.384 -48.36 ]
agent1_energy_min, agent1_attention_min
[-38.433  -1.993]
agent2_energy_min, agent2_attention_min
[ -1.858 -48.032]
agent3_energy_min, agent3_attention_min
[ -8.349 -41.055]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -144.68008611606498, time: 449.02
agent0_energy_min, agent0_attention_min
[ -1.82  -47.966]
agent1_energy_min, agent1_attention_min
[-36.531  -2.207]
agent2_energy_min, agent2_attention_min
[ -1.782 -47.794]
agent3_energy_min, agent3_attention_min
[-11.579 -38.227]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -210.37108950243808, time: 449.093
agent0_energy_min, agent0_attention_min
[ -2.157 -47.105]
agent1_energy_min, agent1_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_4agents-3__2018-07-15_17-41-19...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -334.130472933623, time: 461.5
agent0_energy_min, agent0_attention_min
[-13.99299299 -17.97697698]
agent1_energy_min, agent1_attention_min
[-15.6006006  -16.59059059]
agent2_energy_min, agent2_attention_min
[-16.62162162 -15.78778779]
agent3_energy_min, agent3_attention_min
[-16.43243243 -16.26626627]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -717.2472935526238, time: 715.887
agent0_energy_min, agent0_attention_min
[-12.506 -21.351]
agent1_energy_min, agent1_attention_min
[-11.007 -20.553]
agent2_energy_min, agent2_attention_min
[ -7.326 -36.051]
agent3_energy_min, agent3_attention_min
[-14.135 -19.486]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -297.6392758671789, time: 717.76
agent0_energy_min, agent0_attention_min
[ -8.323 -37.21 ]
agent1_energy_min, agent1_attention_min
[ -8.258 -36.747]
agent2_energy_min, agent2_attention_min
[ -4.37  -37.712]
agent3_energy_min, agent3_attention_min
[ -6.956 -40.742]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -253.64949389035453, time: 719.648
agent0_energy_min, agent0_attention_min
[ -5.447 -41.949]
agent1_energy_min, agent1_attention_min
[ -8.483 -39.293]
agent2_energy_min, agent2_attention_min
[ -6.75 -38.54]
agent3_energy_min, agent3_attention_min
[ -1.412 -48.199]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -228.23835060384855, time: 714.19
agent0_energy_min, agent0_attention_min
[ -5.552 -42.894]
agent1_energy_min, agent1_attention_min
[ -7.294 -41.105]
agent2_energy_min, agent2_attention_min
[-11.718 -35.527]
agent3_energy_min, agent3_attention_min
[ -1.793 -47.782]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -226.41008050622068, time: 715.643
agent0_energy_min, agent0_attention_min
[ -2.744 -45.627]
agent1_energy_min, agent1_attention_min
[ -7.829 -41.63 ]
agent2_energy_min, agent2_attention_min
[-14.115 -32.058]
agent3_energy_min, agent3_attention_min
[ -1.919 -47.79 ]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -215.59474377942047, time: 713.856
agent0_energy_min, agent0_attention_min
[ -2.297 -46.147]
agent1_energy_min, agent1_attention_min
[ -7.984 -41.491]
agent2_energy_min, agent2_attention_min
[-17.495 -30.647]
agent3_energy_min, agent3_attention_min
[ -1.928 -47.783]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -221.55726551015385, time: 715.485
agent0_energy_min, agent0_attention_min
[ -2.196 -46.866]
agent1_energy_min, agent1_attention_min
[ -9.378 -40.419]
agent2_energy_min, agent2_attention_min
[-23.162 -22.618]
agent3_energy_min, agent3_attention_min
[ -2.023 -47.683]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -210.3794142179401, time: 708.224
agent0_energy_min, agent0_attention_min
[ -1.498 -48.102]
agent1_energy_min, agent1_attention_min
[ -9.444 -40.393]
agent2_energy_min, agent2_attention_min
[-21.714 -26.115]
agent3_energy_min, agent3_attention_min
[ -2.059 -47.756]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -208.62441228890307, time: 715.392
agent0_energy_min, agent0_attention_min
[ -3.836 -45.586]
agent1_energy_min, agent1_attention_min
[-12.221 -37.624]
agent2_energy_min, agent2_attention_min
[-25.645 -22.34 ]
agent3_energy_min, agent3_attention_min
[ -2.576 -47.216]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -196.82884123612482, time: 707.283
agent0_energy_min, agent0_attention_min
[ -3.727 -45.757]
agent1_energy_min, agent1_attention_min
[-14.972 -34.866]
agent2_energy_min, agent2_attention_min
[-29.651 -18.995]
agent3_energy_min, agent3_attention_min
[ -2.685 -47.072]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -178.47069064891798, time: 699.927
agent0_energy_min, agent0_attention_min
[ -5.698 -43.591]
agent1_energy_min, agent1_attention_min
[-20.7   -29.078]
agent2_energy_min, agent2_attention_min
[-35.847 -11.845]
agent3_energy_min, agent3_attention_min
[ -4.021 -45.855]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -171.79675521701387, time: 684.762
agent0_energy_min, agent0_attention_min
[ -8.92  -40.522]
agent1_energy_min, agent1_attention_min
[-27.451 -22.312]
agent2_energy_min, agent2_attention_min
[-35.987 -11.965]
agent3_energy_min, agent3_attention_min
[ -3.841 -45.985]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -179.53107741398935, time: 677.88
agent0_energy_min, agent0_attention_min
[ -8.542 -40.785]
agent1_energy_min, agent1_attention_min
[-32.387 -17.119]
agent2_energy_min, agent2_attention_min
[-41.54   -7.007]
agent3_energy_min, agent3_attention_min
[ -4.31  -45.515]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -172.9050494740298, time: 679.09
agent0_energy_min, agent0_attention_min
[-10.816 -38.948]
agent1_energy_min, agent1_attention_min
[-38.544 -11.114]
agent2_energy_min, agent2_attention_min
[-41.663  -5.112]
agent3_energy_min, agent3_attention_min
[ -5.706 -43.776]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -164.23744288421207, time: 685.551
agent0_energy_min, agent0_attention_min
[-10.807 -38.66 ]
agent1_energy_min, agent1_attention_min
[-36.965 -12.778]
agent2_energy_min, agent2_attention_min
[-40.358  -7.788]
agent3_energy_min, agent3_attention_min
[ -4.454 -44.788]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -155.0975122770317, time: 688.426
agent0_energy_min, agent0_attention_min
[ -9.923 -39.375]
agent1_energy_min, agent1_attention_min
[-32.784 -17.165]
agent2_energy_min, agent2_attention_min
[-48.442  -0.55 ]
agent3_energy_min, agent3_attention_min
[ -5.979 -43.66 ]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -157.94560940313642, time: 658.392
agent0_energy_min, agent0_attention_min
[ -9.274 -39.233]
agent1_energy_min, agent1_attention_min
[-28.299 -21.66 ]
agent2_energy_min, agent2_attention_min
[-42.63   -0.837]
agent3_energy_min, agent3_attention_min
[ -6.206 -43.477]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -152.1243874693284, time: 438.341
agent0_energy_min, agent0_attention_min
[ -6.946 -41.494]
agent1_energy_min, agent1_attention_min
[-35.57  -14.397]
agent2_energy_min, agent2_attention_min
[-47.933  -0.236]
agent3_energy_min, agent3_attention_min
[ -6.127 -43.57 ]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -144.63259284422375, time: 439.012
agent0_energy_min, agent0_attention_min
[ -7.514 -40.507]
agent1_energy_min, agent1_attention_min
[-35.855 -14.085]
agent2_energy_min, agent2_attention_min
[-47.348  -0.166]
agent3_energy_min, agent3_attention_min
[ -4.843 -44.982]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -154.99233562997594, time: 441.647
agent0_energy_min, agent0_attention_min
[ -7.856 -41.199]
agent1_energy_min, agent1_attention_min
[-29.488 -20.493]
agent2_energy_min, agent2_attention_min
[-47.958  -0.753]
agent3_energy_min, agent3_attention_min
[ -2.456 -47.02 ]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -138.765737438823, time: 441.362
agent0_energy_min, agent0_attention_min
[ -6.907 -42.176]
agent1_energy_min, agent1_attention_min
[-30.503 -19.354]
agent2_energy_min, agent2_attention_min
[-47.368  -2.427]
agent3_energy_min, agent3_attention_min
[ -3.943 -45.808]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -148.94090358254607, time: 441.714
agent0_energy_min, agent0_attention_min
[ -5.405 -43.642]
agent1_energy_min, agent1_attention_min
[-23.879 -19.856]
agent2_energy_min, agent2_attention_min
[-42.003  -5.741]
agent3_energy_min, agent3_attention_min
[ -4.142 -45.751]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -134.18186935806622, time: 441.982
agent0_energy_min, agent0_attention_min
[ -2.328 -47.114]
agent1_energy_min, agent1_attention_min
[-22.283 -16.842]
agent2_energy_min, agent2_attention_min
[-30.103 -12.59 ]
agent3_energy_min, agent3_attention_min
[ -5.434 -44.543]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -163.1696745604576, time: 445.726
agent0_energy_min, agent0_attention_min
[ -2.385 -43.242]
agent1_energy_min, agent1_attention_min
agent1_energy_min, agent1_attention_min
[-28.294 -21.001]
agent2_energy_min, agent2_attention_min
[-15.782 -33.752]
agent3_energy_min, agent3_attention_min
[-27.032 -19.649]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -106.4492189187309, time: 433.07
agent0_energy_min, agent0_attention_min
[-4.7195e+01 -1.8000e-02]
agent1_energy_min, agent1_attention_min
[-27.859 -21.952]
agent2_energy_min, agent2_attention_min
[-13.574 -35.992]
agent3_energy_min, agent3_attention_min
[-41.237  -8.536]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -139.2116649952894, time: 437.665
agent0_energy_min, agent0_attention_min
[-4.6352e+01 -1.1000e-02]
agent1_energy_min, agent1_attention_min
[-23.069 -26.184]
agent2_energy_min, agent2_attention_min
[-13.35  -36.373]
agent3_energy_min, agent3_attention_min
[-42.966  -6.89 ]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -125.61921059349436, time: 435.24
agent0_energy_min, agent0_attention_min
[-41.487  -1.387]
agent1_energy_min, agent1_attention_min
[-26.406 -22.947]
agent2_energy_min, agent2_attention_min
[-13.061 -36.596]
agent3_energy_min, agent3_attention_min
[-36.871 -12.851]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -134.94978371642839, time: 435.99
agent0_energy_min, agent0_attention_min
[-40.101  -3.238]
agent1_energy_min, agent1_attention_min
[-28.751 -20.853]
agent2_energy_min, agent2_attention_min
[-14.249 -34.977]
agent3_energy_min, agent3_attention_min
[-41.158  -8.434]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -128.78611057414892, time: 438.074
agent0_energy_min, agent0_attention_min
[-43.444  -0.413]
agent1_energy_min, agent1_attention_min
[-31.466 -15.034]
agent2_energy_min, agent2_attention_min
[-14.584 -35.129]
agent3_energy_min, agent3_attention_min
[-44.559  -5.397]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -100.65618831372498, time: 439.067
agent0_energy_min, agent0_attention_min
[-34.205  -4.163]
agent1_energy_min, agent1_attention_min
[-38.659 -11.205]
agent2_energy_min, agent2_attention_min
[-10.362 -39.588]
agent3_energy_min, agent3_attention_min
[-45.327  -4.667]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -99.31873435177499, time: 441.755
agent0_energy_min, agent0_attention_min
[-29.704  -7.663]
agent1_energy_min, agent1_attention_min
[-26.94  -22.867]
agent2_energy_min, agent2_attention_min
[-10.888 -38.985]
agent3_energy_min, agent3_attention_min
[-41.803  -5.092]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -112.09067306319592, time: 438.637
agent0_energy_min, agent0_attention_min
[-32.787  -3.827]
agent1_energy_min, agent1_attention_min
[-30.524 -18.325]
agent2_energy_min, agent2_attention_min
[-13.585 -35.954]
agent3_energy_min, agent3_attention_min
[-45.323  -2.858]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -115.77075000740408, time: 440.671
agent0_energy_min, agent0_attention_min
[-37.879  -4.881]
agent1_energy_min, agent1_attention_min
[-37.258 -10.698]
agent2_energy_min, agent2_attention_min
[ -8.852 -40.855]
agent3_energy_min, agent3_attention_min
[-46.221  -1.726]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -94.55427568121186, time: 442.286
agent0_energy_min, agent0_attention_min
[-38.345  -5.449]
agent1_energy_min, agent1_attention_min
[-36.932 -11.421]
agent2_energy_min, agent2_attention_min
[ -6.392 -43.342]
agent3_energy_min, agent3_attention_min
[-43.845  -2.573]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -133.4518977667407, time: 443.798
agent0_energy_min, agent0_attention_min
[-31.112  -5.043]
agent1_energy_min, agent1_attention_min
[-31.601 -16.992]
agent2_energy_min, agent2_attention_min
[-12.301 -37.317]
agent3_energy_min, agent3_attention_min
[-42.68   -4.254]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -95.00882084917805, time: 443.563
agent0_energy_min, agent0_attention_min
[-27.544  -3.363]
agent1_energy_min, agent1_attention_min
[-37.694 -11.806]
agent2_energy_min, agent2_attention_min
[ -8.939 -40.599]
agent3_energy_min, agent3_attention_min
[-43.119  -3.941]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -105.53711605713329, time: 444.259
agent0_energy_min, agent0_attention_min
[-33.421  -3.554]
agent1_energy_min, agent1_attention_min
[-27.048 -19.534]
agent2_energy_min, agent2_attention_min
[ -8.005 -41.665]
agent3_energy_min, agent3_attention_min
[-41.288  -2.328]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -98.51342993285415, time: 446.657
agent0_energy_min, agent0_attention_min
[-33.152  -3.591]
agent1_energy_min, agent1_attention_min
[-26.004  -9.982]
agent2_energy_min, agent2_attention_min
[ -9.557 -40.239]
agent3_energy_min, agent3_attention_min
[-37.7    -2.577]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -100.0266432872485, time: 445.743
agent0_energy_min, agent0_attention_min
[-34.548  -3.374]
agent1_energy_min, agent1_attention_min
[-33.999  -5.641]
agent2_energy_min, agent2_attention_min
[-12.673 -37.169]
agent3_energy_min, agent3_attention_min
[-34.525  -2.681]
...Finished!
Trained episodes: 1 -> 40000
Total time: 6.09 hr

[-45.454  -2.1  ]
agent2_energy_min, agent2_attention_min
[-41.073  -2.909]
agent3_energy_min, agent3_attention_min
[-35.673 -12.76 ]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -98.242477499359, time: 450.414
agent0_energy_min, agent0_attention_min
[-49.786  -0.134]
agent1_energy_min, agent1_attention_min
[-46.007  -2.398]
agent2_energy_min, agent2_attention_min
[-40.343  -3.327]
agent3_energy_min, agent3_attention_min
[-34.1   -14.526]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -136.32455598566423, time: 452.294
agent0_energy_min, agent0_attention_min
[-49.496  -0.468]
agent1_energy_min, agent1_attention_min
[-45.209  -2.229]
agent2_energy_min, agent2_attention_min
[-32.717  -3.972]
agent3_energy_min, agent3_attention_min
[-34.743 -14.248]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -115.8266198139007, time: 441.903
agent0_energy_min, agent0_attention_min
[-49.227  -0.313]
agent1_energy_min, agent1_attention_min
[-46.597  -2.03 ]
agent2_energy_min, agent2_attention_min
[-32.127  -6.81 ]
agent3_energy_min, agent3_attention_min
[-33.272 -15.707]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -103.7801114660413, time: 441.281
agent0_energy_min, agent0_attention_min
[-47.696  -1.552]
agent1_energy_min, agent1_attention_min
[-48.459  -0.459]
agent2_energy_min, agent2_attention_min
[-37.116  -7.677]
agent3_energy_min, agent3_attention_min
[-34.668 -14.431]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -97.73072992079447, time: 436.415
agent0_energy_min, agent0_attention_min
[-49.037  -0.762]
agent1_energy_min, agent1_attention_min
[-48.522  -0.22 ]
agent2_energy_min, agent2_attention_min
[-32.84  -11.876]
agent3_energy_min, agent3_attention_min
[-30.429 -18.791]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -110.00386663874805, time: 438.079
agent0_energy_min, agent0_attention_min
[-47.533  -1.587]
agent1_energy_min, agent1_attention_min
[-47.824  -0.673]
agent2_energy_min, agent2_attention_min
[-32.229 -13.486]
agent3_energy_min, agent3_attention_min
[-30.39  -18.868]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -115.87511228461621, time: 438.18
agent0_energy_min, agent0_attention_min
[-47.141  -0.238]
agent1_energy_min, agent1_attention_min
[-46.017  -1.258]
agent2_energy_min, agent2_attention_min
[-34.608 -10.872]
agent3_energy_min, agent3_attention_min
[-31.362 -18.112]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -130.60388164947165, time: 439.03
agent0_energy_min, agent0_attention_min
[-49.248  -0.146]
agent1_energy_min, agent1_attention_min
[-46.607  -1.219]
agent2_energy_min, agent2_attention_min
[-34.786 -10.877]
agent3_energy_min, agent3_attention_min
[-29.697 -19.402]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -98.62866249883582, time: 438.708
agent0_energy_min, agent0_attention_min
[-47.909  -0.484]
agent1_energy_min, agent1_attention_min
[-48.526  -0.301]
agent2_energy_min, agent2_attention_min
[-39.048  -8.754]
agent3_energy_min, agent3_attention_min
[-30.005 -19.832]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -95.9770452509431, time: 437.911
agent0_energy_min, agent0_attention_min
[-48.408  -1.535]
agent1_energy_min, agent1_attention_min
[-48.897  -0.219]
agent2_energy_min, agent2_attention_min
[-37.808  -8.702]
agent3_energy_min, agent3_attention_min
[-26.102 -23.855]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -105.40045376491732, time: 441.547
agent0_energy_min, agent0_attention_min
[-47.007  -2.944]
agent1_energy_min, agent1_attention_min
[-46.447  -0.604]
agent2_energy_min, agent2_attention_min
[-36.606  -9.175]
agent3_energy_min, agent3_attention_min
[-26.839 -23.096]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -106.52879709812345, time: 439.779
agent0_energy_min, agent0_attention_min
[-47.99  -0.97]
agent1_energy_min, agent1_attention_min
[-47.76  -0.97]
agent2_energy_min, agent2_attention_min
[-34.701  -8.391]
agent3_energy_min, agent3_attention_min
[-26.839 -23.1  ]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -102.83521190000057, time: 441.131
agent0_energy_min, agent0_attention_min
[-47.761  -1.218]
agent1_energy_min, agent1_attention_min
[-48.436  -0.533]
agent2_energy_min, agent2_attention_min
[-34.524  -6.972]
agent3_energy_min, agent3_attention_min
[-26.669 -23.276]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -95.92925001604138, time: 440.745
agent0_energy_min, agent0_attention_min
[-48.436  -0.838]
agent1_energy_min, agent1_attention_min
[-49.085  -0.359]
agent2_energy_min, agent2_attention_min
[-30.77  -9.55]
agent3_energy_min, agent3_attention_min
[-29.517 -20.424]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -117.58808516784362, time: 436.417
agent0_energy_min, agent0_attention_min
[-48.226  -1.182]
agent1_energy_min, agent1_attention_min
[-48.896  -0.387]
agent2_energy_min, agent2_attention_min
[-36.569  -6.518]
agent3_energy_min, agent3_attention_min
[-30.468 -19.449]
...Finished!
Trained episodes: 1 -> 40000
Total time: 6.12 hr

[-39.12   -1.584]
agent2_energy_min, agent2_attention_min
[ -2.909 -46.226]
agent3_energy_min, agent3_attention_min
[-17.242 -29.703]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -150.56872250484528, time: 452.181
agent0_energy_min, agent0_attention_min
[ -0.629 -48.579]
agent1_energy_min, agent1_attention_min
[-30.803  -6.686]
agent2_energy_min, agent2_attention_min
[ -2.373 -47.467]
agent3_energy_min, agent3_attention_min
[ -5.709 -43.546]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -142.38975289272, time: 456.151
agent0_energy_min, agent0_attention_min
[ -0.64  -49.038]
agent1_energy_min, agent1_attention_min
[-31.548  -6.694]
agent2_energy_min, agent2_attention_min
[ -1.605 -47.722]
agent3_energy_min, agent3_attention_min
[ -6.619 -43.139]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -130.08249337577257, time: 454.774
agent0_energy_min, agent0_attention_min
[ -1.01  -48.526]
agent1_energy_min, agent1_attention_min
[-30.114  -4.906]
agent2_energy_min, agent2_attention_min
[ -1.691 -48.226]
agent3_energy_min, agent3_attention_min
[-10.266 -39.575]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -134.1824674527142, time: 453.029
agent0_energy_min, agent0_attention_min
[ -0.838 -48.865]
agent1_energy_min, agent1_attention_min
[-34.664  -2.83 ]
agent2_energy_min, agent2_attention_min
[ -1.244 -48.685]
agent3_energy_min, agent3_attention_min
[-14.146 -35.578]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -123.12781441684812, time: 452.154
agent0_energy_min, agent0_attention_min
[ -1.752 -48.125]
agent1_energy_min, agent1_attention_min
[-32.413  -2.747]
agent2_energy_min, agent2_attention_min
[ -0.642 -49.036]
agent3_energy_min, agent3_attention_min
[-12.033 -37.825]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -167.8731575429832, time: 456.504
agent0_energy_min, agent0_attention_min
[ -1.429 -48.453]
agent1_energy_min, agent1_attention_min
[-27.478  -1.335]
agent2_energy_min, agent2_attention_min
[ -0.636 -49.111]
agent3_energy_min, agent3_attention_min
[ -9.994 -39.828]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -132.09323819384232, time: 454.539
agent0_energy_min, agent0_attention_min
[ -1.061 -48.792]
agent1_energy_min, agent1_attention_min
[-31.628  -1.95 ]
agent2_energy_min, agent2_attention_min
[ -0.538 -49.074]
agent3_energy_min, agent3_attention_min
[ -9.524 -40.201]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -121.4823158979871, time: 453.463
agent0_energy_min, agent0_attention_min
[ -0.327 -49.423]
agent1_energy_min, agent1_attention_min
[-29.597  -1.268]
agent2_energy_min, agent2_attention_min
[ -0.465 -49.463]
agent3_energy_min, agent3_attention_min
[ -4.015 -44.807]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -148.17951087363664, time: 451.891
agent0_energy_min, agent0_attention_min
[ -0.649 -49.056]
agent1_energy_min, agent1_attention_min
[-27.198  -1.605]
agent2_energy_min, agent2_attention_min
[ -0.821 -49.156]
agent3_energy_min, agent3_attention_min
[ -3.959 -42.4  ]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -121.6989738241368, time: 451.426
agent0_energy_min, agent0_attention_min
[ -0.384 -49.261]
agent1_energy_min, agent1_attention_min
[-24.163  -2.271]
agent2_energy_min, agent2_attention_min
[ -0.729 -49.209]
agent3_energy_min, agent3_attention_min
[ -5.218 -38.514]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -131.57404273197815, time: 447.303
agent0_energy_min, agent0_attention_min
[ -0.487 -49.183]
agent1_energy_min, agent1_attention_min
[-26.705  -2.328]
agent2_energy_min, agent2_attention_min
[ -0.929 -49.025]
agent3_energy_min, agent3_attention_min
[ -8.074 -38.522]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -156.43137058419472, time: 442.027
agent0_energy_min, agent0_attention_min
[ -1.123 -48.136]
agent1_energy_min, agent1_attention_min
[-26.091  -2.832]
agent2_energy_min, agent2_attention_min
[ -1.752 -48.169]
agent3_energy_min, agent3_attention_min
[-10.097 -38.842]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -125.72945404709613, time: 441.051
agent0_energy_min, agent0_attention_min
[ -0.62  -48.906]
agent1_energy_min, agent1_attention_min
[-28.765  -1.057]
agent2_energy_min, agent2_attention_min
[ -0.979 -48.794]
agent3_energy_min, agent3_attention_min
[ -7.919 -40.95 ]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -133.8880878188149, time: 440.029
agent0_energy_min, agent0_attention_min
[ -0.678 -48.651]
agent1_energy_min, agent1_attention_min
[-29.089  -1.107]
agent2_energy_min, agent2_attention_min
[ -0.878 -48.704]
agent3_energy_min, agent3_attention_min
[-12.581 -36.682]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -132.37273835323188, time: 420.861
agent0_energy_min, agent0_attention_min
[ -0.479 -48.959]
agent1_energy_min, agent1_attention_min
[-34.155  -0.704]
agent2_energy_min, agent2_attention_min
[ -1.066 -48.655]
agent3_energy_min, agent3_attention_min
[-16.288 -33.274]
...Finished!
Trained episodes: 1 -> 40000
Total time: 6.16 hr

[ -2.19  -47.774]
agent2_energy_min, agent2_attention_min
[-14.766 -21.402]
agent3_energy_min, agent3_attention_min
[-17.994 -31.913]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -126.41371743394046, time: 448.708
agent0_energy_min, agent0_attention_min
[-24.992  -6.292]
agent1_energy_min, agent1_attention_min
[ -2.773 -47.211]
agent2_energy_min, agent2_attention_min
[-13.469 -22.954]
agent3_energy_min, agent3_attention_min
[-18.276 -30.62 ]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -124.92061962291832, time: 448.333
agent0_energy_min, agent0_attention_min
[-24.553  -8.842]
agent1_energy_min, agent1_attention_min
[ -3.082 -46.889]
agent2_energy_min, agent2_attention_min
[-14.546 -26.159]
agent3_energy_min, agent3_attention_min
[-15.828 -34.102]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -133.22022000357404, time: 449.237
agent0_energy_min, agent0_attention_min
[-22.632  -7.318]
agent1_energy_min, agent1_attention_min
[ -3.091 -46.898]
agent2_energy_min, agent2_attention_min
[-15.408 -27.062]
agent3_energy_min, agent3_attention_min
[-15.222 -34.726]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -116.94019356501262, time: 449.604
agent0_energy_min, agent0_attention_min
[-20.283  -9.675]
agent1_energy_min, agent1_attention_min
[ -1.78  -48.201]
agent2_energy_min, agent2_attention_min
[-12.853 -28.023]
agent3_energy_min, agent3_attention_min
[-13.288 -36.684]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -120.81343767799368, time: 450.305
agent0_energy_min, agent0_attention_min
[-19.41  -11.951]
agent1_energy_min, agent1_attention_min
[ -1.89  -48.103]
agent2_energy_min, agent2_attention_min
[-16.024 -24.712]
agent3_energy_min, agent3_attention_min
[-12.075 -37.903]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -114.7793369741861, time: 450.918
agent0_energy_min, agent0_attention_min
[-19.01 -14.91]
agent1_energy_min, agent1_attention_min
[ -1.399 -48.598]
agent2_energy_min, agent2_attention_min
[-14.341 -27.723]
agent3_energy_min, agent3_attention_min
[-10.951 -39.027]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -125.04807870882124, time: 454.805
agent0_energy_min, agent0_attention_min
[-22.392 -12.669]
agent1_energy_min, agent1_attention_min
[ -2.294 -47.689]
agent2_energy_min, agent2_attention_min
[-18.208 -21.071]
agent3_energy_min, agent3_attention_min
[-14.615 -35.331]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -110.42613220394834, time: 452.735
agent0_energy_min, agent0_attention_min
[-18.977 -11.809]
agent1_energy_min, agent1_attention_min
[ -2.318 -47.594]
agent2_energy_min, agent2_attention_min
[-18.48  -19.419]
agent3_energy_min, agent3_attention_min
[-12.741 -37.239]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -119.74627001614223, time: 452.725
agent0_energy_min, agent0_attention_min
[-18.232 -11.008]
agent1_energy_min, agent1_attention_min
[ -2.584 -47.409]
agent2_energy_min, agent2_attention_min
[-20.298 -18.921]
agent3_energy_min, agent3_attention_min
[-11.947 -37.942]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -122.22178776982236, time: 453.265
agent0_energy_min, agent0_attention_min
[-15.919 -13.61 ]
agent1_energy_min, agent1_attention_min
[ -2.419 -47.568]
agent2_energy_min, agent2_attention_min
[-19.114 -19.192]
agent3_energy_min, agent3_attention_min
[-12.103 -37.757]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -109.71259433378515, time: 458.059
agent0_energy_min, agent0_attention_min
[-16.637 -12.855]
agent1_energy_min, agent1_attention_min
[ -2.157 -47.811]
agent2_energy_min, agent2_attention_min
[-19.394 -21.854]
agent3_energy_min, agent3_attention_min
[-15.487 -34.235]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -105.98972954634128, time: 451.634
agent0_energy_min, agent0_attention_min
[-12.644 -17.141]
agent1_energy_min, agent1_attention_min
[ -1.49  -48.501]
agent2_energy_min, agent2_attention_min
[-17.242 -21.118]
agent3_energy_min, agent3_attention_min
[-10.119 -39.791]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -111.34649486690626, time: 458.283
agent0_energy_min, agent0_attention_min
[-13.844 -15.7  ]
agent1_energy_min, agent1_attention_min
[ -1.845 -47.968]
agent2_energy_min, agent2_attention_min
[-19.955 -17.897]
agent3_energy_min, agent3_attention_min
[ -8.797 -41.042]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -119.61488596944939, time: 454.019
agent0_energy_min, agent0_attention_min
[-13.63  -20.096]
agent1_energy_min, agent1_attention_min
[ -1.9   -48.058]
agent2_energy_min, agent2_attention_min
[-17.542 -20.553]
agent3_energy_min, agent3_attention_min
[-10.786 -38.622]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -117.6456983944978, time: 429.467
agent0_energy_min, agent0_attention_min
[-12.713 -18.585]
agent1_energy_min, agent1_attention_min
[ -2.203 -47.726]
agent2_energy_min, agent2_attention_min
[-15.472 -23.367]
agent3_energy_min, agent3_attention_min
[-10.159 -38.824]
...Finished!
Trained episodes: 1 -> 40000
Total time: 6.17 hr

[-17.633 -19.756]
agent2_energy_min, agent2_attention_min
[ -2.918 -46.225]
agent3_energy_min, agent3_attention_min
[-20.254 -13.241]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -128.32473362126302, time: 446.39
agent0_energy_min, agent0_attention_min
[ -1.43  -48.204]
agent1_energy_min, agent1_attention_min
[-15.569 -18.658]
agent2_energy_min, agent2_attention_min
[ -3.34  -45.806]
agent3_energy_min, agent3_attention_min
[-22.526 -12.503]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -136.36424573097653, time: 448.002
agent0_energy_min, agent0_attention_min
[ -1.746 -48.03 ]
agent1_energy_min, agent1_attention_min
[-14.936 -14.821]
agent2_energy_min, agent2_attention_min
[ -4.991 -44.514]
agent3_energy_min, agent3_attention_min
[-23.302 -11.341]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -162.3835654911357, time: 448.039
agent0_energy_min, agent0_attention_min
[ -1.739 -47.82 ]
agent1_energy_min, agent1_attention_min
[-17.029 -14.464]
agent2_energy_min, agent2_attention_min
[ -2.663 -45.921]
agent3_energy_min, agent3_attention_min
[-25.174 -11.736]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -136.44213787338046, time: 449.35
agent0_energy_min, agent0_attention_min
[ -2.323 -47.265]
agent1_energy_min, agent1_attention_min
[-19.367 -11.883]
agent2_energy_min, agent2_attention_min
[ -3.953 -45.307]
agent3_energy_min, agent3_attention_min
[-16.539 -15.161]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -118.93997273211369, time: 447.743
agent0_energy_min, agent0_attention_min
[ -1.225 -48.36 ]
agent1_energy_min, agent1_attention_min
[-16.27  -11.427]
agent2_energy_min, agent2_attention_min
[ -3.966 -45.954]
agent3_energy_min, agent3_attention_min
[-15.418 -18.74 ]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -123.97269425169938, time: 458.713
agent0_energy_min, agent0_attention_min
[ -1.649 -47.817]
agent1_energy_min, agent1_attention_min
[-19.532 -11.776]
agent2_energy_min, agent2_attention_min
[ -5.8   -44.172]
agent3_energy_min, agent3_attention_min
[-17.071 -17.864]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -121.52572532368916, time: 452.731
agent0_energy_min, agent0_attention_min
[ -1.799 -47.841]
agent1_energy_min, agent1_attention_min
[-18.082 -11.425]
agent2_energy_min, agent2_attention_min
[ -5.16  -44.815]
agent3_energy_min, agent3_attention_min
[-17.911 -19.498]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -125.50810097132516, time: 450.574
agent0_energy_min, agent0_attention_min
[ -1.984 -47.812]
agent1_energy_min, agent1_attention_min
[-17.35 -12.3 ]
agent2_energy_min, agent2_attention_min
[ -3.578 -46.34 ]
agent3_energy_min, agent3_attention_min
[-16.431 -21.668]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -154.41868877106847, time: 452.581
agent0_energy_min, agent0_attention_min
[ -2.931 -46.938]
agent1_energy_min, agent1_attention_min
[-19.325  -6.996]
agent2_energy_min, agent2_attention_min
[ -4.511 -45.454]
agent3_energy_min, agent3_attention_min
[-16.389 -18.636]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -135.35595696167442, time: 457.955
agent0_energy_min, agent0_attention_min
[ -2.376 -47.562]
agent1_energy_min, agent1_attention_min
[-19.596  -4.859]
agent2_energy_min, agent2_attention_min
[ -3.999 -45.941]
agent3_energy_min, agent3_attention_min
[-15.139 -18.755]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -148.84069160377607, time: 455.648
agent0_energy_min, agent0_attention_min
[ -1.932 -48.042]
agent1_energy_min, agent1_attention_min
[-17.839  -4.595]
agent2_energy_min, agent2_attention_min
[ -4.429 -45.016]
agent3_energy_min, agent3_attention_min
[-15.836 -15.623]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -145.57900172889697, time: 455.146
agent0_energy_min, agent0_attention_min
[ -3.035 -46.809]
agent1_energy_min, agent1_attention_min
[-21.999  -3.712]
agent2_energy_min, agent2_attention_min
[ -6.965 -42.791]
agent3_energy_min, agent3_attention_min
[-23.621  -8.742]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -136.41912383852332, time: 458.64
agent0_energy_min, agent0_attention_min
[ -1.407 -48.506]
agent1_energy_min, agent1_attention_min
[-18.715  -8.006]
agent2_energy_min, agent2_attention_min
[ -2.849 -47.139]
agent3_energy_min, agent3_attention_min
[-21.961 -11.874]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -121.46813151349893, time: 456.828
agent0_energy_min, agent0_attention_min
[ -1.449 -48.419]
agent1_energy_min, agent1_attention_min
[-17.16  -13.573]
agent2_energy_min, agent2_attention_min
[ -2.812 -47.171]
agent3_energy_min, agent3_attention_min
[-25.039  -6.602]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -121.57084775787321, time: 429.221
agent0_energy_min, agent0_attention_min
[ -1.923 -47.635]
agent1_energy_min, agent1_attention_min
[-17.106 -11.986]
agent2_energy_min, agent2_attention_min
[ -2.839 -47.15 ]
agent3_energy_min, agent3_attention_min
[-18.942 -10.641]
...Finished!
Trained episodes: 1 -> 40000
Total time: 6.17 hr

[-25.078 -21.564]
agent2_energy_min, agent2_attention_min
[-25.915 -15.091]
agent3_energy_min, agent3_attention_min
[ -5.136 -44.813]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -173.89321885653607, time: 445.993
agent0_energy_min, agent0_attention_min
[ -4.083 -41.376]
agent1_energy_min, agent1_attention_min
[-24.698 -19.312]
agent2_energy_min, agent2_attention_min
[-31.702  -5.104]
agent3_energy_min, agent3_attention_min
[ -5.901 -43.959]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -155.91031384113248, time: 448.969
agent0_energy_min, agent0_attention_min
[ -3.881 -45.075]
agent1_energy_min, agent1_attention_min
[-26.598 -17.165]
agent2_energy_min, agent2_attention_min
[-29.19   -3.575]
agent3_energy_min, agent3_attention_min
[ -6.978 -42.9  ]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -155.04693002221182, time: 447.355
agent0_energy_min, agent0_attention_min
[ -4.562 -44.436]
agent1_energy_min, agent1_attention_min
[-23.419 -13.888]
agent2_energy_min, agent2_attention_min
[-25.742  -3.188]
agent3_energy_min, agent3_attention_min
[ -6.13  -43.271]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -123.95149699961283, time: 449.371
agent0_energy_min, agent0_attention_min
[ -4.525 -44.491]
agent1_energy_min, agent1_attention_min
[-14.263 -15.012]
agent2_energy_min, agent2_attention_min
[-25.553  -0.986]
agent3_energy_min, agent3_attention_min
[ -4.059 -45.885]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -120.34380112473885, time: 448.692
agent0_energy_min, agent0_attention_min
[ -6.836 -42.622]
agent1_energy_min, agent1_attention_min
[-14.169 -15.881]
agent2_energy_min, agent2_attention_min
[-20.929  -0.795]
agent3_energy_min, agent3_attention_min
[ -4.123 -45.799]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -127.28158284330583, time: 450.629
agent0_energy_min, agent0_attention_min
[ -5.139 -44.334]
agent1_energy_min, agent1_attention_min
[-16.715 -15.026]
agent2_energy_min, agent2_attention_min
[-19.605  -5.161]
agent3_energy_min, agent3_attention_min
[ -3.979 -45.968]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -125.72832983844192, time: 452.999
agent0_energy_min, agent0_attention_min
[ -5.881 -43.534]
agent1_energy_min, agent1_attention_min
[-16.346 -15.132]
agent2_energy_min, agent2_attention_min
[-23.172  -2.798]
agent3_energy_min, agent3_attention_min
[ -4.387 -45.416]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -125.0732109551479, time: 451.653
agent0_energy_min, agent0_attention_min
[ -5.504 -43.945]
agent1_energy_min, agent1_attention_min
[-16.614 -12.591]
agent2_energy_min, agent2_attention_min
[-22.548  -1.047]
agent3_energy_min, agent3_attention_min
[ -4.611 -45.322]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -138.80448481389857, time: 452.02
agent0_energy_min, agent0_attention_min
[ -7.109 -42.368]
agent1_energy_min, agent1_attention_min
[-15.497 -12.823]
agent2_energy_min, agent2_attention_min
[-24.44   -2.068]
agent3_energy_min, agent3_attention_min
[ -5.163 -44.817]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -116.27228552291731, time: 451.905
agent0_energy_min, agent0_attention_min
[ -4.492 -45.329]
agent1_energy_min, agent1_attention_min
[-14.544 -12.429]
agent2_energy_min, agent2_attention_min
[-24.001  -1.158]
agent3_energy_min, agent3_attention_min
[ -4.164 -45.774]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -125.86077394248173, time: 453.393
agent0_energy_min, agent0_attention_min
[ -6.235 -43.612]
agent1_energy_min, agent1_attention_min
[-16.967 -10.719]
agent2_energy_min, agent2_attention_min
[-24.661  -1.227]
agent3_energy_min, agent3_attention_min
[ -4.752 -45.189]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -123.49750829771595, time: 452.964
agent0_energy_min, agent0_attention_min
[ -4.616 -45.202]
agent1_energy_min, agent1_attention_min
[-15.273 -10.   ]
agent2_energy_min, agent2_attention_min
[-25.197  -1.083]
agent3_energy_min, agent3_attention_min
[ -4.509 -45.399]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -147.0850455627119, time: 452.453
agent0_energy_min, agent0_attention_min
[ -6.482 -43.357]
agent1_energy_min, agent1_attention_min
[-17.313  -8.534]
agent2_energy_min, agent2_attention_min
[-26.725  -3.   ]
agent3_energy_min, agent3_attention_min
[ -5.231 -44.274]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -126.90987528446412, time: 453.53
agent0_energy_min, agent0_attention_min
[ -5.608 -44.196]
agent1_energy_min, agent1_attention_min
[-16.385  -9.889]
agent2_energy_min, agent2_attention_min
[-26.789  -2.822]
agent3_energy_min, agent3_attention_min
[ -4.181 -45.627]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -124.94024688173053, time: 429.424
agent0_energy_min, agent0_attention_min
[ -5.779 -44.077]
agent1_energy_min, agent1_attention_min
[-18.825  -7.631]
agent2_energy_min, agent2_attention_min
[-23.84   -5.684]
agent3_energy_min, agent3_attention_min
[ -3.566 -46.347]
...Finished!
Trained episodes: 1 -> 40000
Total time: 6.17 hr
