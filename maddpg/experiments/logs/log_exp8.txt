python train.py --scenario wanderer1_2agents-1 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-2 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-2 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-2 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-2 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-3 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-3 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-3 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_2agents-3 --num-episodes 20000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-2__2018-07-11_20-44-12...
200 50
steps: 9950, episodes: 200, mean episode reward: -188.13184736432078, time: 31.725
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.30150754  -0.47236181 -13.39306533]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.7638191   -0.55778894 -13.24834171]
400 50
steps: 19950, episodes: 400, mean episode reward: -181.05870310494424, time: 34.879
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.385   -0.555  -13.4861]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.425   -0.465  -13.4533]
600 50
steps: 29950, episodes: 600, mean episode reward: -166.57797139936957, time: 34.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.565  -0.475 -13.3  ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.895   -0.51   -13.3152]
800 50
steps: 39950, episodes: 800, mean episode reward: -175.63770136116042, time: 34.382
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.885   -0.51   -13.1621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.705  -0.57  -13.569]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -171.91430288867284, time: 34.304
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.29    -0.495  -13.3109]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.42   -0.575 -13.718]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -755.4076740820093, time: 47.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.745   -0.295   -8.3223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.88    -0.64   -15.9079]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -246.16801427665786, time: 49.143
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.1     -0.765  -15.7659]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.945   -0.63   -14.3661]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -177.49084849521228, time: 48.803
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.675   -0.44    -9.3582]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.365   -0.83   -21.7126]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -160.84966219849449, time: 47.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.125   -0.595  -18.7476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.57    -0.625  -10.4288]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -149.00705431078512, time: 48.014
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.595   -0.51   -13.9541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.77    -0.7    -13.8081]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -157.22077382654663, time: 47.522
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.935   -0.35    -9.4689]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.305  -0.63  -11.662]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -143.65110308161442, time: 46.981
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.305   -0.43   -12.5321]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.83    -0.575  -13.0199]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -134.7653181604173, time: 47.269
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.095   -0.31    -9.6522]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.12    -0.72   -15.8582]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -118.86839991440975, time: 46.713
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.01    -0.42   -13.2322]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.44    -0.645  -15.2887]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -121.49336296729336, time: 47.091
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.42    -0.495  -15.2025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.975   -0.695  -16.6754]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -115.21107974679373, time: 47.269
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.56    -0.565  -15.5953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.75    -0.605  -17.4876]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -107.72016065259452, time: 47.83
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.2     -0.495  -13.8652]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.495   -0.65   -19.8702]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -114.40377892019028, time: 48.33
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.705   -0.385  -12.1076]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.2     -0.78   -21.0547]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -96.57810689500413, time: 47.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.46    -0.42   -13.0544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.695   -0.88   -22.5193]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -96.45934089684943, time: 47.582
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.645   -0.355  -13.3836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.79    -0.87   -22.5568]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -88.46888271258881, time: 47.674
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.255  -0.365 -10.636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.975   -0.9    -22.4971]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -99.65224844768915, time: 50.517
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.925  -0.54  -16.006]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.73    -0.86   -22.5248]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -83.18709902965125, time: 47.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.855   -0.525  -12.6793]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.785   -0.96   -23.7925]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -95.33391292294976, time: 47.842
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.905   -0.435  -12.2567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.235   -0.985  -23.9165]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -82.49466001130726, time: 46.973
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.905   -0.5    -13.7305]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.74    -0.99   -24.7297]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -78.99340857822999, time: 47.511
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.46    -0.57   -15.1347]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.925   -0.965  -24.7909]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -87.02650542216335, time: 47.384
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.25    -0.695  -15.3298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.26   -0.96  -24.394]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -86.50502932746708, time: 48.335
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.955   -0.7    -15.1286]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.575   -0.97   -24.2067]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -90.78046730838662, time: 47.942
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.69    -0.685  -15.5418]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.71   -0.99  -24.324]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -79.72549536483824, time: 47.979
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.125  -0.71  -15.749]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.195   -0.96   -24.9222]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -82.79804930226543, time: 48.663
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.705  -0.665 -13.683]
agent1_energy_min, agent1_energy_max, agent1_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-1__2018-07-11_20-44-10...
200 50
steps: 9950, episodes: 200, mean episode reward: -164.0873364805713, time: 30.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.49246231  -0.49246231 -12.12472362]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.74874372  -0.47236181 -11.72371859]
400 50
steps: 19950, episodes: 400, mean episode reward: -158.0912296202047, time: 34.062
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.015  -0.54  -12.244]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.615   -0.465  -11.6921]
600 50
steps: 29950, episodes: 600, mean episode reward: -166.66829396661188, time: 34.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.215   -0.425  -12.3367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.175   -0.445  -11.4455]
800 50
steps: 39950, episodes: 800, mean episode reward: -182.32045692194123, time: 33.988
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.01    -0.46   -12.1537]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.835   -0.405  -11.7027]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -175.39115859027382, time: 34.522
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.095   -0.52   -12.3992]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.39    -0.45   -11.4549]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -371.49939738743143, time: 47.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.005   -0.345   -8.6952]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.33    -0.555  -12.4254]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -177.41446936306363, time: 49.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.455   -0.44   -11.4976]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.485   -0.61   -14.5367]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -167.60357454132682, time: 48.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.355   -0.205   -7.2964]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.195   -0.36    -8.4796]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -101.39857881239894, time: 48.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.58    -0.47   -14.0434]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.115   -0.55   -13.1204]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -92.87886816666489, time: 50.303
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.845   -0.575  -15.4602]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.955   -0.525  -15.2171]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -95.72088820858045, time: 49.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.135  -0.67  -17.85 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.795   -0.44   -14.1249]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -72.32252496624484, time: 48.051
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.075   -0.73   -19.6086]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.1    -0.48  -16.899]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -75.63501273408758, time: 48.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.885   -0.785  -21.3709]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.825   -0.34   -16.6837]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -68.61798311445219, time: 48.686
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.73   -0.87  -21.951]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.31    -0.48   -17.2159]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -70.33065956397516, time: 48.91
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.29    -0.885  -23.0812]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.645   -0.375  -14.0977]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -61.36736110297451, time: 48.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.485   -0.89   -23.2988]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.89    -0.685  -21.2577]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -65.66497682992878, time: 49.846
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.375   -0.925  -24.4325]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.005   -0.54   -16.5724]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -60.08553218563771, time: 48.777
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.28    -0.86   -24.2191]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.205   -0.55   -18.4827]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -60.13823644854916, time: 48.426
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.235   -0.93   -24.9279]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.66    -0.515  -17.5075]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -59.8624783577161, time: 47.763
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.405   -0.925  -25.0249]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.62    -0.56   -17.6256]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -59.47099061824352, time: 49.49
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.07    -0.955  -24.8301]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.91    -0.68   -19.0754]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -54.18592621688623, time: 49.081
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.23    -0.94   -24.9582]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.39    -0.625  -20.2356]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -63.62026404374507, time: 47.774
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.84    -0.935  -24.6381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.085   -0.78   -21.7227]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -53.61437417912255, time: 48.518
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.21    -0.95   -24.8983]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.225   -0.795  -21.7672]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -62.58884664969257, time: 48.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.29    -0.94   -24.9572]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.825   -0.9    -22.7312]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -49.655449515127685, time: 49.262
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.675   -0.98   -25.2403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.205  -0.83  -22.15 ]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -55.264233248100446, time: 48.605
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.39    -0.99   -25.0496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.44    -0.885  -23.6315]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -55.603969724077, time: 48.308
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.435   -0.98   -25.1238]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.52    -0.95   -23.7134]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -54.429968806232225, time: 48.78
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.545   -0.975  -25.1752]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.225   -0.98   -24.1716]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -48.14989530527701, time: 48.337
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.085   -0.955  -24.8174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.68    -0.915  -23.7139]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -51.57249470201934, time: 48.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.41    -0.98   -25.0727]
agent1_energy_min, agent1_energy_max, agent1_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-2__2018-07-11_20-44-14...
200 50
steps: 9950, episodes: 200, mean episode reward: -183.07214761359245, time: 32.006
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.09547739  -0.49748744 -13.20984925]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.36683417  -0.45226131 -12.81437186]
400 50
steps: 19950, episodes: 400, mean episode reward: -164.66410595581883, time: 35.063
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.79    -0.55   -13.1431]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.71    -0.545  -13.2806]
600 50
steps: 29950, episodes: 600, mean episode reward: -176.71976685907427, time: 35.315
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.3     -0.5    -13.5204]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.22    -0.5    -12.8315]
800 50
steps: 39950, episodes: 800, mean episode reward: -160.49425648537147, time: 34.355
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.045   -0.53   -13.3536]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.165   -0.475  -12.8247]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -180.39956692360687, time: 34.258
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.875   -0.515  -13.3791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.56    -0.47   -13.0056]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -722.6598859743441, time: 47.99
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.57    -0.65   -16.0992]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.01    -0.57   -14.6103]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -291.2621222870444, time: 49.62
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.78    -0.32    -8.7238]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.7    -0.685 -13.458]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -187.9856299002957, time: 50.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.125   -0.655  -13.5395]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.81    -0.68   -12.8296]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -145.55245018105353, time: 49.834
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.225   -0.755  -16.7604]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.24    -0.34   -11.0661]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -142.58402602223208, time: 50.065
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.165   -0.525  -12.5674]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.44    -0.375  -12.0051]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -143.07297191083887, time: 49.457
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.7    -0.075  -1.4699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.78    -0.455  -10.3299]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -116.04424116318879, time: 49.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.02   -0.135  -2.4193]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.825   -0.495  -12.7534]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -136.77607512581548, time: 50.027
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.3    -0.175  -3.2713]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.95    -0.52   -15.8925]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -134.49197110721659, time: 49.249
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.2    -0.13   -1.2542]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.365   -0.695  -16.8558]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -106.82206399739921, time: 48.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.4    -0.155  -2.5442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.395   -0.665  -18.3872]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -111.75631918018117, time: 48.208
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.13   -0.14   -1.7388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.675   -0.71   -20.7009]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -117.74972623361195, time: 48.921
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.45   -0.195  -2.1811]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.305   -0.62   -20.3017]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -110.2841604207265, time: 47.473
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.405  -0.225  -3.1222]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.425   -0.75   -21.3576]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -105.22370621304908, time: 47.591
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.61   -0.09   -2.0861]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.93    -0.715  -21.0085]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -92.327458367535, time: 46.91
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.695  -0.1    -2.4738]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.555   -0.725  -22.3834]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -98.81866431648689, time: 47.8
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.68  -0.19  -4.311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.205   -0.82   -22.5113]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -101.6091201595207, time: 47.563
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.135  -0.215  -2.1159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.8     -0.795  -22.6184]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -101.8321943454344, time: 46.847
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.3    -0.225  -4.0604]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.695   -0.82   -23.0904]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -99.55060114924255, time: 47.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.015  -0.26   -3.7038]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.78   -0.71  -22.824]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -90.64248900716339, time: 47.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.32   -0.26   -3.8815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.325  -0.715 -23.256]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -108.82660704707835, time: 47.76
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.845  -0.265  -2.8291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.59    -0.815  -23.4728]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -106.91126378821131, time: 47.781
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.775 -0.395 -4.869]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.655   -0.63   -22.7024]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -101.06394491551048, time: 47.676
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.135  -0.19   -1.7948]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.345   -0.635  -23.3189]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -98.2258092212514, time: 48.347
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.4    -0.35   -4.6369]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.05    -0.58   -22.3567]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -90.7925918975527, time: 48.271
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.88  -0.27  -4.166]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.54    -0.66   -23.2472]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -81.83163001487577, time: 48.855
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.06    -0.405   -6.2673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.575   -0.7    -23.2678]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-2__2018-07-11_20-44-17...
200 50
steps: 9950, episodes: 200, mean episode reward: -160.0231601299491, time: 32.369
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.28643216  -0.44221106 -12.39507538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.94974874  -0.48743719 -12.18753769]
400 50
steps: 19950, episodes: 400, mean episode reward: -169.66085940079574, time: 33.895
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.53    -0.465  -12.4069]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.02    -0.5    -12.3351]
600 50
steps: 29950, episodes: 600, mean episode reward: -160.140142518399, time: 34.707
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.625   -0.55   -12.4766]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.235   -0.435  -12.3412]
800 50
steps: 39950, episodes: 800, mean episode reward: -175.41756254614708, time: 33.69
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.69    -0.56   -12.6101]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.29    -0.515  -12.3761]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -156.73911506925134, time: 35.232
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.3     -0.505  -12.4339]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.995   -0.445  -12.2108]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -455.3272707229914, time: 48.317
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.58    -0.445  -14.1826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.675   -0.475  -14.0707]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -511.46776229931027, time: 49.799
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.595  -0.37   -6.768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.53    -0.685  -10.1068]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -168.9408332923574, time: 49.914
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.155   -0.515  -13.4179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.31    -0.7    -12.7705]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -122.89729621360105, time: 49.3
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.795   -0.865  -19.7546]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.335   -0.395  -10.1518]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -151.4446070555567, time: 50.193
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.035   -0.875  -18.7669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.15    -0.375  -10.0363]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -157.53079394257736, time: 49.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.48    -0.775  -13.0858]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.42    -0.295   -8.8701]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -115.83728159327607, time: 49.02
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.26    -0.65    -8.9936]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.355   -0.225   -5.7638]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -117.80659362737474, time: 49.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.715   -0.51    -6.8381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.98   -0.145  -4.6548]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -112.08212726887469, time: 49.156
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.445   -0.53    -7.8531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.86   -0.19   -4.9015]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -135.20866050754182, time: 49.961
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.635   -0.515  -11.0615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.875   -0.23    -6.2095]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -142.00808818134556, time: 49.494
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.185   -0.495  -12.6459]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.12  -0.2   -4.126]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -123.93809689958037, time: 50.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.03    -0.46   -10.4058]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.95    -0.145   -5.9201]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -123.23991474340976, time: 49.43
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.28   -0.645 -14.628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.35    -0.195   -6.7082]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -121.12966466945824, time: 49.702
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.095   -0.57   -11.3075]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.61    -0.245   -6.9637]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -148.00826384380025, time: 50.513
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.64    -0.68    -6.9064]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.6    -0.135  -3.0643]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -115.60497462240029, time: 49.255
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.62    -0.58    -8.7547]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.82    -0.245   -7.0112]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -117.6616470307044, time: 48.432
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.125   -0.74   -19.5574]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.85    -0.27   -13.9559]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -128.43729026832253, time: 47.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.48   -0.89  -19.818]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.09    -0.3     -9.0333]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -100.64688982540298, time: 47.668
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.81    -0.875  -19.8912]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.575   -0.275   -9.6355]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -98.49484218493879, time: 47.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.275  -0.94  -22.01 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.645   -0.315   -9.7974]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -94.20173377714116, time: 48.03
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.105  -0.98  -20.962]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.17    -0.305  -11.3187]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -103.00092426160886, time: 47.303
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.      -0.985  -21.3801]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.41    -0.445  -17.9845]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -97.3703804531772, time: 47.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.365   -0.96   -22.5238]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.595   -0.39   -16.9868]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -86.88719847107797, time: 46.654
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.68    -0.975  -22.9746]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.38    -0.3    -14.8509]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -106.17121283000424, time: 46.396
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.865   -0.995  -23.9374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.735   -0.565  -18.6784]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -83.9835770797418, time: 47.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.305   -0.995  -25.1428]
agent1_energy_min, agent1_energy_max, agent1_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-3__2018-07-11_20-44-28...
200 50
steps: 9950, episodes: 200, mean episode reward: -166.52268121241366, time: 34.163
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.01005025  -0.48241206 -10.9360804 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.59798995  -0.43718593 -11.74482412]
400 50
steps: 19950, episodes: 400, mean episode reward: -184.5254676114442, time: 34.48
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.435   -0.46   -11.0879]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.835  -0.455 -11.74 ]
600 50
steps: 29950, episodes: 600, mean episode reward: -166.1589312779489, time: 34.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.99   -0.395 -11.337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.48    -0.525  -12.1142]
800 50
steps: 39950, episodes: 800, mean episode reward: -185.9701491269323, time: 35.056
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.285   -0.485  -11.1036]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.945  -0.53  -11.957]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -184.30866939325088, time: 35.452
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.61    -0.395  -11.2763]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.89    -0.485  -11.8425]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -727.6730564585424, time: 48.383
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.525   -0.54   -13.1881]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.735   -0.585  -13.3411]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -269.37905570877615, time: 49.561
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.75    -0.56   -11.6011]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.04    -0.735  -14.5465]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -151.77842683822408, time: 50.543
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.195 -0.405 -4.916]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.55    -0.505  -14.7526]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -134.53334971460671, time: 50.317
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.14    -0.475   -5.8412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.665   -0.18    -5.5163]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -129.3202444406334, time: 49.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.805   -0.315   -5.6994]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.97   -0.115  -2.7109]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -123.5758078713024, time: 49.287
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.96   -0.19   -5.3208]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.15    -0.21    -5.1154]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -101.39258187541294, time: 49.169
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.91    -0.19    -6.1636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.485  -0.175  -3.1024]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -124.30083966875533, time: 48.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.57    -0.22    -6.9875]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.125 -0.195 -4.479]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -148.82459723820537, time: 49.48
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.58    -0.18    -6.7684]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.13   -0.245  -5.713]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -106.40760042557389, time: 49.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.715   -0.31    -7.9984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.545  -0.22   -3.7106]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -119.79940255101033, time: 50.111
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.66   -0.305  -9.045]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.11   -0.205  -3.3344]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -111.84165582911943, time: 49.887
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.6     -0.395  -10.0412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.4    -0.295  -4.4618]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -127.60295423084322, time: 49.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.34    -0.4    -12.9441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.565  -0.325  -5.2891]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -102.48130045800194, time: 48.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.25    -0.69   -15.7971]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.385 -0.28  -3.484]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -98.46278167230983, time: 50.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.645   -0.405   -8.9664]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.345   -0.5     -9.1154]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -82.32978090046709, time: 50.203
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.07    -0.435   -8.4153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.23    -0.53    -9.8536]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -83.8972861147372, time: 48.306
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.465   -0.58   -10.4418]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.86    -0.78   -17.6211]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -80.6949730775578, time: 48.627
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.215   -0.46   -12.0741]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.38    -0.72   -16.4671]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -82.33154431277188, time: 48.061
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.47    -0.505  -13.3264]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.765   -0.715  -16.6592]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -78.64908042413391, time: 49.308
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.2     -0.645  -14.1361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.625   -0.895  -23.4862]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -82.83657110309794, time: 49.208
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.86    -0.63   -15.7666]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.415   -0.89   -23.3147]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -76.37869871221494, time: 49.179
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.81    -0.66   -15.4077]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.485   -0.94   -24.5502]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -77.59537232012218, time: 48.848
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.87    -0.67   -16.5171]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.26    -0.89   -23.7921]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -98.28971388409482, time: 48.937
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.165   -0.83   -17.6302]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.895   -0.88   -21.9099]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -75.55134118368669, time: 49.12
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.355   -0.8    -14.8174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.59  -0.79 -22.24]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -75.65463822944304, time: 50.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.755   -0.765  -16.8649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.06   -0.83  -21.614]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-2__2018-07-11_20-44-19...
200 50
steps: 9950, episodes: 200, mean episode reward: -165.33463850386332, time: 34.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.79899497  -0.52763819 -14.02261307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.94974874  -0.42713568 -11.73055276]
400 50
steps: 19950, episodes: 400, mean episode reward: -165.25745466819353, time: 34.67
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.24   -0.565 -13.711]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.725   -0.49   -11.5035]
600 50
steps: 29950, episodes: 600, mean episode reward: -164.79447249879416, time: 35.17
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.925   -0.515  -14.1547]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.425   -0.41   -11.3978]
800 50
steps: 39950, episodes: 800, mean episode reward: -160.90912682506453, time: 34.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.4     -0.51   -13.8564]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.075   -0.43   -11.7445]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -174.26205515943627, time: 34.092
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.665   -0.52   -13.8163]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.835   -0.445  -11.6514]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1238.003043172855, time: 48.76
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.655   -0.53    -7.8373]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.15    -0.385  -10.6711]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -274.781391824159, time: 50.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.84    -0.575  -11.9617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.7     -0.545  -12.2843]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -205.82243713121687, time: 50.514
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.85   -0.28   -9.248]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.19    -0.585  -12.4493]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -217.06403861219297, time: 50.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.04  -0.025 -0.559]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.425   -0.335   -5.6805]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -161.91186618266462, time: 50.559
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.76   -0.065  -0.4877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.855   -0.315   -6.7027]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -149.8316827010869, time: 49.944
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.265  -0.035  -0.2291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.3     -0.305   -7.7567]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -142.59859562405384, time: 49.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.475  -0.045  -0.3034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.205   -0.255   -7.8082]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -125.17414320885636, time: 49.479
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.63   -0.04   -0.3955]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.825   -0.145   -6.0091]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -127.62200742150148, time: 50.08
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.395  -0.05   -0.2493]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.98    -0.22    -6.9355]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -128.68095380471328, time: 49.206
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.575  -0.09   -0.8428]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.345   -0.255   -8.1242]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -117.13094322606106, time: 50.431
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.505  -0.11   -0.7725]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.985  -0.075  -5.747]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -122.57736098489802, time: 50.428
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.15   -0.195  -2.5784]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.025   -0.09    -6.6474]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -123.76093053886831, time: 49.601
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.125 -0.31  -4.783]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.365  -0.055  -5.892]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -130.66973475816997, time: 49.977
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.035  -0.255  -4.5759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.23   -0.035  -2.1138]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -110.59818288283395, time: 50.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.475   -0.265   -7.7594]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.565  -0.035  -3.0697]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -99.00409045025688, time: 49.963
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.345   -0.345   -9.2401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.18   -0.025  -1.8937]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -110.08791042942346, time: 49.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.56    -0.275  -10.3128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.805  -0.01   -1.8689]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -108.70799071504229, time: 49.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.235   -0.365  -12.0484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.505  -0.015  -2.7314]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -104.64336486032933, time: 48.549
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.055   -0.285  -13.3686]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.415   0.     -1.5495]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -107.01984675943302, time: 49.243
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.51    -0.24   -13.7774]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.475   0.     -1.0015]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -109.16644154498705, time: 50.325
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.535  -0.18  -11.792]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.525 -0.02  -2.859]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -100.92078339486568, time: 49.557
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.77    -0.515  -15.0335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.85   -0.035  -3.3484]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -108.94959845937396, time: 50.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.975   -0.28   -14.0991]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.005   -0.115   -5.1939]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -110.43940892405182, time: 49.783
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.65    -0.32   -13.4699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.785   -0.15    -5.4007]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -108.41105768601211, time: 48.906
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.635   -0.27   -13.2367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.535  -0.11   -3.7519]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -94.02270199292016, time: 49.975
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.645   -0.49   -16.6979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.615 -0.08  -3.552]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-3__2018-07-11_20-44-21...
200 50
steps: 9950, episodes: 200, mean episode reward: -166.96306601049363, time: 34.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.25125628  -0.45728643 -11.49417085]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.06030151  -0.49748744 -12.7798995 ]
400 50
steps: 19950, episodes: 400, mean episode reward: -172.51945021470027, time: 34.971
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.605   -0.41   -10.9558]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.67   -0.525 -12.683]
600 50
steps: 29950, episodes: 600, mean episode reward: -182.0802703188694, time: 35.0
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.795   -0.475  -11.2809]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.84    -0.525  -12.6322]
800 50
steps: 39950, episodes: 800, mean episode reward: -169.55449468598164, time: 34.813
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.705   -0.48   -11.2124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.79   -0.46  -12.682]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -170.80961171475406, time: 35.693
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.94    -0.46   -11.3316]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.87    -0.475  -12.5946]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -641.2526935806035, time: 49.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.825   -0.24    -7.0104]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.6     -0.435  -10.1237]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -186.67916154231688, time: 50.59
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.23    -0.27    -9.0663]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.435   -0.36    -8.4784]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -189.29586834788176, time: 50.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.47    -0.265   -7.6697]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.925  -0.22   -4.0705]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -151.38062889131766, time: 51.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.46    -0.305  -11.5408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.72  -0.32  -4.858]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -177.39995406954537, time: 50.425
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.52    -0.2     -9.1641]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.575  -0.265  -4.8685]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -139.35943615873788, time: 49.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.605   -0.3    -10.8027]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.08    -0.39    -8.0598]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -125.96154365299775, time: 49.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.88    -0.225   -6.0852]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.66   -0.375  -4.1265]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -134.0401398951859, time: 49.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.695  -0.23   -5.584]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.95    -0.48    -7.4729]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -142.94087403734437, time: 50.116
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.675  -0.135  -3.0518]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.67    -0.57    -7.8597]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -174.98010730581294, time: 50.169
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.685  -0.175  -4.1818]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.64   -0.555  -5.6445]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -151.45204568521476, time: 50.399
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.975   -0.39   -13.6628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.935   -0.495  -10.3315]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -139.1650510861542, time: 50.369
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.175   -0.315  -14.6548]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.26    -0.475  -12.3905]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -111.54079014130234, time: 50.046
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.51    -0.235  -16.8616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.9     -0.585  -15.9433]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -98.17196638773686, time: 49.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.76    -0.24   -13.2924]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.335   -0.8    -23.2972]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -92.9657508704895, time: 49.346
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.67    -0.31   -15.0114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.395   -0.8    -22.1169]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -104.37949512436779, time: 49.62
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.95    -0.22   -12.9129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.605   -0.87   -23.0536]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -97.68456229101994, time: 48.888
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.25   -0.335 -11.463]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.255   -0.825  -22.1037]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -96.7482349956839, time: 49.711
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.61    -0.245  -11.6961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.64   -0.845 -22.8  ]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -95.99852437149974, time: 49.426
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.03    -0.155   -8.8669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.94    -0.91   -24.1303]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -103.6945377540643, time: 49.673
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.49    -0.21    -8.7254]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.355   -0.93   -24.5247]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -99.22045368020292, time: 49.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.18    -0.26   -10.4011]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.035   -0.935  -24.4086]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -89.06116462998916, time: 49.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.135   -0.165   -7.5785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.935   -0.925  -24.3129]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -91.81504603075052, time: 49.536
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.59    -0.29   -10.8197]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.63    -0.95   -24.6898]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -87.08804666217092, time: 48.832
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.505   -0.26   -10.6642]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.115   -0.86   -23.0176]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -87.27940825662958, time: 49.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.145   -0.24    -9.8758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.555   -0.96   -24.5552]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -84.27746986623201, time: 50.966
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.47    -0.315  -11.1387]
agent1_energy_min, agent1_energy_max, agent1_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-3__2018-07-11_20-44-23...
200 50
steps: 9950, episodes: 200, mean episode reward: -196.33756980741353, time: 34.405
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.6281407   -0.58291457 -13.96753769]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.04020101  -0.61809045 -15.11125628]
400 50
steps: 19950, episodes: 400, mean episode reward: -186.18862543979597, time: 34.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.7     -0.545  -13.9595]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.83   -0.575 -15.451]
600 50
steps: 29950, episodes: 600, mean episode reward: -186.0289287996698, time: 34.716
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.555   -0.555  -13.9356]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.245   -0.525  -15.1913]
800 50
steps: 39950, episodes: 800, mean episode reward: -194.82938147226457, time: 34.502
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.78    -0.48   -13.9648]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.09    -0.54   -15.0388]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -192.2743641902844, time: 34.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.035   -0.515  -14.2331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.83   -0.595 -15.416]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -481.8655554347323, time: 48.67
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.65    -0.29    -6.6095]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.59    -0.575  -12.6255]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -333.28120801089165, time: 49.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.73    -0.485  -12.9379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.48    -0.165   -6.3481]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -173.4958680675475, time: 49.461
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.22    -0.34    -9.0573]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.585   -0.42   -12.1801]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -122.48684670259281, time: 50.596
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.66    -0.265   -5.6907]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.545  -0.125  -4.4707]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -125.56162204237324, time: 49.876
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.255  -0.14   -3.5336]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.345  -0.02   -0.1933]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -127.5269460692746, time: 49.872
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.825  -0.045  -1.7952]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.155  -0.035  -0.1177]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -121.82985875872963, time: 49.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.33   -0.01   -0.1865]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.305  -0.03   -0.1757]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -127.48116703826796, time: 49.692
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.655  -0.01   -0.3743]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.19   -0.015  -0.1247]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -136.59426508448584, time: 49.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.17    0.     -0.0621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.525  -0.03   -0.2296]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -131.35242624691816, time: 49.626
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.765  -0.005  -0.3343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.15  -0.045 -0.107]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -113.36855454590346, time: 50.264
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.35   -0.01   -0.1166]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.55   -0.015  -0.2494]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -125.6027296266458, time: 50.609
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.485  -0.005  -0.2108]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.595  -0.01   -0.2345]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -120.67354793264745, time: 49.902
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.29   -0.005  -0.1071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.01   -0.025  -0.3924]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -116.57491970790286, time: 49.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.115   0.     -0.0432]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.845  -0.07   -0.7972]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -110.63334449067088, time: 50.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.77   -0.01   -0.3413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.32   -0.045  -0.8167]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -123.13605018737648, time: 50.029
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.605  -0.02   -0.6405]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.635  -0.065  -0.5732]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -132.9147663613752, time: 49.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.95   -0.025  -1.5259]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.54   -0.06   -1.1442]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -118.08341439543312, time: 49.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.26   -0.035  -0.9143]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.995  -0.105  -1.9905]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -104.85363269685666, time: 49.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.565   -0.31   -14.5458]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.585  -0.13   -3.0413]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -105.50150734359336, time: 49.842
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.795   -0.195  -10.2143]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.53  -0.15  -2.236]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -109.98711845048263, time: 50.358
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.695   -0.35   -16.6637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.685   -0.17    -4.7945]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -109.57411835289412, time: 49.442
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.7     -0.485  -18.6448]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.81    -0.15    -5.7982]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -105.20638264046971, time: 49.978
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.28    -0.395  -18.3143]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.335   -0.315  -11.6685]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -101.57607530460861, time: 49.125
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.9     -0.41   -16.4531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.715   -0.245  -13.4775]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -96.39828310290211, time: 52.168
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.675   -0.53   -19.3583]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.845  -0.3   -15.971]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -91.19435245793551, time: 51.083
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.285   -0.53   -19.5132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.93    -0.38   -14.9279]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -90.93875365453977, time: 48.676Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_2agents-3__2018-07-11_20-44-25...
200 50
steps: 9950, episodes: 200, mean episode reward: -195.97225507386025, time: 34.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.75376884  -0.46733668 -11.98462312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.34170854  -0.47738693 -11.92110553]
400 50
steps: 19950, episodes: 400, mean episode reward: -184.19610033594495, time: 34.731
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.37    -0.535  -12.5307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.035  -0.46  -11.75 ]
600 50
steps: 29950, episodes: 600, mean episode reward: -193.87356047469987, time: 34.675
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.005   -0.505  -12.2826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.195   -0.56   -11.8757]
800 50
steps: 39950, episodes: 800, mean episode reward: -177.66506533017574, time: 34.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.495   -0.565  -12.5445]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.895  -0.435 -11.662]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -189.4235498920621, time: 35.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.54    -0.535  -12.6143]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.55    -0.475  -11.5187]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -537.5864872856563, time: 49.329
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.085   -0.595  -15.8662]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.41    -0.265   -7.5073]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -209.20758398282328, time: 50.729
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.675  -0.52  -11.497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.535   -0.62   -12.2303]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -136.6185008670614, time: 50.458
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.33    -0.435  -12.2942]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.945  -0.09   -1.6875]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -138.66708049120518, time: 50.551
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.815  -0.07   -1.4906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.405  -0.19   -1.0536]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -114.11418972358044, time: 50.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.65   -0.06   -0.4513]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.325  -0.18   -0.9075]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -125.1086874322851, time: 49.824
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.925  -0.05   -0.4896]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.23   -0.11   -0.8195]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -131.95836017054785, time: 49.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.335  -0.08   -0.7533]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.82   -0.06   -0.4473]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -142.4917912433226, time: 49.671
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.855  -0.065  -0.4823]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.315  -0.015  -0.2098]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -127.69024311979618, time: 50.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.595  -0.15   -3.3069]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.645  -0.005  -0.2861]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -122.0657876768067, time: 49.335
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.13    -0.475  -15.3303]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.705  -0.015  -0.2759]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -108.71785707137842, time: 50.118
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.16    -0.42   -16.6296]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.955  -0.01   -0.4015]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -113.07005495748724, time: 50.269
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.255   -0.55   -18.5539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.145 -0.04  -0.958]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -113.2961417685976, time: 49.793
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.315  -0.535 -18.097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.115  -0.035  -0.5015]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -100.5862174717457, time: 49.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.645   -0.535  -20.0209]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.135 -0.04  -1.422]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -103.1513229613293, time: 51.115
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.74    -0.61   -20.6447]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.565 -0.05  -1.114]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -84.37634872203627, time: 49.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.6     -0.83   -22.1283]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.96   -0.065  -2.1433]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -84.44522751174729, time: 49.162
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.065  -1.    -24.347]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.21   -0.175  -3.2389]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -81.9424997234793, time: 49.103
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.715   -0.985  -24.0231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.775  -0.275  -6.355]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -79.0496145676469, time: 49.228
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.44    -1.     -24.0099]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.6    -0.145  -4.5915]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -83.95598666919075, time: 49.392
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.715   -0.975  -24.1357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.87   -0.145  -3.8906]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -81.37069654162752, time: 50.445
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.56    -0.99   -24.7121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.145  -0.11   -3.4928]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -83.23832055478168, time: 49.666
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.99    -0.98   -24.3557]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.065   -0.05    -4.3409]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -76.44314024014402, time: 49.883
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.81    -0.995  -24.1435]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.445  -0.145  -6.548]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -89.87643982748473, time: 50.181
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.36    -1.     -24.6342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.7    -0.07   -3.5232]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -77.4053106580223, time: 50.151
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.035  -1.    -24.979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.785   -0.13    -7.1363]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -88.1927697859026, time: 51.248
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.225   -0.995  -24.9935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.73    -0.615  -22.0394]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -77.4717638771647, time: 49.402
[-48.21    -1.     -24.1269]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -56.93681604978681, time: 49.394
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.45    -0.995  -25.1055]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.86    -0.99   -23.9824]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -53.46157741246362, time: 48.153
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.28    -0.965  -24.9916]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.92    -0.955  -24.0908]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -54.42048330029365, time: 49.695
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.36    -0.99   -25.0696]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.895   -0.995  -23.9022]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -55.470503825527594, time: 48.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.475   -0.995  -25.1678]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.375   -0.96   -24.4163]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -52.46697722763478, time: 49.216
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.125   -0.995  -24.9532]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.945   -0.985  -24.8236]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -53.22960024077998, time: 48.494
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.295   -0.995  -24.9939]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.9     -0.985  -24.6685]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -45.68391238018927, time: 48.969
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.705   -1.     -25.2944]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.235   -0.975  -24.9616]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -50.089072496753, time: 49.26
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.39    -0.985  -25.0717]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.44    -0.985  -24.3772]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -52.78071590137515, time: 48.392
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.695   -1.     -25.2883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.57    -0.99   -23.7278]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -47.39392842591928, time: 48.913
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.3     -0.99   -25.0045]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.44    -1.     -25.0983]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -47.57160260667569, time: 48.359
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.86    -0.99   -25.4008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.28   -0.995 -25.008]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -55.1077805436141, time: 48.193
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.54    -0.995  -25.1833]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.29    -0.995  -25.0397]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -45.53297178690647, time: 48.415
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.465   -1.     -25.1438]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.525   -0.985  -25.1788]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -44.86823909859948, time: 47.68
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.585   -1.     -25.2059]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.62    -0.995  -25.2465]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -51.63648783412321, time: 48.294
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.45    -0.995  -24.4762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.19    -1.     -25.0189]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -57.70919682214585, time: 48.586
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.6    -1.    -24.565]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.69    -1.     -24.7258]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -50.52691417533095, time: 48.616
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.625   -1.     -25.2543]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.435   -1.     -25.1771]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -46.64655683022414, time: 48.532
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.515   -0.995  -25.1538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.455   -1.     -25.1127]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -45.63528768000642, time: 48.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.305   -1.     -25.0927]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.41    -1.     -25.0908]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -40.74000229795622, time: 49.029
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.66    -1.     -25.2424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.76    -1.     -25.3271]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -49.56319125589747, time: 47.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.885   -1.     -25.4342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.685   -1.     -25.2809]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -46.78507752926194, time: 48.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.675   -1.     -25.2761]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.855   -1.     -25.4032]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -43.84459557088718, time: 47.978
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.715  -0.995 -25.293]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.705   -1.     -25.3206]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -43.15495902153612, time: 48.721
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.75    -1.     -25.3673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595   -1.     -25.2546]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -40.842845790562315, time: 48.932
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.91    -1.     -25.4496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.575   -1.     -25.2678]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -42.43785228127106, time: 48.068
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.85    -1.     -24.8287]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.675   -1.     -25.2871]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -43.92611641739366, time: 47.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.665   -1.     -24.7292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.925   -1.     -25.4695]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -42.39976217423009, time: 48.018
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.73    -1.     -25.3698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.725   -1.     -25.3399]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -39.37279965962069, time: 48.986
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.675   -1.     -24.3623]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.87    -1.     -25.4284]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -46.72022954621838, time: 48.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.415   -1.     -23.5156]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.6     -0.99   -25.2768]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -41.15381827202846, time: 47.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.05    -1.     -24.5899]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.935   -1.     -25.4649]
[-39.325   -0.51   -19.0035]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -82.4042924840382, time: 47.669
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.435   -0.99   -24.1582]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.4    -0.59  -19.584]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -93.31192288807638, time: 47.11
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.83    -1.     -24.8704]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.375   -0.595  -20.5901]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -79.6244772444066, time: 47.25
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.885   -0.995  -24.8584]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.4     -0.675  -22.4838]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -84.96445529865574, time: 46.91
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.68    -1.     -23.8534]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.64    -0.705  -22.1326]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -78.32575521693889, time: 47.547
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.07    -1.     -24.9757]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.04    -0.75   -22.2567]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -79.61746705453221, time: 46.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.455   -0.995  -25.1441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.73    -0.7    -22.5669]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -77.93123269650405, time: 47.128
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.66    -0.975  -25.2768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.6     -0.76   -22.0944]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -69.33687272716155, time: 47.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.43    -0.975  -25.1235]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.81    -0.73   -22.7192]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -68.97954574526754, time: 46.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.49    -1.     -25.2039]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.095   -0.785  -22.3669]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -71.32378068548272, time: 46.9
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.735   -0.97   -24.1057]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.68    -0.68   -22.8319]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -76.86610990456748, time: 47.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.485   -0.975  -24.5526]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.005   -0.68   -22.3507]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -76.71080716394715, time: 46.629
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.615   -0.995  -24.7414]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.64    -0.69   -22.6453]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -78.12167551802683, time: 47.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.565   -0.98   -24.7448]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.47    -0.825  -22.6324]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -69.8061544630678, time: 46.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.56    -0.98   -24.6059]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.82   -0.905 -23.491]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -72.37208933796987, time: 46.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.255   -0.965  -24.3907]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.76    -0.82   -23.2289]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -73.31788207275541, time: 46.95
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.51    -0.95   -24.5993]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.755   -0.84   -23.2609]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -70.43515984426742, time: 47.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.775   -0.985  -24.1735]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.12    -0.82   -22.9442]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -66.20471819833547, time: 46.988
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.81    -0.955  -24.7586]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.355   -0.77   -22.8646]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -64.74341789777893, time: 46.961
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.75    -0.945  -24.6769]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.945   -0.795  -23.3293]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -68.60184311805766, time: 48.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.485   -0.965  -24.5343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.61    -0.84   -23.0314]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -75.39271379425462, time: 49.901
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.39    -0.96   -24.6327]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.255   -0.845  -23.5097]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -65.74686586274623, time: 46.496
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.335   -0.955  -24.4941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.57    -0.86   -22.9784]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -62.012025414194085, time: 46.739
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.005   -0.94   -24.3609]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.065   -0.825  -22.6685]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -73.31502821888516, time: 47.349
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.58    -0.87   -23.9713]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.75    -0.8    -21.8443]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -64.55461738400324, time: 47.473
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.95   -0.93  -24.843]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.385   -0.825  -22.8203]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -59.97585492934543, time: 46.917
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.76    -0.955  -24.7879]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.35   -0.875 -23.478]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -60.7510386326443, time: 46.868
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.755  -0.935 -24.605]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.33    -0.92   -23.5102]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -63.64032494596345, time: 50.696
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.21    -0.955  -24.9119]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.77    -0.795  -22.9593]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -62.613142690635904, time: 46.902
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.495   -0.945  -24.5069]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.145   -0.75   -23.3413]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -68.26866252233226, time: 47.779
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.345   -0.955  -25.0778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.375   -0.82   -22.7214]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -63.075676348704135, time: 46.889
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.755  -0.95  -24.634]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.87    -0.885  -22.3482]
12600 50

[-48.72    -0.985  -24.6733]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -85.35318566940651, time: 48.533
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.65    -0.69   -15.0386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.76    -0.995  -24.7537]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -75.54540963650965, time: 48.427
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.505   -0.555  -12.3922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.6     -1.     -24.6112]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -70.99471772269685, time: 47.974
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.715   -0.525  -12.3216]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.125   -0.975  -24.9364]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -76.60524123790175, time: 48.205
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.305   -0.69   -16.6711]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.8     -0.985  -23.8599]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -75.46054840441634, time: 48.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.86    -0.585  -16.6799]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.38    -0.975  -23.2469]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -85.45088622430355, time: 48.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.675  -0.475 -12.744]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.445   -0.995  -23.2408]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -83.90471647142108, time: 48.716
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.98    -0.445  -12.2626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.155   -0.98   -23.5339]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -70.37292581971212, time: 48.367
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.99    -0.51   -13.7565]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.805   -0.995  -24.7415]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -81.53256336330442, time: 49.478
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.865   -0.315  -11.9876]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.95    -0.99   -24.3196]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -87.18120109548381, time: 48.654
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.43    -0.335  -14.0956]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.585   -1.     -24.7942]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -81.93711137466953, time: 48.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.41    -0.29   -11.2934]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.32    -1.     -24.5831]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -93.27511756121424, time: 47.833
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.655   -0.355  -13.6095]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.92    -0.995  -24.4323]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -76.13503385402032, time: 48.389
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.535   -0.515  -15.6406]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.595  -0.995 -24.805]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -73.58128754195761, time: 48.003
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.495   -0.465  -15.0844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.765   -0.995  -24.7273]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -80.18057475489074, time: 48.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.785   -0.33   -12.2084]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.215   -1.     -24.4824]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -80.72021100726352, time: 48.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.25    -0.415  -13.6655]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.435   -0.97   -24.0134]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -78.3827976478106, time: 48.233
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.13    -0.33   -12.2884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.475   -0.99   -24.1317]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -80.88553548835407, time: 47.923
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.535   -0.31   -11.8282]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.95    -0.985  -24.3061]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -70.64033021831828, time: 48.068
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.005   -0.58   -17.4773]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.905   -0.99   -24.7777]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -70.45618843855348, time: 48.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.54    -0.48   -15.0377]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.295   -0.975  -24.0202]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -78.7215562197763, time: 47.264
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.835   -0.495  -15.4032]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.43    -0.99   -23.0077]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -69.07721904974984, time: 48.006
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.6     -0.395  -14.3291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.035   -0.995  -24.9665]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -68.31717535562201, time: 48.094
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.87    -0.315  -11.3071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.89    -1.     -24.8293]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -64.9433433350639, time: 48.293
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.08    -0.45   -14.4316]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.18   -0.995 -24.935]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -82.44381499611426, time: 48.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.87    -0.62   -15.4324]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.035   -1.     -24.4872]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -75.42406984321505, time: 47.883
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.74    -0.555  -16.1663]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.31    -1.     -24.5154]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -74.66153470330806, time: 47.882
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.295   -0.505  -11.4101]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.235   -0.99   -23.8266]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -63.21360307007041, time: 46.791
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.33   -0.575 -16.032]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.95   -0.99  -24.795]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -71.6741217288293, time: 48.552
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.05    -0.595  -15.6905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.95    -1.     -24.8642]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -69.01372308850746, time: 48.362
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.67    -0.4    -13.6904]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.215   -0.995  -24.9947]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -74.69968933911412, time: 47.678
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.93    -0.645  -17.1553]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.48    -0.99   -24.6288]
12600
6400 50
steps: 319950, episodes: 6400, mean episode reward: -94.32575974389245, time: 48.863
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.495   -0.275   -4.5676]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.49    -0.715  -22.1677]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -84.5609652584343, time: 47.809
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.985  -0.2    -3.5535]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.185   -0.59   -23.0819]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -85.41792067882841, time: 48.118
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.705  -0.17   -2.5489]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.8     -0.725  -22.4945]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -88.44704759807091, time: 47.667
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.98   -0.215  -3.6392]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.14    -0.62   -22.2934]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -74.76869813342137, time: 48.605
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.9    -0.255  -4.4862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.17    -0.765  -23.5222]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -76.93140372133003, time: 48.289
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.08  -0.25  -3.765]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.35    -0.73   -23.5976]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -93.82762593308604, time: 48.178
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.205  -0.19   -2.8454]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.7     -0.72   -22.6953]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -76.06219177037589, time: 48.638
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.915 -0.22  -4.239]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.965   -0.78   -23.3771]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -73.22606632770652, time: 48.768
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.495  -0.275  -4.4737]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.05    -0.77   -23.3919]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -73.88257064699096, time: 48.574
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.52   -0.275  -5.477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.49    -0.76   -23.6973]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -73.34742220237277, time: 47.884
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.925  -0.245  -4.0984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.915   -0.895  -24.1604]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -67.59452435249246, time: 47.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.915   -0.305   -5.5219]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.485   -0.83   -24.4362]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -80.08938189177105, time: 47.693
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.295  -0.315  -5.0635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.375   -0.835  -23.7654]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -73.20319650032584, time: 47.01
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.315   -0.35    -6.2571]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.42    -0.795  -23.7286]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -67.01199307612123, time: 48.512
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.69    -0.305   -5.9509]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.575   -0.805  -24.3995]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -68.10227581246058, time: 48.174
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.575   -0.32    -6.5712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.23    -0.94   -24.3463]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -66.75913035195538, time: 48.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.37    -0.28    -5.1226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.705   -0.975  -24.6009]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -69.81870488462914, time: 48.023
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.305   -0.35    -5.1374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.51    -0.905  -24.4208]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -68.26098456554662, time: 48.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.34    -0.325   -5.1427]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.105   -0.965  -24.8135]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -73.576913044346, time: 48.797
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.2     -0.355   -5.8694]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.755   -0.975  -24.6482]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -68.4424950525529, time: 47.101
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.36    -0.265   -5.2164]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.33    -0.985  -24.4443]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -69.15754308851409, time: 47.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.645   -0.35    -6.5719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.765   -0.955  -24.6374]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -65.70724549702703, time: 47.853
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.845  -0.295  -3.7697]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.615   -0.975  -24.5414]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -60.7816148966019, time: 47.672
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.955  -0.3    -4.3827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.895   -0.99   -24.6962]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -61.076774594926476, time: 48.416
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.39   -0.27   -4.1615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.605  -0.985 -24.517]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -68.21942882626715, time: 47.699
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.135  -0.27   -3.5858]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.42    -0.98   -24.3482]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -68.38391330899918, time: 47.391
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.23   -0.275  -4.4366]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.975   -0.975  -24.1537]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -58.31982007210574, time: 50.947
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.875   -0.315   -5.5451]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.805   -0.94   -24.5989]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -69.35408956737383, time: 48.982
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.065  -0.29   -3.7684]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.365   -0.98   -24.3477]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -61.61683857215323, time: 48.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.06   -0.305  -4.9718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.655   -0.97   -24.5222]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -73.5215219670231, time: 47.809
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.115 -0.285 -4.896]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.185  -0.97  -24.202]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -61.73574166302945, time: 48.554
6400 50
steps: 319950, episodes: 6400, mean episode reward: -91.83708488652434, time: 48.052
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.17   -0.48  -15.937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.765  -0.07   -2.7534]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -100.36625784858566, time: 48.521
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.65    -0.47   -16.4381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.335  -0.045  -1.1174]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -109.38553823988008, time: 48.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.015   -0.425  -16.2089]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.01   -0.045  -1.4452]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -91.55574877902797, time: 50.11
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.53   -0.355 -16.03 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.43   -0.025  -0.7818]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -96.96711176832636, time: 48.255
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.535   -0.465  -17.7929]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.695  -0.02   -0.3133]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -104.9553334087563, time: 48.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.89    -0.53   -17.8911]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.13   -0.045  -0.6273]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -90.40314930254303, time: 48.277
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.415   -0.53   -17.3313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.725 -0.02  -0.39 ]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -89.52299947569732, time: 47.52
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.425   -0.6    -18.7403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.385  -0.015  -0.2214]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -90.25816592828221, time: 46.401
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.52    -0.61   -18.0021]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.94   -0.02   -0.4998]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -96.71297934285946, time: 47.445
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.4     -0.645  -17.5256]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.335  -0.025  -0.2286]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -97.42045414070726, time: 46.855
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.435   -0.685  -18.7037]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.585  -0.02   -0.3382]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -93.44839515126874, time: 46.978
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.845   -0.58   -19.3776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.485  -0.005  -0.2533]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -91.08854959277014, time: 47.002
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.655   -0.575  -18.6138]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.61  -0.01  -0.336]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -88.99223745438056, time: 46.777
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.1     -0.77   -19.8412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.58   -0.025  -0.3628]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -85.9650598424614, time: 47.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.87    -0.65   -19.6082]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.08   -0.005  -0.5716]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -99.02645122296366, time: 47.932
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.385   -0.57   -19.0304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.83   -0.025  -0.5009]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -96.73772715360536, time: 47.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.19    -0.625  -19.0783]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.535  -0.02   -0.3166]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -97.33524726661952, time: 47.056
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.655   -0.715  -20.1074]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.43   -0.025  -0.2647]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -85.18429924128843, time: 47.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.91    -0.76   -19.7504]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.22   -0.015  -0.1515]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -92.5920741790039, time: 49.842
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.96   -0.725 -20.237]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.635  -0.005  -0.3985]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -91.83744975551079, time: 46.763
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.21    -0.795  -20.3541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.345  -0.01   -0.8974]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -80.12405367865715, time: 46.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.835   -0.805  -20.2491]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.58   -0.005  -0.3491]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -74.57466124487522, time: 47.089
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.995   -0.775  -20.8128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.53  -0.105 -1.735]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -94.3977184101572, time: 47.496
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.83    -0.82   -20.5294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.19   -0.07   -1.3788]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -79.86555522550849, time: 48.367
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.675   -0.74   -17.8616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.085  -0.065  -0.7245]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -77.13028548277516, time: 48.178
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.24    -0.84   -18.7641]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.64   -0.055  -1.0611]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -86.68475941681274, time: 46.479
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.95    -0.855  -19.3199]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.835  -0.035  -0.5605]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -80.38171817110982, time: 47.512
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.88    -0.855  -19.6984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.845  -0.035  -0.9745]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -68.26855547127803, time: 47.642
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.115   -0.83   -20.1124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.35   -0.02   -0.7256]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -93.70250283697524, time: 47.737
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.5     -0.885  -20.3839]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.07   -0.055  -1.5456]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -74.8229554241277, time: 47.829
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.375   -0.93   -20.1254]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.69   -0.035  -0.9508]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -80.84412130057346, time: 47.901
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.92   -0.905 -24.124]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -91.41623219454438, time: 48.81
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.95    -0.155   -7.3391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.29    -0.885  -23.7552]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -83.68020920642014, time: 49.415
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.02    -0.18    -8.3407]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.385   -0.975  -24.5388]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -87.18431915327186, time: 49.579
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.095  -0.185  -7.342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.42    -0.985  -24.6857]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -79.27717517213894, time: 50.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.645   -0.23    -9.0903]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.775   -0.945  -24.7231]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -92.9567320951798, time: 50.473
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.65    -0.21    -8.6228]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.4     -0.875  -24.3639]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -84.8422798531612, time: 49.843
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.295   -0.215   -9.1509]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.34    -0.9    -24.3978]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -78.39040686524696, time: 49.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.56    -0.19    -8.4287]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.685   -0.91   -24.6149]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -79.69259728807725, time: 48.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.27    -0.21    -8.5428]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.22    -0.93   -24.2887]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -83.15993877761696, time: 49.156
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.14    -0.125   -6.6634]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.44    -0.895  -23.8335]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -83.25134994042666, time: 49.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.22    -0.25    -7.6966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.96    -0.895  -23.5935]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -72.87752967755156, time: 48.78
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.77   -0.185  -7.549]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.625  -0.97  -22.967]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -80.1945180444835, time: 48.588
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.44    -0.2     -6.5542]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.435   -0.95   -22.6911]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -86.27224054455975, time: 47.962
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.96    -0.215   -6.0816]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.185   -0.91   -21.7324]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -80.7671092748951, time: 48.016
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.98    -0.245   -5.4943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.705   -0.995  -22.7066]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -76.28559732987982, time: 48.686
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.1     -0.235   -7.7171]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.115   -0.96   -19.7261]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -78.23454793777553, time: 48.19
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.695   -0.28    -7.5496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.235   -0.985  -21.4194]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -74.12345770468676, time: 48.61
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.89   -0.18   -3.8204]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.955   -0.975  -21.8451]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -77.77540530381586, time: 47.349
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.78   -0.24   -5.458]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.33   -0.995 -20.371]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -71.01520017921075, time: 47.593
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.74   -0.17   -3.3759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.425   -0.995  -20.7684]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -74.374004147377, time: 47.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.03    -0.17    -4.9656]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.93    -0.995  -20.1847]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -67.7710857195486, time: 47.675
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.13   -0.135  -3.4978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.3     -0.985  -20.3142]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -70.16337431957021, time: 47.28
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.27    -0.14    -5.3525]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.33    -0.99   -19.5091]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -67.6235162666784, time: 47.933
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.365  -0.17   -4.8062]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.47    -0.99   -19.7257]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -66.0323119471821, time: 47.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.39  -0.2   -3.938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.595   -1.     -18.3337]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -66.45561691780658, time: 47.468
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.03   -0.19   -4.7609]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.74    -1.     -19.2455]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -63.52465806775467, time: 47.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.86   -0.135  -1.9378]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.485  -0.985 -18.735]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -64.99644137679068, time: 50.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.48   -0.265  -4.6962]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.56    -0.995  -18.7526]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -62.48803946381691, time: 46.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.72    -0.415   -6.0892]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.145   -1.     -17.7352]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -69.04428459847256, time: 46.331
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.235  -0.345  -5.3849]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.67    -1.     -19.1982]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -57.52474545158216, time: 47.934
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.335  -0.255  -4.5413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.77    -0.995  -16.1653]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -53.93288952961056, time: 47.871
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.03   -0.23   -3.9402]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.445   -1.     -15.6467]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -62.641581329145744, time: 47.735
6400 50
steps: 319950, episodes: 6400, mean episode reward: -89.8131966861511, time: 48.819
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.74    -0.785  -16.1826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.4     -0.695  -16.6638]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -73.66375790566104, time: 48.646
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.075  -0.715 -15.06 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.35    -0.84   -21.3067]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -64.34343792299642, time: 49.003
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.355   -0.695  -16.4367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.51    -0.83   -18.2409]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -70.53769726597207, time: 48.696
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.515   -0.88   -21.4267]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.055   -0.83   -19.2189]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -71.90340120387768, time: 49.89
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.22    -0.8    -16.8836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.605   -0.87   -20.2367]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -66.06105805233743, time: 48.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.055   -0.84   -17.3172]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.63    -0.915  -18.1799]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -68.1106731361385, time: 49.536
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.12    -0.87   -22.3973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.75    -0.83   -17.5894]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -69.35875397048784, time: 49.327
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.975   -0.87   -16.9208]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.3     -0.895  -16.8303]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -63.82226966129747, time: 48.778
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.31    -0.73   -16.0697]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.09    -0.83   -16.9474]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -65.31790818549266, time: 49.244
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.495   -0.805  -14.0083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.69    -0.865  -17.3725]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -68.58911184754754, time: 48.539
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.805   -0.755  -12.0894]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.97    -0.82   -16.7554]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -65.64789831610055, time: 48.989
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.32    -0.855  -14.0557]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.55    -0.83   -15.8998]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -59.85401238350953, time: 48.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.72    -0.795  -12.4277]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.255  -0.83  -15.877]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -64.00022469176021, time: 48.908
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.6     -0.835  -13.6179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.345   -0.875  -17.0874]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -66.93739199437736, time: 49.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.62    -0.92   -16.5102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.505   -0.865  -14.2313]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -55.8610926702948, time: 49.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.555   -0.905  -14.6775]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.185   -0.915  -15.5174]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -57.113650938663305, time: 49.152
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.64    -0.825  -13.3371]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.975   -0.995  -16.7676]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -63.35876233129469, time: 49.174
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.36    -0.85   -15.5764]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.245   -0.835  -16.3001]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -62.484589384316315, time: 49.747
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.45    -0.895  -15.1425]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.46    -0.835  -15.9925]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -68.36072395312539, time: 48.401
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.78    -0.87   -15.9128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.545   -0.81   -16.2909]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -58.27558402919854, time: 48.296
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.04    -0.865  -15.4177]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.72    -0.865  -16.4403]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -54.73700257877359, time: 48.651
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.335   -0.905  -14.6243]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.175   -0.89   -15.7126]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -58.01003915156027, time: 49.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.31    -0.92   -15.5421]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.41    -0.95   -16.6638]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -59.9502815033539, time: 49.198
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.975   -0.93   -15.0633]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.58    -0.85   -16.9349]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -69.2162022899051, time: 48.755
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.45    -0.855  -13.2519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.92   -0.83  -17.421]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -65.00349724584686, time: 48.245
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.805  -0.885 -13.49 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.865   -0.865  -15.2467]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -56.81748916614906, time: 47.741
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.55    -0.93   -14.1972]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.345   -0.88   -17.0016]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -53.90087079515199, time: 48.964
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.31    -0.915  -13.5272]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.315   -0.895  -17.0694]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -58.99362513076821, time: 49.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.35    -0.91   -13.8051]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.905   -0.94   -16.3105]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -52.59036395905338, time: 49.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.205  -0.905 -13.732]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.65    -0.87   -13.8918]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -57.497883099934924, time: 49.804
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.095   -0.895  -14.1606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.775   -0.78   -16.1028]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -59.24083054277041, time: 48.638
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.15    -0.425  -22.4405]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.105   -0.555  -18.5592]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -90.37117159438031, time: 49.387
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.96   -0.46  -19.297]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.825   -0.34   -14.7733]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -87.04515286833156, time: 49.499
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.805   -0.535  -21.9654]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.28    -0.51   -18.5131]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -81.32760640149544, time: 49.697
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.53    -0.49   -21.5375]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.43    -0.65   -18.8698]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -84.21302359397643, time: 50.357
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.455  -0.635 -21.848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.79    -0.555  -19.5062]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -79.92476932254897, time: 49.746
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.36    -0.71   -23.3801]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.165   -0.655  -19.2943]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -81.58429532272444, time: 49.782
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.385   -0.65   -22.5877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.975   -0.655  -20.9281]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -77.80172389279865, time: 49.215
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.375   -0.775  -23.4165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.92    -0.685  -20.1807]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -77.62245442600947, time: 49.153
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.555   -0.72   -22.8798]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.845   -0.575  -19.1852]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -76.10439640842907, time: 48.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.525  -0.64  -22.642]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.855  -0.645 -21.716]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -74.89006781512822, time: 49.015
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.8     -0.575  -22.7866]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.275   -0.67   -22.1008]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -72.83839306530915, time: 49.099
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.76    -0.635  -23.5983]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.14    -0.64   -21.6408]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -80.1375308974557, time: 48.853
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.405   -0.665  -23.4845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.44    -0.63   -19.7204]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -77.14219116346266, time: 48.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.635   -0.77   -23.7405]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.49   -0.55  -20.635]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -78.33129525440812, time: 50.087
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.47    -0.77   -23.6536]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.97   -0.585 -19.333]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -78.25464938851533, time: 49.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.94    -0.755  -23.8399]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.34    -0.72   -21.9277]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -82.19932462568266, time: 49.811
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.935   -0.72   -23.0349]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.285  -0.625 -20.434]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -85.05561331927137, time: 49.566
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.565   -0.6    -22.0498]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.48    -0.66   -20.4133]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -82.45110580475219, time: 52.266
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.49    -0.765  -23.1224]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.045   -0.72   -20.7538]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -84.08241489892231, time: 49.195
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.905   -0.76   -23.4818]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.11    -0.73   -21.2236]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -86.13651590754007, time: 48.759
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.79    -0.795  -22.6492]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.855   -0.67   -20.1257]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -82.38703837042182, time: 48.625
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.855   -0.81   -22.8283]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.14   -0.635 -21.358]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -89.49447553111537, time: 48.579
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.48    -0.89   -23.4065]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.97   -0.73  -22.305]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -77.37691866198524, time: 49.327
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.25   -0.76  -23.013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.14    -0.705  -21.6432]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -84.80700208822373, time: 49.678
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.725   -0.85   -23.4799]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.      -0.705  -21.1234]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -79.13026124023008, time: 47.68
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.17    -0.89   -23.8214]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.98    -0.73   -21.3405]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -76.4266128650968, time: 48.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.815  -0.78  -23.39 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.705   -0.725  -21.1595]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -79.61160244868887, time: 48.327
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.38    -0.86   -23.7735]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.64    -0.735  -20.7199]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -76.27178108924356, time: 47.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.455   -0.815  -24.4893]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.875   -0.665  -20.3225]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -78.99481189390268, time: 49.476
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.26    -0.9    -24.2919]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.31    -0.59   -18.6301]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -77.27617507163832, time: 49.277
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.44    -0.87   -23.3182]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.24    -0.59   -19.4146]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -69.22270902465762, time: 46.928
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.95    -1.     -24.8351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.69    -0.385  -15.0089]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -77.71979439955149, time: 49.667
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.54    -1.     -25.2078]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.67    -0.48   -18.2583]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -68.5644874704065, time: 50.019
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.115   -0.97   -24.4145]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.14   -0.465 -17.122]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -71.99826712695483, time: 51.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.73    -1.     -24.7497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.015  -0.24  -11.527]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -75.18146691013295, time: 50.385
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.815   -1.     -24.3954]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.235   -0.43   -17.2704]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -73.49879680835315, time: 50.08
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.375   -1.     -25.1521]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.79    -0.675  -22.3821]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -74.83490184510987, time: 50.076
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.86    -0.985  -20.8114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.555   -0.515  -18.5236]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -73.54743322080255, time: 48.191
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.73    -0.995  -18.6722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.28    -0.57   -20.9404]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -71.70553041739068, time: 49.223
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.315   -0.985  -17.4361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.79    -0.645  -22.3287]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -74.77330932517926, time: 49.854
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.365   -0.98   -14.1261]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.15    -0.81   -24.8462]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -70.34627309314568, time: 49.207
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.18    -1.     -18.3794]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.52    -0.81   -25.0561]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -78.99379634560674, time: 48.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.83  -1.   -20.82]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.21    -0.735  -24.1805]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -76.80970418944706, time: 49.472
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.31    -0.995  -18.3701]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.925   -0.875  -24.7359]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -62.92922225943558, time: 49.239
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.62    -0.995  -17.4212]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.665   -0.835  -24.5704]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -66.17417265149449, time: 50.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.96    -0.98   -18.0861]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -0.87   -25.0926]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -81.41202830770798, time: 49.663
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.785   -0.99   -19.0219]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.885   -0.775  -24.6791]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -79.65576192555366, time: 49.967
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.6     -0.975  -18.5403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.09    -0.855  -24.8036]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -69.6395645231155, time: 50.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.165   -0.99   -17.6731]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.23    -0.85   -24.8762]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -70.34309630793571, time: 51.798
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.495   -0.995  -17.1419]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.225   -0.755  -24.8313]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -67.74243362825672, time: 49.637
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.91    -0.89   -16.4148]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.285   -0.735  -24.1232]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -69.9494665079699, time: 48.745
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.585   -0.735  -12.6261]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.475   -0.885  -25.1021]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -65.06205336542067, time: 49.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.84    -0.88   -12.3863]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.6     -0.905  -25.2009]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -71.8475215699288, time: 49.634
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.875   -0.92   -14.3613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.26    -0.8    -24.9035]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -66.04426391781466, time: 49.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.855  -0.875 -10.915]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.145   -0.8    -24.8123]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -63.3151810825691, time: 49.843
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.56    -0.91   -10.3977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.185   -0.845  -24.1988]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -64.63243648417051, time: 49.094
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.595   -0.9    -12.0233]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.035  -0.86  -24.815]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -67.40121659743102, time: 49.261
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.57    -0.98   -12.9793]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.865   -0.87   -24.7282]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -67.94711163843871, time: 49.467
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.73    -0.94   -12.8715]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.565   -0.785  -24.5052]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -65.24124785634056, time: 48.868
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.345   -0.785   -9.6511]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.22    -0.795  -24.9443]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -67.70384452015237, time: 50.417
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.895   -0.72   -10.1067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.895   -0.765  -24.2883]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -61.926681055176886, time: 49.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.68    -0.89   -12.4502]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.255  -0.86  -24.469]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -63.21423533811767, time: 52.229
agent0_energy_min, agent0_energy_max, agent0_energy_avgsteps: 629950, episodes: 12600, mean episode reward: -58.43221129726033, time: 47.574
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.84    -0.97   -24.7413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.785  -0.86  -21.494]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -63.654515460796446, time: 47.277
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.12    -0.98   -24.8935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.605   -0.87   -23.6296]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -69.5425028562426, time: 46.826
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.92    -0.94   -24.7882]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.545   -0.895  -23.0993]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -65.48721869874717, time: 47.561
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.36    -0.985  -25.1112]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.15    -0.94   -23.5767]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -52.691458454820406, time: 46.945
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.88   -0.955 -24.735]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.13    -0.88   -23.3801]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -64.55609960574661, time: 46.035
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.295  -0.94  -24.418]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.8     -0.94   -24.0208]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -72.77914504163321, time: 47.372
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.005   -0.915  -24.8559]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.56    -0.89   -23.9381]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -67.1497263005225, time: 47.166
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.02    -0.925  -24.7902]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.36    -0.86   -23.6185]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -60.10385352102067, time: 47.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.56    -0.99   -24.4487]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.825   -0.88   -23.8977]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -60.30374543661817, time: 47.301
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.755   -1.     -24.6285]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.935  -0.89  -24.049]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -56.456307585235464, time: 47.374
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.11    -1.     -24.8593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.8     -0.915  -23.1763]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -70.5756022565749, time: 47.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.615   -0.995  -24.6006]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.52    -0.92   -23.8077]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -61.098293994523445, time: 47.271
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.31    -0.995  -24.9897]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.19    -0.885  -24.2242]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -60.31547970441577, time: 48.067
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.645   -0.995  -25.2697]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.395  -0.895 -24.284]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -62.511297789861445, time: 48.044
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.6     -1.     -25.2333]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.12    -0.955  -24.1391]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -64.45284553380522, time: 48.095
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.235   -1.     -25.0262]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.495   -0.935  -24.4173]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -60.96154646111314, time: 46.984
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.435   -1.     -25.1304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.205   -0.885  -24.1479]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -67.79598117139736, time: 47.155
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.22    -0.945  -25.0102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.865   -0.915  -23.9032]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -66.26990549345095, time: 48.354
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.81    -0.9    -24.6207]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.295   -0.975  -24.2161]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -61.05813659405872, time: 47.884
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.385   -1.     -25.0337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.95    -0.975  -24.8046]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -63.40805652121857, time: 47.496
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.23    -0.995  -24.9563]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.855   -0.985  -24.6595]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -66.14266651048469, time: 47.527
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.075   -1.     -24.9161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.065  -0.97  -24.803]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -67.95130895489581, time: 46.549
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.835   -1.     -24.6678]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.51    -0.99   -24.4029]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -67.35892705682657, time: 47.722
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.1     -1.     -24.8808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.155   -0.955  -24.1099]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -63.340814997943724, time: 47.916
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.385   -1.     -25.0501]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.195   -0.95   -24.1661]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -74.21022607623559, time: 47.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.18    -1.     -24.9621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.845   -0.95   -23.9838]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -61.02021333574871, time: 47.476
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.85   -1.    -24.804]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.335   -0.98   -24.2036]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -67.0058391906875, time: 47.555
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.17    -1.     -24.9828]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.475   -0.97   -24.3983]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -70.07782883622964, time: 47.369
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.17    -1.     -24.9683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.31    -0.94   -24.2568]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -66.823127270036, time: 48.095
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.89    -1.     -24.8133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.795   -0.955  -24.5865]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -72.13387101763934, time: 48.38
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.015   -1.     -24.9688]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.935   -0.93   -24.7014]
18800 50

12600 50
steps: 629950, episodes: 12600, mean episode reward: -39.596186212774, time: 48.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.02    -1.     -25.0763]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.83    -0.985  -25.3908]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -36.532654483582704, time: 48.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.265   -1.     -22.3132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.825   -1.     -25.3881]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -37.99912579524422, time: 48.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.29    -1.     -23.8783]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.85    -1.     -25.3906]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -38.503515938663234, time: 48.252
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.96    -1.     -24.4741]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.69    -1.     -25.3042]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -35.89304163374924, time: 46.983
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.64    -0.99   -24.8134]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.735   -1.     -25.3233]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -37.48645586686349, time: 50.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.965   -1.     -25.0033]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -1.     -25.2979]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -36.29370944374778, time: 48.97
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.33    -0.985  -24.2215]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.735   -1.     -25.3095]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -31.346528836936482, time: 47.786
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.21    -1.     -24.5298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.865   -1.     -25.4071]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -37.923398336581656, time: 48.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.325   -1.     -24.7981]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.715  -1.    -25.301]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -34.40036135212815, time: 48.546
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.155   -1.     -24.4788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.84    -1.     -25.3736]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -35.2379627418639, time: 48.281
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.48    -1.     -24.2446]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.79    -1.     -25.3463]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -40.57076327730573, time: 47.733
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.455   -1.     -25.1696]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -0.995  -25.2486]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -32.10341124492975, time: 48.979
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.65    -1.     -25.3287]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.725   -1.     -25.3156]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -39.9788440774961, time: 49.652
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.365   -1.     -25.2046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.44    -1.     -25.1874]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -42.561868059005484, time: 48.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.54    -1.     -24.7314]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.68    -1.     -25.2805]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -39.32668171193775, time: 48.236
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.45    -1.     -25.2293]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.74    -0.995  -25.3139]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -34.84089076074153, time: 48.284
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.395   -1.     -25.1665]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.77    -0.995  -25.3365]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -34.59047449076928, time: 48.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.36    -1.     -25.1221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.85    -1.     -25.3925]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -38.25871682299402, time: 48.807
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.06    -1.     -25.0556]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.54    -1.     -25.1919]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -37.22954012656522, time: 48.189
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.865  -1.    -24.915]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.755   -0.995  -25.3657]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -34.4483463776228, time: 49.04
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.5     -1.     -25.2765]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.84    -0.995  -25.3786]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -36.021375458033965, time: 48.339
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.53    -1.     -25.3062]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.975   -1.     -24.9292]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -39.74604942397579, time: 48.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.965  -1.    -25.044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -1.     -25.2584]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -38.83237539181801, time: 49.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.17    -1.     -23.1666]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.285   -1.     -24.6201]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -31.65510447635139, time: 48.469
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.405   -1.     -23.3046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.57    -1.     -25.2277]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -33.2802928749062, time: 48.858
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.16    -1.     -21.9605]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.45    -0.99   -24.5974]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -35.2470788059246, time: 49.584
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.44    -1.     -23.0507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.355   -1.     -25.0787]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -38.59197400261803, time: 48.383
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.03    -0.995  -23.0871]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.35    -0.995  -25.0669]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -32.901293267129574, time: 49.126
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.93    -0.995  -22.2623]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.435   -0.995  -24.5598]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -33.17319858729847, time: 48.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.105   -1.     -21.4859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.76    -0.985  -24.8205]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -30.01762590920709, time: 48.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.025   -1.     -23.2502]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.765   -0.99   -24.8606] 50
steps: 629950, episodes: 12600, mean episode reward: -73.15368252250228, time: 47.994
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.335   -0.51   -15.8693]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.23    -0.995  -24.4936]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -68.9238665590231, time: 48.599
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.88   -0.655 -16.754]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.26    -0.995  -25.0219]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -68.59497024946185, time: 47.455
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.255   -0.76   -19.2887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.625   -1.     -25.2537]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -65.1190158003465, time: 47.778
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.045   -0.78   -18.8158]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.915   -0.985  -24.7497]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -60.27682434363965, time: 47.666
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.285   -0.82   -16.9016]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.155   -1.     -24.9316]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -66.74444387953392, time: 49.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.305   -0.66   -13.6762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.52   -0.995 -25.224]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -55.55861888977773, time: 47.34
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.67    -0.61   -12.8333]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.51    -0.995  -25.1537]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -57.245356247948266, time: 48.104
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.99    -0.845  -15.8372]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.73    -1.     -25.3074]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -61.4426724405418, time: 47.499
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.295   -0.84   -16.4741]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.11    -0.99   -24.8981]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -55.38464012740083, time: 47.943
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.925   -0.765  -13.3636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.42    -1.     -25.1043]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -57.177294604656275, time: 47.779
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.52    -0.755  -14.3615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.59    -0.985  -25.2019]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -61.035237582925035, time: 48.034
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.08    -0.94   -16.7902]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.7     -1.     -25.2758]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -60.379836280044486, time: 47.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.91    -0.945  -15.7195]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.16    -1.     -24.8836]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -57.45175635996593, time: 48.569
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.22    -0.915  -14.6452]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.32    -1.     -25.0145]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -66.58865173976822, time: 47.29
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.78    -0.785  -13.5209]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.93    -1.     -24.8024]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -67.10062769644031, time: 48.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.625   -0.735  -12.8763]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.24    -0.995  -25.0219]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -56.21256059720374, time: 47.729
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.645   -0.78   -14.1136]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.555  -0.995 -25.18 ]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -56.802211937046266, time: 48.7
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.295   -0.88   -16.1133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.615   -0.995  -25.2255]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -58.259537573789466, time: 48.381
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.325   -0.895  -17.1701]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.465   -0.975  -25.1419]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -62.55820717327282, time: 48.164
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.295   -0.735  -15.3461]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.495   -0.995  -25.2055]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -61.2288958597118, time: 47.967
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.005   -0.84   -16.5485]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.695   -1.     -25.3081]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -58.31554394969798, time: 48.211
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.24    -0.765  -15.0362]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.7     -1.     -25.2907]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -55.936488274988854, time: 47.862
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.76    -0.875  -17.7718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.685  -0.995 -25.266]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -60.40055248412802, time: 49.063
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.085  -0.87  -15.92 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.455   -1.     -25.1314]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -74.1425300300163, time: 48.141
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.55    -0.74   -16.0014]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.84    -0.995  -24.7229]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -57.15619905582258, time: 47.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.77   -0.975 -18.8  ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -1.     -25.2571]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -56.096278421462145, time: 48.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.215   -0.92   -16.9438]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.57    -0.99   -25.1981]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -58.91024290530446, time: 48.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.      -0.895  -16.8266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.475  -0.995 -25.162]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -53.826172463010465, time: 49.027
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.915   -0.855  -15.8261]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.45    -1.     -25.1117]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -50.70092348112834, time: 48.241
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.525   -0.925  -15.9022]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.575   -0.995  -25.1872]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -63.960893104226756, time: 48.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.715   -0.855  -16.6248]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.235   -1.     -24.9973]
18800
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.07   -0.28   -4.3144]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.29    -0.96   -24.2964]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -67.67079619597865, time: 47.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.985  -0.28   -4.5367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.43    -0.96   -24.3523]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -66.98314484581448, time: 47.089
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.08   -0.295  -5.51 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.68    -0.995  -24.5641]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -70.27908360537738, time: 48.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.195  -0.265  -4.5643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.8     -0.97   -24.5925]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -58.38212775622131, time: 48.052
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.49   -0.285  -4.7953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.07    -0.99   -24.7835]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -65.34915428565337, time: 47.902
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.025  -0.23   -4.0569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.965   -0.995  -24.7808]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -68.2961838722355, time: 47.293
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.805  -0.24   -4.6595]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.01    -0.995  -24.7687]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -65.34781591495351, time: 47.719
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.42    -0.295   -6.3303]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.555   -0.955  -24.4619]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -59.37551317586257, time: 47.511
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.43    -0.28    -5.3119]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.715   -0.985  -24.5959]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -67.47271215440097, time: 47.969
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.385   -0.315   -7.0876]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.97    -0.98   -24.0619]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -65.1058435140781, time: 48.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.425   -0.305   -5.4544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.525   -1.     -24.4261]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -69.71187023701286, time: 47.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.63    -0.305   -5.8923]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.425   -0.99   -24.4293]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -67.92202321028, time: 48.49
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.665   -0.275   -5.1589]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.975   -0.995  -24.7724]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -60.73140802227294, time: 49.302
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.215   -0.345   -7.2183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.285   -1.     -24.9826]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -63.34297997485806, time: 48.53
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.88   -0.24   -3.9635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.36   -0.995 -25.051]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -67.60273584559404, time: 47.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.36   -0.29   -4.9801]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.175   -0.995  -24.9302]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -56.872449930295325, time: 47.413
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.615  -0.315  -4.2131]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.925   -0.99   -24.7152]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -58.38336052037519, time: 48.248
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.05    -0.3     -6.0211]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.395   -0.99   -25.0486]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -55.60812267827223, time: 48.699
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.835   -0.345   -8.3328]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.67    -0.995  -25.2482]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -56.651890100429554, time: 48.425
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.45    -0.34    -6.6355]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.67    -0.995  -25.2607]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -56.554495762358684, time: 48.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.515  -0.265  -4.9982]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.965   -0.99   -24.7647]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -68.78293095845815, time: 47.895
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.2     -0.33    -6.2899]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.275   -0.995  -24.9892]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -63.78163080622471, time: 48.254
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.065   -0.29    -7.2702]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.535  -0.995 -25.2  ]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -61.06562317151051, time: 48.97
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.84    -0.345   -7.8715]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.19    -0.99   -24.9465]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -59.74103079990822, time: 48.67
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.755   -0.325   -6.5383]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.45    -0.995  -25.1413]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -65.16215254831522, time: 48.407
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.695   -0.325   -7.9729]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.435  -0.995 -25.153]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -64.44388904167073, time: 48.126
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.805   -0.32    -5.9031]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.46   -0.995 -25.157]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -69.10380465353902, time: 47.575
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.135   -0.265   -5.4608]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.435   -0.995  -25.1206]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -61.956329280444045, time: 49.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.93    -0.3     -6.3791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.1     -0.975  -24.9374]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -64.20442275204861, time: 48.795
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.68    -0.325   -7.6593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.415   -0.99   -25.0989]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -64.67483473120362, time: 48.712
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.475  -0.32   -4.4931]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.425   -0.99   -25.0999]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -56.91025086926305, time: 47.564
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.09    -0.935  -19.3362]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.065   0.     -0.5426]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -77.77077588509734, time: 48.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.615   -0.935  -19.7722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.375  -0.005  -0.1677]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -79.8128761327578, time: 46.526
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.64    -0.855  -20.5317]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.74   -0.04   -0.8008]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -86.27555975428402, time: 47.819
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.02    -0.965  -20.1998]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.315  -0.01   -0.1953]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -76.17035429921857, time: 46.949
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.61    -0.905  -21.2462]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.78   -0.01   -0.3645]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -79.18347420046919, time: 47.359
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.175   -0.86   -20.7337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.81   -0.04   -0.4748]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -73.29021594754764, time: 47.79
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.02    -0.94   -21.2712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.43  -0.035 -0.764]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -77.7206251084605, time: 47.015
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.62    -0.87   -20.0692]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.335  -0.035  -0.5613]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -70.56145781206163, time: 48.203
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.89    -0.855  -20.9772]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.79   -0.04   -0.9356]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -84.67870691545919, time: 47.914
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.8     -0.81   -20.4791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.69   -0.02   -0.7429]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -81.37464833742087, time: 47.525
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.395   -0.85   -21.6117]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.08   -0.06   -1.7223]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -87.0235607438646, time: 47.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.605   -0.865  -20.5762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.795  -0.065  -1.7414]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -76.87863207450518, time: 47.839
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.295   -0.85   -21.2593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.92   -0.055  -2.1749]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -73.85651651512804, time: 48.351
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.71    -0.795  -23.2229]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.085 -0.1   -3.874]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -79.01497246234732, time: 47.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.83    -0.59   -20.8238]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.805  -0.085  -2.5206]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -77.28030294834159, time: 47.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.725   -0.66   -20.6854]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.6    -0.015  -1.4669]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -69.57991569484908, time: 47.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.16    -0.82   -21.9282]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.97   -0.06   -3.6549]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -66.85069869089561, time: 47.699
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.89    -0.92   -22.2716]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.245  -0.02   -1.8275]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -60.410389385929356, time: 48.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.235   -0.92   -22.6373]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.79   -0.035  -2.2068]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -60.809832964125725, time: 47.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.595   -0.975  -24.2664]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.72   -0.015  -2.1392]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -66.37320062057579, time: 48.126
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.13    -0.9    -23.5735]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.12   -0.055  -2.3154]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -73.43544521279699, time: 47.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.415   -0.905  -23.5923]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.015  -0.05   -2.4577]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -68.70233839523729, time: 48.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.44    -0.875  -22.6804]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.25    0.     -1.1686]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -78.59913450385783, time: 48.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.41    -0.95   -23.3124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.1    -0.04   -2.2665]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -71.49065298417732, time: 47.678
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.67    -0.98   -23.7342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.975  -0.05   -2.3218]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -70.29479831702427, time: 47.993
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.155   -0.98   -22.9811]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.455  -0.055  -2.0073]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -71.42322589346858, time: 48.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.105   -0.95   -23.6155]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.265  -0.025  -1.5581]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -64.44175651755138, time: 47.311
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.4     -0.985  -22.1691]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.02   -0.015  -1.8477]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -63.038502759411955, time: 48.354
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.965   -0.96   -22.8709]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.6    -0.045  -1.9652]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -67.00511949473962, time: 48.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.395   -0.885  -22.2923]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.04   -0.04   -2.6699]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -66.6303322456145, time: 48.048
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.245   -0.92   -22.8161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.285  -0.115  -3.3068]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -70.44736750830187, time: 48.425
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.285   -0.975  -23.5522]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.005  -0.19   -3.6953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.405   -1.     -15.2272]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -65.13688131085075, time: 47.168
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.47   -0.32   -3.7592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.26    -0.95   -14.6856]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -65.31805839027145, time: 47.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.525  -0.275  -4.2988]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.185  -0.97  -14.254]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -59.19404440265768, time: 47.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.605  -0.32   -5.0157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.89    -0.98   -14.9264]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -60.38657974949365, time: 47.247
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.475  -0.44   -5.0795]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.115   -1.     -15.2065]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -63.81510794311554, time: 47.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.14    -0.425   -6.7487]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.485   -0.99   -13.7777]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -60.18273104429809, time: 47.262
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.31   -0.445  -5.1295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.27    -0.99   -12.9338]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -67.89303921945103, time: 49.218
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.735  -0.39   -4.9917]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.465   -1.     -14.3154]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -59.46176993128477, time: 48.34
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.185  -0.295  -3.8695]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.865  -0.985 -13.488]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -55.40439677862959, time: 47.638
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.875  -0.36   -3.5228]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.865  -1.    -14.309]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -58.47610419824979, time: 48.303
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.7    -0.365  -4.0436]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.155   -1.     -15.0932]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -58.4846246722344, time: 46.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.73   -0.295  -3.0891]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.345  -1.    -13.59 ]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -55.87803979971975, time: 47.313
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.335 -0.375 -3.258]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.43    -1.     -11.5415]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -62.9126349731562, time: 48.576
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.32   -0.395  -3.6903]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.335   -0.965  -13.3448]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -64.23499950879629, time: 48.34
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.17   -0.335  -3.5939]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.795   -0.995  -13.8577]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -66.57401971430082, time: 48.182
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.06   -0.335  -4.5891]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.915   -0.995  -12.2647]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -65.91087645231435, time: 48.404
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.87   -0.45   -4.2694]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.905   -0.995  -13.2407]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -66.98699514391288, time: 47.803
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.06   -0.43   -3.0577]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.235   -0.98   -14.3708]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -60.773286484131546, time: 47.972
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.92   -0.395  -4.1511]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.475   -0.99   -13.1687]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -63.16491631491583, time: 47.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.355  -0.385  -4.1888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.445   -1.     -13.7587]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -59.43786771799595, time: 47.656
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.67   -0.43   -4.1784]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.535   -0.99   -11.6399]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -52.188320891004004, time: 48.216
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.67   -0.315  -3.0102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.465  -0.985 -12.29 ]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -59.05037862977163, time: 48.117
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.015  -0.445  -3.8898]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.      -0.9    -11.4674]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -53.04329616189637, time: 48.055
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.535  -0.365  -4.3703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.555   -0.945  -10.7828]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -52.36481234693045, time: 47.508
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.69    -0.395   -6.7189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.59    -0.985  -11.1179]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -53.17392122806303, time: 47.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.875  -0.415  -5.9699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.215   -0.965  -10.9174]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -54.63314196183932, time: 48.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.515  -0.305  -4.7336]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.825   -0.95   -10.9096]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -50.97991960834032, time: 49.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.945  -0.365  -5.9522]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.54    -0.98   -10.4354]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -60.96436172128666, time: 48.804
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.37    -0.35    -6.4424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.015  -0.87  -10.392]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -52.55790846928556, time: 48.129
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.975  -0.345  -6.5449]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.73    -0.98   -10.6341]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -60.23782203995372, time: 48.113
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.72    -0.25    -8.1562]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.55   -0.995 -10.486]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -58.301819994222804, time: 47.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.575   -0.4     -9.3846]
[-48.25    -0.84   -24.2627]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.335   -0.66   -19.9631]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -66.86199996418536, time: 48.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.59    -0.81   -23.4017]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.935   -0.615  -19.9537]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -73.61775025243993, time: 48.364
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.56    -0.835  -22.3067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.625   -0.66   -19.1313]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -72.94217129062947, time: 49.879
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.945   -0.92   -23.0899]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.82    -0.66   -18.4766]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -73.94054881229198, time: 47.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.75    -0.835  -22.4083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.305   -0.59   -18.6018]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -73.61277972084264, time: 47.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.79    -0.83   -21.8103]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.935   -0.65   -20.8509]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -80.01089014745445, time: 48.93
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.68    -0.825  -22.7025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.99    -0.605  -21.8759]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -75.7041707703104, time: 47.147
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.79    -0.845  -22.1846]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.23    -0.635  -22.3525]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -90.13169772885189, time: 48.25
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.305   -0.78   -20.0689]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.66    -0.595  -21.4218]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -73.30638790545221, time: 46.901
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.115   -0.72   -18.9601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.25    -0.575  -21.3427]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -76.36049313742522, time: 47.146
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.365   -0.81   -20.7998]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.17    -0.475  -19.2757]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -71.3695198447311, time: 46.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.965   -0.865  -20.9116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.98    -0.55   -21.9083]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -73.5762596479069, time: 47.044
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.235   -0.875  -22.2117]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.265   -0.61   -21.7119]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -67.53664359634395, time: 48.361
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.285   -0.9    -21.6903]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.695   -0.625  -23.0446]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -71.03404187038836, time: 46.522
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.74    -0.92   -21.7588]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.43    -0.645  -22.2831]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -68.71680793103512, time: 47.745
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.045   -0.955  -21.1894]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.665   -0.69   -23.2935]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -65.61341003526343, time: 47.518
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.825   -0.945  -20.0959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.68    -0.62   -20.1167]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -65.96062295383625, time: 47.086
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.12    -0.95   -20.1094]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.855   -0.655  -20.9928]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -65.03780068898648, time: 47.793
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.335  -0.905 -20.125]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.205   -0.705  -20.9693]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -64.38207064540225, time: 47.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.555   -0.91   -20.1226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.055   -0.775  -21.9197]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -67.43096407874935, time: 46.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.28    -0.895  -20.8628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.225   -0.725  -21.9937]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -66.55938770932804, time: 47.18
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.325   -0.945  -19.1177]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.595   -0.715  -21.3188]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -66.99082525666368, time: 47.218
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.79    -0.895  -19.5711]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.825   -0.67   -20.2113]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -71.58404022894766, time: 47.045
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.735   -0.89   -19.6645]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.35    -0.705  -20.7195]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -60.040149346146976, time: 46.899
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.12    -0.96   -19.0455]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.42    -0.795  -21.0192]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -61.66279697976368, time: 49.037
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.355   -0.925  -19.0755]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.34    -0.775  -20.9522]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -63.20622941174889, time: 48.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.265   -0.91   -18.5478]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.435  -0.74  -21.095]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -63.53847155232322, time: 47.696
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.13    -0.92   -18.4723]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.62    -0.825  -20.4887]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -63.58139992418217, time: 49.233
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.31    -0.9    -20.2274]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.565   -0.76   -21.7938]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -63.739857995960975, time: 47.309
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.37    -0.875  -19.5399]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.525   -0.66   -22.2106]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -60.58225091646894, time: 47.996
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.745   -0.94   -17.8303]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.54    -0.655  -19.7309]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -66.18431358563734, time: 47.454
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.87    -0.94   -14.3942]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.64    -0.74   -16.8912]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -63.492977007836956, time: 50.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.235   -0.885  -14.1903]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.325   -0.815  -16.0183]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -53.96025789126503, time: 48.951
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.52    -0.925  -15.0086]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.915  -0.79  -16.277]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -69.66325181958072, time: 49.393
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.585  -0.965 -14.91 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.22    -0.92   -16.8632]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -61.3880878992796, time: 48.997
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.63    -0.895  -14.0537]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.64    -0.885  -17.1721]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -61.69825874405706, time: 48.807
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.01    -0.895  -13.0408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.42    -0.9    -15.5316]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -54.316341568563146, time: 50.319
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.815   -0.89   -13.0998]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.775   -0.885  -15.4408]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -69.1030008744541, time: 48.691
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.295   -0.875  -12.5866]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.11    -0.84   -15.1323]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -53.4956406661235, time: 49.753
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.32    -0.89   -10.4773]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.675   -0.84   -15.4008]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -58.916790020840196, time: 49.374
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.585   -0.87   -11.8071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.69    -0.83   -16.4261]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -59.902337983671, time: 49.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.245   -0.835  -10.7638]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.445   -0.855  -15.0105]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -64.72146888192518, time: 48.819
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.5     -0.855  -13.5008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.135   -0.725  -14.9027]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -48.948147166720844, time: 49.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.09    -0.86   -13.6443]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.05    -0.905  -16.0985]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -53.72148157665415, time: 50.065
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.245   -0.85   -12.8788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.635   -0.775  -15.1305]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -61.89527965580428, time: 48.405
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.215   -0.87   -11.6343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.66    -0.785  -14.3358]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -56.327587981383694, time: 49.409
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.82    -0.885  -12.7385]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.01    -0.82   -14.9935]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -55.1714478885432, time: 48.742
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.93    -0.925  -13.9935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.17    -0.845  -15.7706]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -60.11574552155469, time: 48.985
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.93    -0.89   -13.1249]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.59    -0.81   -16.9933]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -51.8726330091815, time: 49.651
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.32    -0.94   -12.5576]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.765   -0.85   -16.3481]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -63.79556142173546, time: 48.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.115   -0.95   -12.5491]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.835   -0.88   -17.7303]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -60.146653059694835, time: 48.626
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.775  -0.99  -12.458]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.225   -0.95   -17.2376]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -52.24549782321479, time: 49.535
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.335   -0.965  -13.1144]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.775   -0.87   -16.2733]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -54.95787865430386, time: 49.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.375   -0.875  -11.9157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.875   -0.91   -15.2764]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -51.01876393819654, time: 49.664
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.95   -0.95  -11.911]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.22    -0.845  -15.1162]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -61.87443778901215, time: 48.28
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.885   -0.955  -16.3827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.99    -0.76   -15.0677]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -57.811406689893964, time: 48.172
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.85    -0.955  -14.2904]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.47    -0.745  -14.2706]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -54.837865498967595, time: 48.971
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.12    -0.965  -12.1708]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.555   -0.81   -15.0644]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -52.427074965969176, time: 49.991
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.18    -0.93   -11.4876]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.725   -0.895  -16.6283]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -54.177919692962675, time: 49.8
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.615   -0.945  -12.8325]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.99    -0.89   -14.3764]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -56.36497130533089, time: 48.543
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.94    -0.92   -11.3871]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.975   -0.82   -13.9522]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -61.4451125418322, time: 48.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.74    -0.92   -14.1825]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.53    -0.8    -15.5768]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -55.36325736919401, time: 49.741
[-11.215   -0.885   -8.6695]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.145   -0.76   -22.4989]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -63.77114751820645, time: 48.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.04    -0.91    -9.1431]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.205   -0.76   -23.2193]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -65.40270668642752, time: 49.06
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.435   -0.935   -9.6774]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.375   -0.79   -23.4562]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -60.63557315520415, time: 49.255
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.44    -0.895  -10.6226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.895   -0.875  -23.8055]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -60.34382528968008, time: 48.732
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.91    -0.935  -11.1341]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.31    -0.77   -22.7279]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -74.65521110704297, time: 48.771
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.89   -0.88  -10.606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.2     -0.835  -23.8572]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -74.1245172921238, time: 49.644
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.07    -0.885   -9.3989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.925   -0.775  -22.2311]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -54.37419982210115, time: 49.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.645   -0.725   -8.4505]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.455   -0.79   -20.2981]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -57.01194442273663, time: 49.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.96    -0.73    -8.4702]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.61    -0.795  -20.0556]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -67.53512084002271, time: 49.487
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.135   -0.74    -7.8497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.435   -0.885  -22.4017]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -60.994430632179565, time: 48.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.475   -0.68    -7.7497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.575   -0.83   -19.0581]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -61.31978676447761, time: 49.008
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.095   -0.81    -9.2619]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.86    -0.79   -21.6963]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -61.452484049049474, time: 49.325
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.065   -0.76   -10.0234]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.66    -0.815  -21.4198]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -63.699980119814164, time: 49.068
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.105   -0.84    -9.4338]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.225   -0.815  -19.1132]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -62.97147131080107, time: 48.876
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.1     -0.865  -10.5402]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.745   -0.775  -20.2945]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -54.62575751268277, time: 48.973
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.62    -0.82   -10.0917]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.935   -0.86   -19.1372]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -53.59000798365141, time: 48.594
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.005   -0.955   -9.9115]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.975  -0.75  -18.678]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -59.22119000626098, time: 49.486
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.41    -0.95   -11.5157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.84    -0.895  -21.7967]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -63.38965014577496, time: 49.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.44    -0.91    -9.9579]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.595   -0.79   -21.0759]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -54.338213044111846, time: 47.863
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.925   -0.875   -9.9693]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.125   -0.92   -19.4489]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -56.404827271770834, time: 48.744
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.56    -0.88   -10.2948]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.72    -0.82   -20.6493]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -56.567616874928405, time: 48.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.755   -0.875   -9.8955]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.925   -0.875  -21.4945]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -59.833587488969336, time: 48.069
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.375   -0.97   -12.2066]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.925  -0.855 -20.022]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -61.20784639207038, time: 48.59
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.145   -0.93   -11.4154]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.235  -0.88  -21.253]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -61.918592726101124, time: 47.689
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.585   -0.96   -10.2113]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.39    -0.775  -17.5809]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -67.25134697246915, time: 48.138
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.425   -0.99   -12.1952]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.88    -0.68   -16.5511]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -55.061995569096844, time: 47.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.445   -0.97   -10.3605]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.025   -0.58   -14.7458]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -48.20851159186398, time: 48.454
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.555   -1.      -9.7198]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.965  -0.745 -15.129]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -52.18530526037605, time: 48.707
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.84    -0.995   -9.9864]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.475   -0.845  -16.7944]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -56.08975045781817, time: 47.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.565  -0.995  -9.316]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.135   -0.785  -16.6332]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -54.137558088509856, time: 48.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.375   -0.98    -9.5025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.32    -0.685  -13.4913]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -54.45178548593312, time: 48.501
agent0_energy_min, agent0_energy_max, agent0_energy_avgsteps: 939950, episodes: 18800, mean episode reward: -64.18972383653725, time: 48.114
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.285   -1.     -25.0412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.065   -0.97   -24.8401]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -65.48568868477801, time: 48.296
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.72    -0.975  -22.9873]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.245   -0.975  -24.9382]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -54.85268641940349, time: 47.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.63    -1.     -24.8836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.495   -0.96   -25.1382]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -60.669325708908545, time: 47.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.135   -0.985  -24.6916]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.74    -0.92   -24.5666]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -57.880172839560444, time: 48.959
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.485   -1.     -25.1417]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.035   -0.9    -24.7997]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -63.70655772478994, time: 46.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-49.5     -1.     -25.1938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.165  -0.945 -24.89 ]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -72.55967174330354, time: 48.721
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.625   -1.     -24.7916]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.295   -0.98   -25.0257]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.31 hr
 50
steps: 939950, episodes: 18800, mean episode reward: -62.19459788233245, time: 48.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.655   -0.905  -18.3113]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.24    -0.995  -24.9829]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -60.07162722116832, time: 48.788
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.73    -0.905  -16.6553]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.565   -1.     -25.2346]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -59.023472517147056, time: 48.871
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.655   -0.9    -18.6227]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.18    -1.     -24.9066]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -52.735040127729484, time: 48.523
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.615   -0.935  -20.2859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.515   -1.     -25.1433]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -56.73394085764233, time: 47.407
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.6     -0.97   -22.0938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.185   -1.     -24.8701]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -61.04212456373493, time: 47.963
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.58    -0.955  -18.5337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.34   -1.    -25.009]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -56.73723388003675, time: 46.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.27    -0.92   -19.1935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.715   -0.995  -25.2965]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.32 hr

[-11.86    -0.29    -6.3778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.695   -0.995  -25.2832]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -58.2360290062704, time: 48.459
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.995   -0.29    -7.0968]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595   -1.     -25.2013]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -57.95849133585004, time: 48.587
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.705   -0.26    -5.7884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -1.     -25.1498]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -63.36056802697537, time: 48.728
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.795   -0.33    -7.1949]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.495   -1.     -25.2201]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -59.835038443430655, time: 48.043
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.245   -0.305   -6.3064]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595  -1.    -25.265]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -56.85023132154904, time: 47.994
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.605   -0.3     -6.2728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.74    -0.995  -25.3276]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -67.51905107825206, time: 44.481
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.825   -0.275   -8.4527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.395   -0.99   -25.0961]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.32 hr

[-3.595  -0.02   -1.5903]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -65.12984238486271, time: 48.048
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.99    -0.99   -23.5088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.865  -0.035  -1.7315]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -77.57210618086991, time: 49.242
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.92   -0.97  -23.279]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.415  -0.155  -3.5572]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -63.67099878730573, time: 48.244
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.605  -0.98  -22.28 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.445  -0.05   -2.2138]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -70.56445576717255, time: 47.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.55    -0.97   -23.0403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.39  -0.05  -3.887]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -74.02447960117004, time: 48.249
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.35    -0.99   -22.6887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.85   -0.175  -4.1816]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -72.42659912583477, time: 41.791
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.675   -0.97   -23.1095]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.505   -0.145   -5.2447]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.32 hr

18800 50
steps: 939950, episodes: 18800, mean episode reward: -41.15763329174147, time: 49.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.645   -1.     -20.2058]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.97    -0.975  -24.9369]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -32.18697644895601, time: 49.029
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.195   -0.995  -23.2129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.72   -1.    -23.973]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -31.176720530345712, time: 49.485
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.07    -1.     -23.3206]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.68    -0.995  -24.2606]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -35.632642949172975, time: 48.892
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.115   -1.     -22.8189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.55   -0.995 -24.182]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -34.94839436307529, time: 48.492
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.29    -1.     -23.7625]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.155   -1.     -25.0311]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -29.326116863260378, time: 47.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-48.94   -1.    -24.966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.38    -1.     -25.1121]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -45.99957561942359, time: 40.572
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.      -1.     -24.1152]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.03    -1.     -24.9214]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.33 hr

agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.025   -0.995  -11.7219]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -55.47977398282193, time: 47.8
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.775   -0.4     -7.9586]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.755   -0.915   -9.9467]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -60.31108756753995, time: 49.178
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.875  -0.355  -6.8475]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.925   -0.795  -10.9724]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -52.239190312846716, time: 47.742
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.675   -0.375   -7.3169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.995   -0.96   -11.0388]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -59.33357429452708, time: 47.824
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.68    -0.35    -9.8996]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.3     -0.89   -11.1285]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -55.38682000476736, time: 44.679
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.125  -0.27   -9.95 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.14    -0.76    -9.9559]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -56.16664662290901, time: 37.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.48    -0.345   -9.6092]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.27    -0.96   -11.3843]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.33 hr

[-32.94   -0.925 -18.485]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.51    -0.675  -20.1561]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -64.34856730240682, time: 47.597
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.815   -0.955  -19.0426]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.535   -0.635  -17.9414]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -64.97169737639777, time: 51.376
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.69    -0.945  -19.8735]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.025   -0.64   -17.8983]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -63.09956730801955, time: 47.461
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.53    -0.905  -19.2098]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.39    -0.65   -16.8607]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -62.263688291705165, time: 47.241
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.24    -0.935  -20.1452]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.035   -0.66   -16.8855]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -60.91465461033721, time: 41.608
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.225   -0.925  -19.8046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.89    -0.62   -17.2466]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -60.5558449424677, time: 33.96
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.31    -0.925  -19.6167]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.335   -0.605  -15.2896]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.33 hr

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.475  -0.93  -13.369]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.14    -0.85   -15.0203]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -67.07344846567985, time: 49.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.275   -0.96   -12.1899]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.85    -0.765  -14.3299]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -57.041488512559575, time: 48.756
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.45    -0.905  -13.2312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.32   -0.87  -14.297]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -56.3016115765798, time: 48.556
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.73    -0.955  -14.1768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.26    -0.87   -14.4291]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -50.111807140950845, time: 44.265
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.02    -0.955  -11.5391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.875   -0.815  -14.8039]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -53.44870844218814, time: 35.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.935   -0.975  -12.9053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.725   -0.885  -14.7741]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -51.43428195856053, time: 31.519
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.21    -0.975  -12.7532]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.09    -0.935  -14.9904]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.34 hr

[-13.02    -0.98   -10.1888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.21    -0.78   -13.2516]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -50.40544324356781, time: 47.306
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.155   -0.995  -10.2725]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.855   -0.755  -13.0931]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -53.552649148075886, time: 48.284
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.4    -1.    -10.298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.62    -0.84   -14.4056]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -53.32764230474199, time: 46.383
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.84    -0.995  -10.0412]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.355   -0.82   -15.0683]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -46.07775125042363, time: 39.598
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.515   -0.99    -9.2885]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.98    -0.79   -12.4806]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -52.88578292080182, time: 33.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.22    -0.9     -8.8979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.59    -0.755  -14.7016]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -50.278020409119236, time: 29.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.19    -0.975  -10.1989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.045   -0.71   -13.3634]
...Finished!
Trained episodes: 1 -> 20000
Total time: 1.34 hr
