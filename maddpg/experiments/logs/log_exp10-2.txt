python train.py --scenario wanderer2_2agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-4 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-4 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-4 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-5 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-5 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-5 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-6 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-6 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer2_2agents-6 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-5__2018-07-15_11-12-40...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -176.99322020541138, time: 221.939
agent0_energy_min, agent0_attention_min
[-15.93193193 -16.58358358]
agent1_energy_min, agent1_attention_min
[-19.49249249 -15.82182182]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -282.3037901514756, time: 318.011
agent0_energy_min, agent0_attention_min
[-7.64  -8.485]
agent1_energy_min, agent1_attention_min
[-14.969 -14.701]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -135.0663009885854, time: 320.027
agent0_energy_min, agent0_attention_min
[-0.198 -0.038]
agent1_energy_min, agent1_attention_min
[-4.406 -3.633]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -141.7626549863681, time: 319.219
agent0_energy_min, agent0_attention_min
[-0.059 -0.026]
agent1_energy_min, agent1_attention_min
[-12.488  -2.994]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -136.55023119079033, time: 314.069
agent0_energy_min, agent0_attention_min
[-0.55  -0.039]
agent1_energy_min, agent1_attention_min
[-34.127  -1.273]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -134.0372524286047, time: 318.991
agent0_energy_min, agent0_attention_min
[-0.308 -0.036]
agent1_energy_min, agent1_attention_min
[-37.678  -3.885]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -120.38586285409876, time: 317.188
agent0_energy_min, agent0_attention_min
[-6.367 -0.088]
agent1_energy_min, agent1_attention_min
[-37.18   -4.266]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -107.79453414422406, time: 314.221
agent0_energy_min, agent0_attention_min
[-42.     -0.412]
agent1_energy_min, agent1_attention_min
[-40.022  -1.756]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -91.28651322835898, time: 313.325
agent0_energy_min, agent0_attention_min
[-43.006  -0.225]
agent1_energy_min, agent1_attention_min
[-44.059  -0.647]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -88.25023726461932, time: 312.476
agent0_energy_min, agent0_attention_min
[-4.9763e+01 -4.1000e-02]
agent1_energy_min, agent1_attention_min
[-46.246  -0.815]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -94.32914262609452, time: 311.913
agent0_energy_min, agent0_attention_min
[-49.328  -0.346]
agent1_energy_min, agent1_attention_min
[-37.836  -1.194]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -82.94865046294889, time: 306.28
agent0_energy_min, agent0_attention_min
[-4.9733e+01 -3.7000e-02]
agent1_energy_min, agent1_attention_min
[-31.438  -0.97 ]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -77.26888284769893, time: 300.369
agent0_energy_min, agent0_attention_min
[-49.604  -0.291]
agent1_energy_min, agent1_attention_min
[-36.268  -0.953]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -79.51595230573301, time: 299.19
agent0_energy_min, agent0_attention_min
[-49.867  -0.081]
agent1_energy_min, agent1_attention_min
[-37.631  -0.626]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -75.56829319785192, time: 303.695
agent0_energy_min, agent0_attention_min
[-4.9968e+01 -1.7000e-02]
agent1_energy_min, agent1_attention_min
[-37.412  -0.879]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -73.45010117563041, time: 301.103
agent0_energy_min, agent0_attention_min
[-4.9958e+01 -2.7000e-02]
agent1_energy_min, agent1_attention_min
[-35.09   -0.604]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -77.37142027090988, time: 296.738
agent0_energy_min, agent0_attention_min
[-4.9948e+01 -3.6000e-02]
agent1_energy_min, agent1_attention_min
[-33.212  -1.296]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -77.92594087299786, time: 302.433
agent0_energy_min, agent0_attention_min
[-4.9976e+01 -1.6000e-02]
agent1_energy_min, agent1_attention_min
[-31.543  -0.605]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -71.63049610680831, time: 302.068
agent0_energy_min, agent0_attention_min
[-4.9978e+01 -6.0000e-03]
agent1_energy_min, agent1_attention_min
[-31.283  -0.308]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -71.4692352070502, time: 303.581
agent0_energy_min, agent0_attention_min
[-4.9941e+01 -2.2000e-02]
agent1_energy_min, agent1_attention_min
[-29.474  -0.901]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -70.67521731605882, time: 303.207
agent0_energy_min, agent0_attention_min
[-4.6784e+01 -1.5000e-02]
agent1_energy_min, agent1_attention_min
[-31.543  -1.202]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -86.35994824625496, time: 302.864
agent0_energy_min, agent0_attention_min
[-3.852e+01 -3.100e-02]
agent1_energy_min, agent1_attention_min
[-29.362  -5.104]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -77.41699029284086, time: 303.002
agent0_energy_min, agent0_attention_min
[-35.269  -0.042]
agent1_energy_min, agent1_attention_min
[-34.453  -1.025]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -65.30683802465362, time: 301.501
agent0_energy_min, agent0_attention_min
[-35.203  -0.636]
agent1_energy_min, agent1_attention_min
[-27.401  -1.283]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -96.08306629489182, time: 301.283
agent0_energy_min, agent0_attention_min
[-36.091  -1.47 ]
agent1_energy_min, agent1_attention_min
[-29.827  -1.717]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -111.3877494281855, time: 303.925
agent0_energy_min, agent0_attention_min
[-37.996  -3.328]
agent1_energy_min, agent1_attention_min
[-14.17  -11.226]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -78.83633954639349, time: 301.797
agent0_energy_min, agent0_attention_min
[-37.104  -5.093]
agent1_energy_min, agent1_attention_min
[-26.526  -0.721]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -68.006958408073, time: 304.502
agent0_energy_min, agent0_attention_min
[-41.217  -3.253]
agent1_energy_min, agent1_attention_min
[-17.011  -5.478]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -74.60391693505409, time: 308.041
agent0_energy_min, agent0_attention_min
[-40.899  -0.923]
agent1_energy_min, agent1_attention_min
[-14.577  -2.88 ]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -86.06632793134483, time: 306.689
agent0_energy_min, agent0_attention_min
[-35.562  -2.259]
agent1_energy_min, agent1_attention_min
[-15.535  -7.936]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -75.48996382026276, time: 308.051
agent0_energy_min, agent0_attention_min
[-28.667  -0.4  ]
agent1_energy_min, agent1_attention_min
[-14.76   -4.499]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -65.26929228128326, time: 307.087
agent0_energy_min, agent0_attention_min
[-19.782  -0.402]
agent1_energy_min, agent1_attention_min
[-14.579  -5.778]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -71.1033481931773, time: 305.929
agent0_energy_min, agent0_attention_min
[-20.115  -0.478]
agent1_energy_min, agent1_attention_min
[-16.     -4.447]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -65.8411870592608, time: 306.627
agent0_energy_min, agent0_attention_min
[-20.115  -0.799]
agent1_energy_min, agent1_attention_min
[-16.033  -3.957]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -84.193115296726, time: 308.13
agent0_energy_min, agent0_attention_min
[-19.534  -2.769]
agent1_energy_min, agent1_attention_min
[-17.093  -6.688]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -67.98682314599742, time: 306.611
agent0_energy_min, agent0_attention_min
[-19.026  -0.627]
agent1_energy_min, agent1_attention_min
[-16.569  -5.376]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -68.1992039596373, time: 304.821
agent0_energy_min, agent0_attention_min
[-21.674  -1.017]
agent1_energy_min, agent1_attention_min
[-16.587  -3.408]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -67.52456580877761, time: 305.967
agent0_energy_min, agent0_attention_min
[-22.487  -0.622]
agent1_energy_min, agent1_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-3__2018-07-15_11-12-26...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -161.73247315609316, time: 210.836
agent0_energy_min, agent0_attention_min
[-19.66066066 -14.50850851]
agent1_energy_min, agent1_attention_min
[-16.45045045 -16.51451451]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -250.1710306491478, time: 305.368
agent0_energy_min, agent0_attention_min
[-11.885  -9.053]
agent1_energy_min, agent1_attention_min
[-12.25  -24.582]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -129.78797622144285, time: 313.272
agent0_energy_min, agent0_attention_min
[-0.168 -0.991]
agent1_energy_min, agent1_attention_min
[-12.045 -33.517]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -121.90678782901112, time: 311.244
agent0_energy_min, agent0_attention_min
[-7.874 -1.234]
agent1_energy_min, agent1_attention_min
[-19.38  -26.688]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -101.17567696675326, time: 309.067
agent0_energy_min, agent0_attention_min
[-45.462  -0.16 ]
agent1_energy_min, agent1_attention_min
[-21.224 -22.925]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -97.36069788338706, time: 309.945
agent0_energy_min, agent0_attention_min
[-46.266  -0.694]
agent1_energy_min, agent1_attention_min
[-39.001  -7.392]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -95.70922058526921, time: 310.971
agent0_energy_min, agent0_attention_min
[-46.705  -0.622]
agent1_energy_min, agent1_attention_min
[-25.35  -19.056]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -87.11626515621442, time: 311.12
agent0_energy_min, agent0_attention_min
[-47.494  -0.85 ]
agent1_energy_min, agent1_attention_min
[-28.587 -16.95 ]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -83.67766366791086, time: 306.918
agent0_energy_min, agent0_attention_min
[-46.162  -1.177]
agent1_energy_min, agent1_attention_min
[-36.229 -10.996]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -85.89238214483684, time: 306.328
agent0_energy_min, agent0_attention_min
[-46.8    -0.532]
agent1_energy_min, agent1_attention_min
[-36.224 -10.555]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -90.8993626894959, time: 307.854
agent0_energy_min, agent0_attention_min
[-45.651  -0.076]
agent1_energy_min, agent1_attention_min
[-37.923  -9.83 ]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -77.80752125613155, time: 308.36
agent0_energy_min, agent0_attention_min
[-47.715  -0.664]
agent1_energy_min, agent1_attention_min
[-35.995 -12.698]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -68.77616177808264, time: 308.342
agent0_energy_min, agent0_attention_min
[-48.027  -0.333]
agent1_energy_min, agent1_attention_min
[-34.768 -14.592]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -74.53539702341708, time: 309.648
agent0_energy_min, agent0_attention_min
[-48.316  -0.341]
agent1_energy_min, agent1_attention_min
[-38.984  -9.719]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -74.87930699258439, time: 310.3
agent0_energy_min, agent0_attention_min
[-48.772  -0.229]
agent1_energy_min, agent1_attention_min
[-36.801 -11.974]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -83.80500254282128, time: 308.475
agent0_energy_min, agent0_attention_min
[-4.8609e+01 -1.9000e-02]
agent1_energy_min, agent1_attention_min
[-40.696  -7.332]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -80.06275467294003, time: 307.618
agent0_energy_min, agent0_attention_min
[-4.8974e+01 -1.5000e-02]
agent1_energy_min, agent1_attention_min
[-38.813  -9.29 ]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -86.45980735807152, time: 308.585
agent0_energy_min, agent0_attention_min
[-4.8252e+01 -1.2000e-02]
agent1_energy_min, agent1_attention_min
[-37.041 -12.05 ]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -69.09586349876751, time: 310.861
agent0_energy_min, agent0_attention_min
[-4.942e+01 -2.200e-02]
agent1_energy_min, agent1_attention_min
[-33.784 -15.999]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -69.14654225457382, time: 309.379
agent0_energy_min, agent0_attention_min
[-4.93e+01 -2.10e-02]
agent1_energy_min, agent1_attention_min
[-31.202 -18.254]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -74.10544029694844, time: 309.152
agent0_energy_min, agent0_attention_min
[-49.44  -0.2 ]
agent1_energy_min, agent1_attention_min
[-26.714 -22.022]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -69.60571643906434, time: 310.149
agent0_energy_min, agent0_attention_min
[-4.6453e+01 -4.3000e-02]
agent1_energy_min, agent1_attention_min
[-25.088 -24.483]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -71.22674057854401, time: 306.472
agent0_energy_min, agent0_attention_min
[-44.709  -1.279]
agent1_energy_min, agent1_attention_min
[-21.599 -27.905]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -113.47034476701846, time: 307.613
agent0_energy_min, agent0_attention_min
[-29.952  -1.277]
agent1_energy_min, agent1_attention_min
[-15.647 -29.892]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -73.89864975363665, time: 308.148
agent0_energy_min, agent0_attention_min
[-25.546  -1.609]
agent1_energy_min, agent1_attention_min
[-15.836 -23.551]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -101.29282570183844, time: 309.444
agent0_energy_min, agent0_attention_min
[-31.922  -0.273]
agent1_energy_min, agent1_attention_min
[-15.706 -18.513]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -69.18835895735842, time: 309.181
agent0_energy_min, agent0_attention_min
[-34.155  -1.535]
agent1_energy_min, agent1_attention_min
[-13.02  -25.546]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -74.54893958549614, time: 309.861
agent0_energy_min, agent0_attention_min
[-29.714  -2.293]
agent1_energy_min, agent1_attention_min
[-16.125 -25.408]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -76.11884073764524, time: 310.761
agent0_energy_min, agent0_attention_min
[-27.897  -3.674]
agent1_energy_min, agent1_attention_min
[-14.687 -28.122]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -63.36672811598544, time: 309.62
agent0_energy_min, agent0_attention_min
[-31.373  -2.935]
agent1_energy_min, agent1_attention_min
[-14.491 -24.011]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -82.71324905242128, time: 314.415
agent0_energy_min, agent0_attention_min
[-28.887  -1.983]
agent1_energy_min, agent1_attention_min
[-14.463 -23.804]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -70.47858405624726, time: 312.236
agent0_energy_min, agent0_attention_min
[-26.988  -0.068]
agent1_energy_min, agent1_attention_min
[-13.837 -27.619]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -74.34797517039348, time: 310.477
agent0_energy_min, agent0_attention_min
[-24.067  -2.564]
agent1_energy_min, agent1_attention_min
[-15.527 -27.753]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -72.38877585188817, time: 307.028
agent0_energy_min, agent0_attention_min
[-23.726  -2.682]
agent1_energy_min, agent1_attention_min
[-15.719 -30.398]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -71.50863784688687, time: 307.203
agent0_energy_min, agent0_attention_min
[-21.111  -2.431]
agent1_energy_min, agent1_attention_min
[-14.157 -26.267]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -64.52197358252877, time: 302.6
agent0_energy_min, agent0_attention_min
[-22.828  -2.028]
agent1_energy_min, agent1_attention_min
[-14.173 -25.31 ]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -64.7484533390977, time: 303.275
agent0_energy_min, agent0_attention_min
[-24.078  -2.363]
agent1_energy_min, agent1_attention_min
[-17.866 -19.444]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -79.2663072949404, time: 304.27
agent0_energy_min, agent0_attention_min
[-22.034  -4.464]
agent1_energy_min, agent1_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-6__2018-07-15_11-12-44...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -180.37804694218428, time: 220.888
agent0_energy_min, agent0_attention_min
[-16.57457457 -17.77277277]
agent1_energy_min, agent1_attention_min
[-16.03903904 -17.05605606]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -402.3270940180457, time: 315.87
agent0_energy_min, agent0_attention_min
[-11.139 -22.29 ]
agent1_energy_min, agent1_attention_min
[-16.518 -18.803]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -165.20738647781138, time: 317.777
agent0_energy_min, agent0_attention_min
[-10.103 -32.741]
agent1_energy_min, agent1_attention_min
[-19.156 -20.91 ]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -140.43925142316306, time: 313.272
agent0_energy_min, agent0_attention_min
[-13.407 -31.81 ]
agent1_energy_min, agent1_attention_min
[-40.834  -6.314]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -125.6837631449033, time: 312.756
agent0_energy_min, agent0_attention_min
[-31.32  -16.007]
agent1_energy_min, agent1_attention_min
[-46.197  -0.602]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -109.50001959945355, time: 313.281
agent0_energy_min, agent0_attention_min
[-46.494  -2.475]
agent1_energy_min, agent1_attention_min
[-44.274  -1.022]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -109.23143093832093, time: 316.305
agent0_energy_min, agent0_attention_min
[-47.436  -0.866]
agent1_energy_min, agent1_attention_min
[-45.586  -0.679]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -91.3905757687302, time: 314.787
agent0_energy_min, agent0_attention_min
[-49.017  -0.638]
agent1_energy_min, agent1_attention_min
[-45.239  -0.517]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -89.3860903057238, time: 313.032
agent0_energy_min, agent0_attention_min
[-49.015  -0.731]
agent1_energy_min, agent1_attention_min
[-46.708  -0.483]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -84.21219679442991, time: 308.622
agent0_energy_min, agent0_attention_min
[-48.924  -0.651]
agent1_energy_min, agent1_attention_min
[-46.821  -1.624]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -77.09195717175177, time: 313.993
agent0_energy_min, agent0_attention_min
[-49.051  -0.308]
agent1_energy_min, agent1_attention_min
[-47.352  -1.912]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -82.11228798457546, time: 312.356
agent0_energy_min, agent0_attention_min
[-4.8892e+01 -1.3000e-02]
agent1_energy_min, agent1_attention_min
[-47.391  -1.805]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -83.43693512233854, time: 311.264
agent0_energy_min, agent0_attention_min
[-4.9209e+01 -1.5000e-02]
agent1_energy_min, agent1_attention_min
[-46.204  -3.104]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -84.63397344838847, time: 313.787
agent0_energy_min, agent0_attention_min
[-4.9055e+01 -1.8000e-02]
agent1_energy_min, agent1_attention_min
[-46.87   -1.734]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -82.67691665801667, time: 314.241
agent0_energy_min, agent0_attention_min
[-4.9334e+01 -1.8000e-02]
agent1_energy_min, agent1_attention_min
[-46.441  -2.615]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -76.06768002933944, time: 314.753
agent0_energy_min, agent0_attention_min
[-4.9167e+01 -1.7000e-02]
agent1_energy_min, agent1_attention_min
[-47.314  -1.718]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -75.9210325866602, time: 309.377
agent0_energy_min, agent0_attention_min
[-4.895e+01 -2.000e-02]
agent1_energy_min, agent1_attention_min
[-47.628  -1.651]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -73.52247751936758, time: 308.807
agent0_energy_min, agent0_attention_min
[-4.8962e+01 -1.1000e-02]
agent1_energy_min, agent1_attention_min
[-48.003  -1.305]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -77.92607128653823, time: 306.561
agent0_energy_min, agent0_attention_min
[-4.8569e+01 -1.1000e-02]
agent1_energy_min, agent1_attention_min
[-47.633  -1.945]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -91.31171743666668, time: 304.924
agent0_energy_min, agent0_attention_min
[-4.8254e+01 -7.0000e-03]
agent1_energy_min, agent1_attention_min
[-46.708  -2.687]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -75.47590183863483, time: 303.208
agent0_energy_min, agent0_attention_min
[-4.7951e+01 -1.0000e-02]
agent1_energy_min, agent1_attention_min
[-47.633  -1.555]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -72.46663873851605, time: 300.809
agent0_energy_min, agent0_attention_min
[-4.7869e+01 -9.0000e-03]
agent1_energy_min, agent1_attention_min
[-47.705  -1.139]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -75.32116513802086, time: 299.901
agent0_energy_min, agent0_attention_min
[-4.6583e+01 -1.8000e-02]
agent1_energy_min, agent1_attention_min
[-47.297  -1.338]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -133.94385533796273, time: 300.752
agent0_energy_min, agent0_attention_min
[-42.053  -0.045]
agent1_energy_min, agent1_attention_min
[-42.711  -3.513]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -83.58490587839063, time: 299.685
agent0_energy_min, agent0_attention_min
[-47.369  -0.439]
agent1_energy_min, agent1_attention_min
[-39.125  -3.714]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -79.08218288192735, time: 303.575
agent0_energy_min, agent0_attention_min
[-43.924  -2.224]
agent1_energy_min, agent1_attention_min
[-48.335  -0.673]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -108.22718690338179, time: 301.458
agent0_energy_min, agent0_attention_min
[-4.821e+01 -1.300e-02]
agent1_energy_min, agent1_attention_min
[-44.115  -4.16 ]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -127.38521487768519, time: 302.251
agent0_energy_min, agent0_attention_min
[-4.3665e+01 -2.1000e-02]
agent1_energy_min, agent1_attention_min
[-46.473  -2.156]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -74.88055134537976, time: 304.419
agent0_energy_min, agent0_attention_min
[-47.902  -0.09 ]
agent1_energy_min, agent1_attention_min
[-47.544  -1.406]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -77.52003887224188, time: 303.867
agent0_energy_min, agent0_attention_min
[-47.088  -2.184]
agent1_energy_min, agent1_attention_min
[-47.756  -0.91 ]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -80.71736630824196, time: 304.547
agent0_energy_min, agent0_attention_min
[-48.816  -0.928]
agent1_energy_min, agent1_attention_min
[-41.462  -4.917]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -83.90867808558633, time: 306.234
agent0_energy_min, agent0_attention_min
[-46.563  -3.163]
agent1_energy_min, agent1_attention_min
[-39.085  -4.415]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -117.70350407517833, time: 303.604
agent0_energy_min, agent0_attention_min
[-40.71   -0.423]
agent1_energy_min, agent1_attention_min
[-38.752  -3.192]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -77.2919695820019, time: 305.395
agent0_energy_min, agent0_attention_min
[-28.716  -0.111]
agent1_energy_min, agent1_attention_min
[-37.735  -3.257]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -62.359029387620964, time: 306.027
agent0_energy_min, agent0_attention_min
[-29.098  -0.06 ]
agent1_energy_min, agent1_attention_min
[-41.985  -0.801]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -84.250701960593, time: 305.89
agent0_energy_min, agent0_attention_min
[-22.2    -0.037]
agent1_energy_min, agent1_attention_min
[-41.306  -2.417]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -75.14478322964193, time: 304.027
agent0_energy_min, agent0_attention_min
[-29.813  -0.177]
agent1_energy_min, agent1_attention_min
[-41.324  -5.926]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -73.57237081472385, time: 305.965
agent0_energy_min, agent0_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-6__2018-07-15_11-12-47...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -181.72401541064085, time: 219.915
agent0_energy_min, agent0_attention_min
[-18.15715716 -16.60560561]
agent1_energy_min, agent1_attention_min
[-17.57757758 -16.34034034]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -322.844506233615, time: 314.38
agent0_energy_min, agent0_attention_min
[-14.564 -16.977]
agent1_energy_min, agent1_attention_min
[-21.71  -8.57]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -155.7617275793862, time: 317.086
agent0_energy_min, agent0_attention_min
[-11.275 -34.897]
agent1_energy_min, agent1_attention_min
[-32.082  -3.281]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -131.52373237445738, time: 314.716
agent0_energy_min, agent0_attention_min
[-26.231 -22.882]
agent1_energy_min, agent1_attention_min
[-44.038  -0.479]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -115.82594655368608, time: 310.827
agent0_energy_min, agent0_attention_min
[-45.157  -3.938]
agent1_energy_min, agent1_attention_min
[-43.688  -0.232]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -97.01257413899674, time: 314.268
agent0_energy_min, agent0_attention_min
[-48.676  -0.48 ]
agent1_energy_min, agent1_attention_min
[-43.868  -0.128]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -104.41482083173645, time: 316.176
agent0_energy_min, agent0_attention_min
[-49.126  -0.084]
agent1_energy_min, agent1_attention_min
[-42.167  -0.184]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -80.66124074442419, time: 314.532
agent0_energy_min, agent0_attention_min
[-4.9514e+01 -1.8000e-02]
agent1_energy_min, agent1_attention_min
[-40.775  -0.075]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -80.99358238588792, time: 310.293
agent0_energy_min, agent0_attention_min
[-4.9438e+01 -7.0000e-03]
agent1_energy_min, agent1_attention_min
[-41.061  -0.065]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -86.43443606355703, time: 309.686
agent0_energy_min, agent0_attention_min
[-4.977e+01 -1.500e-02]
agent1_energy_min, agent1_attention_min
[-42.287  -0.11 ]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -77.25221609556645, time: 312.798
agent0_energy_min, agent0_attention_min
[-4.9482e+01 -1.9000e-02]
agent1_energy_min, agent1_attention_min
[-42.717  -0.061]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -72.86745993588187, time: 311.89
agent0_energy_min, agent0_attention_min
[-4.9837e+01 -1.0000e-02]
agent1_energy_min, agent1_attention_min
[-43.947  -0.071]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -87.78677366889544, time: 309.175
agent0_energy_min, agent0_attention_min
[-4.9888e+01 -1.0000e-02]
agent1_energy_min, agent1_attention_min
[-42.627  -0.051]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -81.93526350220033, time: 313.606
agent0_energy_min, agent0_attention_min
[-4.6829e+01 -9.0000e-03]
agent1_energy_min, agent1_attention_min
[-43.248  -0.269]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -74.93637815458175, time: 313.672
agent0_energy_min, agent0_attention_min
[-4.7114e+01 -1.1000e-02]
agent1_energy_min, agent1_attention_min
[-43.371  -0.299]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -71.15746058278275, time: 311.49
agent0_energy_min, agent0_attention_min
[-44.031  -0.162]
agent1_energy_min, agent1_attention_min
[-44.848  -0.266]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -73.44498583916584, time: 312.147
agent0_energy_min, agent0_attention_min
[-45.338  -0.465]
agent1_energy_min, agent1_attention_min
[-43.822  -0.379]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -89.85926634897837, time: 312.775
agent0_energy_min, agent0_attention_min
[-44.768  -0.268]
agent1_energy_min, agent1_attention_min
[-43.625  -0.39 ]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -79.09721567860545, time: 313.223
agent0_energy_min, agent0_attention_min
[-44.351  -0.903]
agent1_energy_min, agent1_attention_min
[-41.669  -3.107]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -72.12513907737346, time: 314.643
agent0_energy_min, agent0_attention_min
[-39.327  -2.575]
agent1_energy_min, agent1_attention_min
[-43.931  -0.99 ]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -76.3054356992209, time: 313.277
agent0_energy_min, agent0_attention_min
[-37.96   -0.516]
agent1_energy_min, agent1_attention_min
[-43.899  -0.438]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -85.7527090454516, time: 309.334
agent0_energy_min, agent0_attention_min
[-38.963  -0.461]
agent1_energy_min, agent1_attention_min
[-43.677  -1.886]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -155.3264651683736, time: 306.763
agent0_energy_min, agent0_attention_min
[-37.117  -3.13 ]
agent1_energy_min, agent1_attention_min
[-36.507  -0.263]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -104.48481860523219, time: 300.624
agent0_energy_min, agent0_attention_min
[-36.467  -3.15 ]
agent1_energy_min, agent1_attention_min
[-42.567  -1.797]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -78.48865576067874, time: 299.614
agent0_energy_min, agent0_attention_min
[-30.904  -3.692]
agent1_energy_min, agent1_attention_min
[-44.373  -1.706]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -84.1478390986734, time: 302.101
agent0_energy_min, agent0_attention_min
[-30.58   -2.115]
agent1_energy_min, agent1_attention_min
[-44.779  -0.747]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -82.95802409061409, time: 299.373
agent0_energy_min, agent0_attention_min
[-26.432  -1.296]
agent1_energy_min, agent1_attention_min
[-47.461  -0.595]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -86.15276926325748, time: 300.801
agent0_energy_min, agent0_attention_min
[-30.951  -0.475]
agent1_energy_min, agent1_attention_min
[-46.516  -1.454]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -96.53585199312025, time: 300.667
agent0_energy_min, agent0_attention_min
[-24.533  -2.646]
agent1_energy_min, agent1_attention_min
[-44.588  -1.163]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -70.41806778349934, time: 304.097
agent0_energy_min, agent0_attention_min
[-26.718  -0.146]
agent1_energy_min, agent1_attention_min
[-43.511  -0.848]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -87.54444049710136, time: 304.964
agent0_energy_min, agent0_attention_min
[-26.447  -0.089]
agent1_energy_min, agent1_attention_min
[-41.656  -1.804]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -84.75605250421506, time: 302.891
agent0_energy_min, agent0_attention_min
[-23.355  -0.036]
agent1_energy_min, agent1_attention_min
[-43.329  -2.632]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -77.5793230743749, time: 303.284
agent0_energy_min, agent0_attention_min
[-23.783  -0.047]
agent1_energy_min, agent1_attention_min
[-35.703  -9.617]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -86.87734619014212, time: 300.913
agent0_energy_min, agent0_attention_min
[-23.529  -0.681]
agent1_energy_min, agent1_attention_min
[-35.921  -9.045]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -80.07658495915331, time: 301.891
agent0_energy_min, agent0_attention_min
[-23.748  -0.797]
agent1_energy_min, agent1_attention_min
[-33.742  -4.849]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -73.54923407881607, time: 302.827
agent0_energy_min, agent0_attention_min
[-20.806  -0.911]
agent1_energy_min, agent1_attention_min
[-38.119  -5.178]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -79.04489825017346, time: 302.451
agent0_energy_min, agent0_attention_min
[-19.812  -0.771]
agent1_energy_min, agent1_attention_min
[-35.936  -3.387]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -68.71389529735949, time: 303.694
agent0_energy_min, agent0_attention_min
[-20.454  -0.086]
agent1_energy_min, agent1_attention_minUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-4__2018-07-15_11-12-33...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -168.17780438406433, time: 221.852
agent0_energy_min, agent0_attention_min
[-15.44344344 -17.62062062]
agent1_energy_min, agent1_attention_min
[-14.97697698 -15.95495495]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -351.60061813021866, time: 321.312
agent0_energy_min, agent0_attention_min
[-15.787 -19.9  ]
agent1_energy_min, agent1_attention_min
[-5.938 -6.689]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -144.55038169718884, time: 323.159
agent0_energy_min, agent0_attention_min
[ -7.425 -36.925]
agent1_energy_min, agent1_attention_min
[-0.047 -0.012]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -137.6673245706275, time: 310.958
agent0_energy_min, agent0_attention_min
[ -6.874 -35.89 ]
agent1_energy_min, agent1_attention_min
[-0.09  -0.015]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -134.39562814258068, time: 303.47
agent0_energy_min, agent0_attention_min
[-22.364 -23.41 ]
agent1_energy_min, agent1_attention_min
[-11.585  -0.186]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -125.24903159829343, time: 308.312
agent0_energy_min, agent0_attention_min
[-45.722  -1.185]
agent1_energy_min, agent1_attention_min
[-4.172 -0.032]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -117.13422974914218, time: 310.1
agent0_energy_min, agent0_attention_min
[-47.205  -0.934]
agent1_energy_min, agent1_attention_min
[-4.024 -0.241]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -119.37240922673824, time: 309.854
agent0_energy_min, agent0_attention_min
[-47.285  -1.521]
agent1_energy_min, agent1_attention_min
[-5.618 -0.709]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -108.60121658657292, time: 307.556
agent0_energy_min, agent0_attention_min
[-47.481  -0.678]
agent1_energy_min, agent1_attention_min
[-19.812  -0.628]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -108.4173785372542, time: 305.705
agent0_energy_min, agent0_attention_min
[-47.166  -0.399]
agent1_energy_min, agent1_attention_min
[-14.634  -0.462]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -109.46967746600474, time: 306.515
agent0_energy_min, agent0_attention_min
[-45.428  -0.262]
agent1_energy_min, agent1_attention_min
[-34.828  -0.086]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -133.21164583558937, time: 305.423
agent0_energy_min, agent0_attention_min
[-44.104  -0.325]
agent1_energy_min, agent1_attention_min
[-41.202  -0.07 ]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -89.05021666798133, time: 305.724
agent0_energy_min, agent0_attention_min
[-45.905  -0.249]
agent1_energy_min, agent1_attention_min
[-45.535  -0.304]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -94.90159783571397, time: 309.222
agent0_energy_min, agent0_attention_min
[-45.951  -0.31 ]
agent1_energy_min, agent1_attention_min
[-45.355  -0.226]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -79.98832922386283, time: 309.993
agent0_energy_min, agent0_attention_min
[-47.347  -0.194]
agent1_energy_min, agent1_attention_min
[-45.111  -0.083]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -75.04721614252365, time: 309.54
agent0_energy_min, agent0_attention_min
[-47.345  -0.105]
agent1_energy_min, agent1_attention_min
[-46.701  -0.238]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -76.94907350882464, time: 309.302
agent0_energy_min, agent0_attention_min
[-46.211  -0.149]
agent1_energy_min, agent1_attention_min
[-46.118  -0.315]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -70.53591093321744, time: 309.245
agent0_energy_min, agent0_attention_min
[-44.345  -0.257]
agent1_energy_min, agent1_attention_min
[-4.7081e+01 -2.9000e-02]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -73.98874053349101, time: 309.262
agent0_energy_min, agent0_attention_min
[-41.029  -0.704]
agent1_energy_min, agent1_attention_min
[-46.956  -0.111]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -69.33513227177079, time: 308.979
agent0_energy_min, agent0_attention_min
[-38.61   -1.885]
agent1_energy_min, agent1_attention_min
[-4.7432e+01 -1.0000e-02]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -77.94879089733757, time: 307.492
agent0_energy_min, agent0_attention_min
[-40.212  -1.633]
agent1_energy_min, agent1_attention_min
[-4.83e+01 -9.00e-03]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -80.03326086954135, time: 308.23
agent0_energy_min, agent0_attention_min
[-37.155  -2.362]
agent1_energy_min, agent1_attention_min
[-4.2261e+01 -1.2000e-02]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -81.95468334536967, time: 308.3
agent0_energy_min, agent0_attention_min
[-36.226  -2.991]
agent1_energy_min, agent1_attention_min
[-4.5159e+01 -1.9000e-02]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -76.8585382036347, time: 308.197
agent0_energy_min, agent0_attention_min
[-32.439  -6.509]
agent1_energy_min, agent1_attention_min
[-45.184  -0.052]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -73.73690279812966, time: 307.342
agent0_energy_min, agent0_attention_min
[-32.1    -3.723]
agent1_energy_min, agent1_attention_min
[-40.27   -0.905]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -78.04744438025885, time: 307.595
agent0_energy_min, agent0_attention_min
[-34.118  -2.146]
agent1_energy_min, agent1_attention_min
[-38.359  -0.089]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -69.49290675047669, time: 307.405
agent0_energy_min, agent0_attention_min
[-33.23   -6.307]
agent1_energy_min, agent1_attention_min
[-34.625  -0.274]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -66.08163773747053, time: 310.07
agent0_energy_min, agent0_attention_min
[-33.94   -3.614]
agent1_energy_min, agent1_attention_min
[-23.672  -0.434]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -66.68958105749492, time: 311.996
agent0_energy_min, agent0_attention_min
[-35.39   -2.373]
agent1_energy_min, agent1_attention_min
[-26.704  -0.793]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -72.53855817674025, time: 310.64
agent0_energy_min, agent0_attention_min
[-36.501  -4.466]
agent1_energy_min, agent1_attention_min
[-32.033  -0.763]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -90.88277696306858, time: 315.298
agent0_energy_min, agent0_attention_min
[-35.314  -6.467]
agent1_energy_min, agent1_attention_min
[-23.968  -4.212]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -74.42601457045618, time: 311.869
agent0_energy_min, agent0_attention_min
[-32.571  -3.004]
agent1_energy_min, agent1_attention_min
[-26.429  -0.567]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -75.98266175903498, time: 310.494
agent0_energy_min, agent0_attention_min
[-30.743  -3.744]
agent1_energy_min, agent1_attention_min
[-21.323  -0.312]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -70.71334480470898, time: 313.348
agent0_energy_min, agent0_attention_min
[-27.065  -7.046]
agent1_energy_min, agent1_attention_min
[-20.658  -0.494]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -81.70061672772138, time: 309.387
agent0_energy_min, agent0_attention_min
[-31.746  -3.135]
agent1_energy_min, agent1_attention_min
[-22.995  -0.1  ]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -84.26147580414298, time: 308.147
agent0_energy_min, agent0_attention_min
[-28.37   -1.836]
agent1_energy_min, agent1_attention_min
[-21.118  -0.985]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -66.72628113918428, time: 307.747
agent0_energy_min, agent0_attention_min
[-24.285  -1.474]
agent1_energy_min, agent1_attention_min
[-16.856  -1.96 ]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -78.97655019551375, time: 311.144
agent0_energy_min, agent0_attention_min
[-25.259  -3.572]
agent1_energy_min, agent1_attention_min
[-18.93   -1.432]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-4__2018-07-15_11-12-34...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -174.77238539416132, time: 218.068
agent0_energy_min, agent0_attention_min
[-17.67567568 -16.29329329]
agent1_energy_min, agent1_attention_min
[-15.96696697 -17.18818819]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -185.94869979239147, time: 314.162
agent0_energy_min, agent0_attention_min
[-22.958  -3.671]
agent1_energy_min, agent1_attention_min
[-11.934 -14.846]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -105.33610466628886, time: 320.614
agent0_energy_min, agent0_attention_min
[-24.549  -0.107]
agent1_energy_min, agent1_attention_min
[ -1.845 -31.103]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -103.33685445561098, time: 315.475
agent0_energy_min, agent0_attention_min
[-22.627  -0.047]
agent1_energy_min, agent1_attention_min
[ -9.337 -26.443]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -92.92482356985678, time: 310.206
agent0_energy_min, agent0_attention_min
[-27.961  -0.044]
agent1_energy_min, agent1_attention_min
[-16.146 -22.878]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -86.32228778199904, time: 302.541
agent0_energy_min, agent0_attention_min
[-23.51   -0.076]
agent1_energy_min, agent1_attention_min
[-21.    -20.708]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -77.78537265455083, time: 302.622
agent0_energy_min, agent0_attention_min
[-26.441  -0.283]
agent1_energy_min, agent1_attention_min
[-28.812 -15.622]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -85.65977920974781, time: 302.623
agent0_energy_min, agent0_attention_min
[-28.224  -0.161]
agent1_energy_min, agent1_attention_min
[-30.266 -16.98 ]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -80.42163950543085, time: 303.036
agent0_energy_min, agent0_attention_min
[-29.064  -0.205]
agent1_energy_min, agent1_attention_min
[-37.095 -11.612]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -71.6727085047952, time: 298.92
agent0_energy_min, agent0_attention_min
[-24.355  -0.065]
agent1_energy_min, agent1_attention_min
[-35.816 -12.584]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -67.71517787547612, time: 302.337
agent0_energy_min, agent0_attention_min
[-21.521  -0.036]
agent1_energy_min, agent1_attention_min
[-36.111 -12.017]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -74.43458323378869, time: 303.714
agent0_energy_min, agent0_attention_min
[-23.979  -0.027]
agent1_energy_min, agent1_attention_min
[-39.671  -7.398]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -73.79277082786005, time: 301.689
agent0_energy_min, agent0_attention_min
[-21.998  -0.035]
agent1_energy_min, agent1_attention_min
[-41.912  -6.381]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -72.96510706802592, time: 304.83
agent0_energy_min, agent0_attention_min
[-24.33   -0.137]
agent1_energy_min, agent1_attention_min
[-44.975  -3.725]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -71.82518586861453, time: 305.052
agent0_energy_min, agent0_attention_min
[-20.704  -0.036]
agent1_energy_min, agent1_attention_min
[-42.128  -6.514]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -73.50711817387248, time: 304.046
agent0_energy_min, agent0_attention_min
[-21.588  -0.055]
agent1_energy_min, agent1_attention_min
[-44.658  -4.563]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -66.99599564077927, time: 303.473
agent0_energy_min, agent0_attention_min
[-19.223  -0.137]
agent1_energy_min, agent1_attention_min
[-41.079  -7.956]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -69.86371463218491, time: 304.266
agent0_energy_min, agent0_attention_min
[-15.474  -1.209]
agent1_energy_min, agent1_attention_min
[-41.965  -6.605]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -85.3160369601314, time: 306.415
agent0_energy_min, agent0_attention_min
[-15.61   -0.091]
agent1_energy_min, agent1_attention_min
[-42.306  -6.819]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -97.02819403605132, time: 307.127
agent0_energy_min, agent0_attention_min
[-14.704  -0.095]
agent1_energy_min, agent1_attention_min
[-38.728  -9.556]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -74.43682727181813, time: 303.744
agent0_energy_min, agent0_attention_min
[-1.7147e+01 -1.7000e-02]
agent1_energy_min, agent1_attention_min
[-36.534 -13.065]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -65.59597283468541, time: 305.559
agent0_energy_min, agent0_attention_min
[-15.747  -0.054]
agent1_energy_min, agent1_attention_min
[-32.894 -16.608]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -80.11874075617911, time: 304.627
agent0_energy_min, agent0_attention_min
[-17.797  -0.034]
agent1_energy_min, agent1_attention_min
[-31.897 -15.826]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -61.69719477421927, time: 304.615
agent0_energy_min, agent0_attention_min
[-16.549  -0.518]
agent1_energy_min, agent1_attention_min
[-26.976 -22.714]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -61.320267281148524, time: 304.211
agent0_energy_min, agent0_attention_min
[-17.696  -1.343]
agent1_energy_min, agent1_attention_min
[-11.462 -38.518]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -59.943863815093636, time: 306.11
agent0_energy_min, agent0_attention_min
[-18.869  -1.528]
agent1_energy_min, agent1_attention_min
[-12.265 -37.639]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -81.32749640838503, time: 305.632
agent0_energy_min, agent0_attention_min
[-19.079  -0.468]
agent1_energy_min, agent1_attention_min
[-10.694 -39.273]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -77.75507078817868, time: 306.501
agent0_energy_min, agent0_attention_min
[-18.186  -1.85 ]
agent1_energy_min, agent1_attention_min
[-12.605 -37.363]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -73.09789792592181, time: 307.759
agent0_energy_min, agent0_attention_min
[-18.388  -0.701]
agent1_energy_min, agent1_attention_min
[-17.166 -32.712]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -64.10454314257062, time: 310.649
agent0_energy_min, agent0_attention_min
[-13.799  -7.993]
agent1_energy_min, agent1_attention_min
[-16.584 -33.376]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -80.22686458515068, time: 309.764
agent0_energy_min, agent0_attention_min
[-15.784  -5.758]
agent1_energy_min, agent1_attention_min
[-13.528 -36.277]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -70.31754414775882, time: 310.047
agent0_energy_min, agent0_attention_min
[-16.755  -4.812]
agent1_energy_min, agent1_attention_min
[-12.908 -35.96 ]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -69.21264656593, time: 310.352
agent0_energy_min, agent0_attention_min
[-15.362  -6.06 ]
agent1_energy_min, agent1_attention_min
[-14.211 -35.765]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -73.94525820500542, time: 307.092
agent0_energy_min, agent0_attention_min
[-19.843  -2.69 ]
agent1_energy_min, agent1_attention_min
[-16.536 -33.429]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -76.88385296744978, time: 308.721
agent0_energy_min, agent0_attention_min
[-22.823  -2.576]
agent1_energy_min, agent1_attention_min
[-11.702 -38.277]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -76.33671348828251, time: 308.331
agent0_energy_min, agent0_attention_min
[-19.098  -4.591]
agent1_energy_min, agent1_attention_min
[ -8.98  -41.001]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -76.9709648727937, time: 305.929
agent0_energy_min, agent0_attention_min
[-20.817  -1.356]
agent1_energy_min, agent1_attention_min
[ -8.782 -41.188]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -102.6341372194362, time: 308.331
agent0_energy_min, agent0_attention_min
[-23.425  -3.102]
agent1_energy_min, agent1_attention_min
[-10.881 -39.097]
39000Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-3__2018-07-15_11-12-30...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -163.7348404111833, time: 216.694
agent0_energy_min, agent0_attention_min
[-16.33633634 -16.55555556]
agent1_energy_min, agent1_attention_min
[-15.12112112 -18.52352352]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -297.1751847818331, time: 313.219
agent0_energy_min, agent0_attention_min
[-13.112 -24.573]
agent1_energy_min, agent1_attention_min
[ -8.106 -16.974]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -134.05528571900618, time: 314.058
agent0_energy_min, agent0_attention_min
[-10.457 -37.942]
agent1_energy_min, agent1_attention_min
[ -0.457 -43.562]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -118.4687419798984, time: 305.324
agent0_energy_min, agent0_attention_min
[ -5.144 -44.044]
agent1_energy_min, agent1_attention_min
[ -0.101 -48.106]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -116.31988879171709, time: 302.639
agent0_energy_min, agent0_attention_min
[ -8.315 -40.903]
agent1_energy_min, agent1_attention_min
[ -0.058 -48.607]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -114.6479811086182, time: 304.69
agent0_energy_min, agent0_attention_min
[-12.788 -36.92 ]
agent1_energy_min, agent1_attention_min
[ -0.144 -46.399]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -114.00355031152102, time: 306.338
agent0_energy_min, agent0_attention_min
[-16.247 -33.69 ]
agent1_energy_min, agent1_attention_min
[ -0.396 -42.573]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -100.90339156134725, time: 308.425
agent0_energy_min, agent0_attention_min
[-43.198  -6.375]
agent1_energy_min, agent1_attention_min
[ -1.129 -43.258]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -90.73113466769753, time: 306.503
agent0_energy_min, agent0_attention_min
[-48.45   -0.806]
agent1_energy_min, agent1_attention_min
[ -2.845 -43.367]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -85.40952865075981, time: 302.146
agent0_energy_min, agent0_attention_min
[-49.093  -0.41 ]
agent1_energy_min, agent1_attention_min
[ -6.813 -41.592]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -81.14729762046971, time: 305.323
agent0_energy_min, agent0_attention_min
[-48.747  -0.335]
agent1_energy_min, agent1_attention_min
[ -6.783 -42.706]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -89.91696243589168, time: 306.002
agent0_energy_min, agent0_attention_min
[-48.434  -0.39 ]
agent1_energy_min, agent1_attention_min
[ -7.734 -42.032]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -82.5018484118789, time: 304.69
agent0_energy_min, agent0_attention_min
[-48.861  -0.095]
agent1_energy_min, agent1_attention_min
[ -9.479 -40.327]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -83.19704380577946, time: 305.53
agent0_energy_min, agent0_attention_min
[-4.9395e+01 -4.7000e-02]
agent1_energy_min, agent1_attention_min
[ -8.798 -41.052]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -94.84115811906098, time: 310.08
agent0_energy_min, agent0_attention_min
[-49.395  -0.291]
agent1_energy_min, agent1_attention_min
[ -8.33  -41.055]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -78.54061933170352, time: 306.079
agent0_energy_min, agent0_attention_min
[-49.187  -0.192]
agent1_energy_min, agent1_attention_min
[ -8.313 -40.647]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -72.90354753891653, time: 303.813
agent0_energy_min, agent0_attention_min
[-47.284  -1.096]
agent1_energy_min, agent1_attention_min
[ -8.212 -41.165]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -75.15469911315307, time: 305.727
agent0_energy_min, agent0_attention_min
[-45.754  -0.707]
agent1_energy_min, agent1_attention_min
[ -7.408 -41.63 ]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -76.1577640219852, time: 306.005
agent0_energy_min, agent0_attention_min
[-45.088  -1.363]
agent1_energy_min, agent1_attention_min
[ -7.331 -41.84 ]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -72.18768758835658, time: 305.601
agent0_energy_min, agent0_attention_min
[-45.994  -2.404]
agent1_energy_min, agent1_attention_min
[ -7.094 -42.2  ]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -73.91971822323467, time: 306.771
agent0_energy_min, agent0_attention_min
[-48.087  -1.035]
agent1_energy_min, agent1_attention_min
[ -7.446 -41.855]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -75.89897555850918, time: 306.336
agent0_energy_min, agent0_attention_min
[-44.794  -1.859]
agent1_energy_min, agent1_attention_min
[ -8.09  -41.484]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -71.89046511725965, time: 303.517
agent0_energy_min, agent0_attention_min
[-44.321  -1.966]
agent1_energy_min, agent1_attention_min
[ -7.615 -41.979]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -83.7621110031384, time: 305.986
agent0_energy_min, agent0_attention_min
[-45.125  -2.466]
agent1_energy_min, agent1_attention_min
[-11.604 -38.136]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -78.83464370148323, time: 306.523
agent0_energy_min, agent0_attention_min
[-40.168  -6.937]
agent1_energy_min, agent1_attention_min
[-11.053 -38.331]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -78.59019497668706, time: 308.114
agent0_energy_min, agent0_attention_min
[-38.862  -5.962]
agent1_energy_min, agent1_attention_min
[ -8.531 -41.222]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -74.75296526183895, time: 303.635
agent0_energy_min, agent0_attention_min
[-47.004  -2.141]
agent1_energy_min, agent1_attention_min
[-12.436 -37.222]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -80.58595688872133, time: 306.411
agent0_energy_min, agent0_attention_min
[-46.882  -2.41 ]
agent1_energy_min, agent1_attention_min
[-10.996 -37.528]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -67.1028758416031, time: 307.644
agent0_energy_min, agent0_attention_min
[-48.605  -0.143]
agent1_energy_min, agent1_attention_min
[ -9.036 -39.99 ]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -68.93841106681872, time: 312.014
agent0_energy_min, agent0_attention_min
[-46.137  -1.961]
agent1_energy_min, agent1_attention_min
[ -8.873 -41.021]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -74.18716596305245, time: 311.729
agent0_energy_min, agent0_attention_min
[-47.624  -0.707]
agent1_energy_min, agent1_attention_min
[-11.576 -38.106]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -70.78712847011357, time: 309.379
agent0_energy_min, agent0_attention_min
[-46.714  -2.011]
agent1_energy_min, agent1_attention_min
[ -7.648 -42.049]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -83.46902474167452, time: 307.399
agent0_energy_min, agent0_attention_min
[-47.489  -1.886]
agent1_energy_min, agent1_attention_min
[ -9.418 -40.509]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -63.06240545359035, time: 308.188
agent0_energy_min, agent0_attention_min
[-45.724  -3.549]
agent1_energy_min, agent1_attention_min
[ -6.932 -42.742]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -61.51054384478257, time: 307.227
agent0_energy_min, agent0_attention_min
[-46.15   -1.681]
agent1_energy_min, agent1_attention_min
[ -7.557 -41.294]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -75.34818577937631, time: 306.095
agent0_energy_min, agent0_attention_min
[-48.916  -0.662]
agent1_energy_min, agent1_attention_min
[ -7.701 -41.851]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -62.125564335057526, time: 306.255
agent0_energy_min, agent0_attention_min
[-45.274  -1.008]
agent1_energy_min, agent1_attention_min
[ -7.507 -42.252]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -61.014353295131905, time: 306.833
agent0_energy_min, agent0_attention_min
[-42.127  -0.766]
agent1_energy_min, agent1_attention_min
[ -8.372 -40.726]
39000Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-5__2018-07-15_11-12-43...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -179.24931263938663, time: 221.949
agent0_energy_min, agent0_attention_min
[-15.71271271 -18.02502503]
agent1_energy_min, agent1_attention_min
[-16.76976977 -16.10710711]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -506.132721774106, time: 316.945
agent0_energy_min, agent0_attention_min
[ -8.249 -16.493]
agent1_energy_min, agent1_attention_min
[-14.232 -12.737]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -140.3136123068771, time: 317.775
agent0_energy_min, agent0_attention_min
[-16.183  -6.059]
agent1_energy_min, agent1_attention_min
[ -0.812 -31.072]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -149.3525656367617, time: 316.881
agent0_energy_min, agent0_attention_min
[-11.134 -24.897]
agent1_energy_min, agent1_attention_min
[ -3.745 -34.164]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -114.80637354514012, time: 313.584
agent0_energy_min, agent0_attention_min
[-26.684  -6.344]
agent1_energy_min, agent1_attention_min
[-34.291 -11.059]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -112.59475640746842, time: 316.088
agent0_energy_min, agent0_attention_min
[-22.51   -1.663]
agent1_energy_min, agent1_attention_min
[-48.091  -1.405]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -105.78453200929484, time: 317.06
agent0_energy_min, agent0_attention_min
[-32.951  -4.043]
agent1_energy_min, agent1_attention_min
[-48.233  -1.11 ]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -99.60015146837029, time: 315.178
agent0_energy_min, agent0_attention_min
[-28.39  -15.575]
agent1_energy_min, agent1_attention_min
[-48.794  -0.377]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -98.47074489089545, time: 313.118
agent0_energy_min, agent0_attention_min
[-31.621 -13.661]
agent1_energy_min, agent1_attention_min
[-48.283  -0.448]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -95.7876011691812, time: 311.485
agent0_energy_min, agent0_attention_min
[-36.333  -8.413]
agent1_energy_min, agent1_attention_min
[-48.36   -1.162]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -97.74615087402626, time: 315.286
agent0_energy_min, agent0_attention_min
[-37.704  -4.613]
agent1_energy_min, agent1_attention_min
[-47.167  -1.487]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -96.15050828310362, time: 314.525
agent0_energy_min, agent0_attention_min
[-33.153  -4.417]
agent1_energy_min, agent1_attention_min
[-46.931  -1.661]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -95.10520834283791, time: 312.881
agent0_energy_min, agent0_attention_min
[-35.122  -3.296]
agent1_energy_min, agent1_attention_min
[-47.259  -0.865]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -95.41620348546282, time: 310.091
agent0_energy_min, agent0_attention_min
[-35.513  -2.52 ]
agent1_energy_min, agent1_attention_min
[-44.83   -2.973]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -96.68236615788474, time: 308.804
agent0_energy_min, agent0_attention_min
[-36.403  -2.67 ]
agent1_energy_min, agent1_attention_min
[-48.398  -0.825]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -84.65733847431967, time: 305.876
agent0_energy_min, agent0_attention_min
[-39.09   -1.144]
agent1_energy_min, agent1_attention_min
[-47.875  -1.511]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -93.96328209350406, time: 296.937
agent0_energy_min, agent0_attention_min
[-39.775  -1.493]
agent1_energy_min, agent1_attention_min
[-43.791  -5.869]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -103.96598602965317, time: 303.294
agent0_energy_min, agent0_attention_min
[-44.228  -2.448]
agent1_energy_min, agent1_attention_min
[-46.89   -1.805]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -94.34459401617409, time: 302.927
agent0_energy_min, agent0_attention_min
[-43.307  -1.131]
agent1_energy_min, agent1_attention_min
[-46.017  -2.01 ]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -81.83061630779606, time: 301.429
agent0_energy_min, agent0_attention_min
[-42.509  -1.116]
agent1_energy_min, agent1_attention_min
[-47.472  -1.196]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -80.96765068832673, time: 299.478
agent0_energy_min, agent0_attention_min
[-43.453  -0.887]
agent1_energy_min, agent1_attention_min
[-47.223  -0.886]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -79.9234350704026, time: 300.828
agent0_energy_min, agent0_attention_min
[-43.117  -0.997]
agent1_energy_min, agent1_attention_min
[-48.08   -1.299]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -107.60569391579146, time: 299.647
agent0_energy_min, agent0_attention_min
[-42.562  -0.503]
agent1_energy_min, agent1_attention_min
[-35.631  -5.871]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -109.80492619697958, time: 300.495
agent0_energy_min, agent0_attention_min
[-38.912  -0.369]
agent1_energy_min, agent1_attention_min
[-32.58   -3.362]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -69.60106723012912, time: 299.281
agent0_energy_min, agent0_attention_min
[-33.739  -0.397]
agent1_energy_min, agent1_attention_min
[-36.381  -1.36 ]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -75.0964679343393, time: 303.046
agent0_energy_min, agent0_attention_min
[-31.919  -0.467]
agent1_energy_min, agent1_attention_min
[-36.27   -2.539]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -65.75947572688928, time: 303.048
agent0_energy_min, agent0_attention_min
[-23.11   -0.585]
agent1_energy_min, agent1_attention_min
[-33.822  -2.327]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -70.30947434079894, time: 301.574
agent0_energy_min, agent0_attention_min
[-18.548  -3.11 ]
agent1_energy_min, agent1_attention_min
[-33.231  -1.132]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -69.20305490784554, time: 305.286
agent0_energy_min, agent0_attention_min
[-18.736  -0.469]
agent1_energy_min, agent1_attention_min
[-34.989  -0.252]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -65.29153538910012, time: 304.782
agent0_energy_min, agent0_attention_min
[-16.973  -0.795]
agent1_energy_min, agent1_attention_min
[-34.259  -0.961]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -65.17149422633595, time: 308.139
agent0_energy_min, agent0_attention_min
[-17.235  -0.461]
agent1_energy_min, agent1_attention_min
[-31.735  -2.023]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -70.40446987437599, time: 305.631
agent0_energy_min, agent0_attention_min
[-16.235  -1.274]
agent1_energy_min, agent1_attention_min
[-30.423  -4.256]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -86.31852889298568, time: 305.24
agent0_energy_min, agent0_attention_min
[-17.021  -0.869]
agent1_energy_min, agent1_attention_min
[-33.73   -0.637]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -71.34346771937929, time: 303.439
agent0_energy_min, agent0_attention_min
[-17.309  -1.662]
agent1_energy_min, agent1_attention_min
[-33.586  -1.764]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -79.50996180810247, time: 302.775
agent0_energy_min, agent0_attention_min
[-16.745  -3.407]
agent1_energy_min, agent1_attention_min
[-34.564  -3.2  ]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -64.33649538769684, time: 305.098
agent0_energy_min, agent0_attention_min
[-15.772  -0.759]
agent1_energy_min, agent1_attention_min
[-30.776  -9.44 ]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -71.24271040063654, time: 304.52
agent0_energy_min, agent0_attention_min
[-13.346  -2.061]
agent1_energy_min, agent1_attention_min
[-27.672  -9.43 ]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -75.75271392193646, time: 306.068
agent0_energy_min, agent0_attention_min
[-14.491  -1.111]
agent1_energy_min, agent1_attention_min
[-26.757  -5.58 ]
39000 50
Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-4__2018-07-15_11-12-36...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -167.48546495132038, time: 221.113
agent0_energy_min, agent0_attention_min
[-16.74074074 -16.5995996 ]
agent1_energy_min, agent1_attention_min
[-15.55955956 -16.45045045]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -356.27423982945623, time: 317.432
agent0_energy_min, agent0_attention_min
[-11.849 -23.494]
agent1_energy_min, agent1_attention_min
[-11.291 -25.769]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -134.13429266306528, time: 319.189
agent0_energy_min, agent0_attention_min
[ -3.201 -44.914]
agent1_energy_min, agent1_attention_min
[ -4.688 -43.091]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -124.20171912540637, time: 317.583
agent0_energy_min, agent0_attention_min
[ -8.573 -41.204]
agent1_energy_min, agent1_attention_min
[ -9.537 -38.898]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -125.54177967978086, time: 319.607
agent0_energy_min, agent0_attention_min
[ -9.614 -40.11 ]
agent1_energy_min, agent1_attention_min
[-13.77  -34.968]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -124.2328594626745, time: 316.496
agent0_energy_min, agent0_attention_min
[-11.344 -38.154]
agent1_energy_min, agent1_attention_min
[-16.125 -31.775]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -109.08563941929539, time: 308.657
agent0_energy_min, agent0_attention_min
[-27.611 -21.385]
agent1_energy_min, agent1_attention_min
[-25.098 -23.877]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -103.84443642901786, time: 302.513
agent0_energy_min, agent0_attention_min
[-38.673 -10.119]
agent1_energy_min, agent1_attention_min
[-29.633 -17.101]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -85.4385736845442, time: 302.451
agent0_energy_min, agent0_attention_min
[-45.552  -3.779]
agent1_energy_min, agent1_attention_min
[-34.283 -11.762]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -98.01034912182544, time: 302.083
agent0_energy_min, agent0_attention_min
[-47.416  -2.328]
agent1_energy_min, agent1_attention_min
[-32.489  -9.613]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -111.70125723945405, time: 305.333
agent0_energy_min, agent0_attention_min
[-48.424  -1.415]
agent1_energy_min, agent1_attention_min
[-35.333  -7.294]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -92.20863033734803, time: 303.956
agent0_energy_min, agent0_attention_min
[-47.597  -1.355]
agent1_energy_min, agent1_attention_min
[-38.462  -5.386]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -92.00461145879041, time: 303.438
agent0_energy_min, agent0_attention_min
[-46.565  -0.962]
agent1_energy_min, agent1_attention_min
[-35.584  -8.611]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -88.05095590892384, time: 305.805
agent0_energy_min, agent0_attention_min
[-45.655  -3.713]
agent1_energy_min, agent1_attention_min
[-34.011 -11.306]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -90.67239869062128, time: 308.852
agent0_energy_min, agent0_attention_min
[-44.936  -4.397]
agent1_energy_min, agent1_attention_min
[-36.965  -8.763]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -85.78494971307857, time: 304.462
agent0_energy_min, agent0_attention_min
[-45.528  -4.236]
agent1_energy_min, agent1_attention_min
[-36.303  -9.484]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -99.35047227828437, time: 303.165
agent0_energy_min, agent0_attention_min
[-46.91   -2.784]
agent1_energy_min, agent1_attention_min
[-37.858  -8.914]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -86.73805813308223, time: 304.427
agent0_energy_min, agent0_attention_min
[-44.812  -1.389]
agent1_energy_min, agent1_attention_min
[-42.13  -4.52]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -123.70206668601365, time: 306.781
agent0_energy_min, agent0_attention_min
[-40.226  -3.691]
agent1_energy_min, agent1_attention_min
[-46.015  -2.582]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -76.40295079030666, time: 307.674
agent0_energy_min, agent0_attention_min
[-45.038  -0.712]
agent1_energy_min, agent1_attention_min
[-46.853  -1.839]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -83.31485383617891, time: 305.205
agent0_energy_min, agent0_attention_min
[-44.087  -1.153]
agent1_energy_min, agent1_attention_min
[-45.775  -2.223]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -71.24358569844209, time: 306.095
agent0_energy_min, agent0_attention_min
[-41.368  -1.199]
agent1_energy_min, agent1_attention_min
[-47.007  -2.118]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -111.30780870243802, time: 306.028
agent0_energy_min, agent0_attention_min
[-34.358  -8.542]
agent1_energy_min, agent1_attention_min
[-44.202  -4.108]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -90.44276724964902, time: 305.448
agent0_energy_min, agent0_attention_min
[-39.498  -7.26 ]
agent1_energy_min, agent1_attention_min
[-41.359  -7.885]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -74.89042618312378, time: 304.119
agent0_energy_min, agent0_attention_min
[-38.561  -4.522]
agent1_energy_min, agent1_attention_min
[-42.084  -4.915]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -70.94322957733492, time: 306.619
agent0_energy_min, agent0_attention_min
[-37.583  -8.439]
agent1_energy_min, agent1_attention_min
[-41.781  -2.853]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -87.9991461561403, time: 306.738
agent0_energy_min, agent0_attention_min
[-30.655 -13.037]
agent1_energy_min, agent1_attention_min
[-43.416  -1.997]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -70.58656961590393, time: 305.608
agent0_energy_min, agent0_attention_min
[-28.867  -4.668]
agent1_energy_min, agent1_attention_min
[-45.482  -3.154]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -71.19600594518198, time: 307.94
agent0_energy_min, agent0_attention_min
[-32.539  -3.439]
agent1_energy_min, agent1_attention_min
[-45.535  -3.887]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -85.30028649185304, time: 307.593
agent0_energy_min, agent0_attention_min
[-34.162  -7.285]
agent1_energy_min, agent1_attention_min
[-45.259  -3.984]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -73.24235215203315, time: 310.443
agent0_energy_min, agent0_attention_min
[-33.232  -8.304]
agent1_energy_min, agent1_attention_min
[-45.736  -3.894]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -64.00611070060623, time: 308.345
agent0_energy_min, agent0_attention_min
[-30.216  -5.798]
agent1_energy_min, agent1_attention_min
[-45.47   -4.239]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -73.14498457694104, time: 307.977
agent0_energy_min, agent0_attention_min
[-23.098  -3.81 ]
agent1_energy_min, agent1_attention_min
[-38.562  -6.499]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -68.8417012201312, time: 307.664
agent0_energy_min, agent0_attention_min
[-29.165  -2.321]
agent1_energy_min, agent1_attention_min
[-33.753  -7.306]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -77.6945840882492, time: 308.056
agent0_energy_min, agent0_attention_min
[-21.777 -10.746]
agent1_energy_min, agent1_attention_min
[-33.573  -6.664]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -83.07279406013782, time: 306.296
agent0_energy_min, agent0_attention_min
[-28.292  -3.901]
agent1_energy_min, agent1_attention_min
[-32.515  -4.045]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -85.73854399184019, time: 307.538
agent0_energy_min, agent0_attention_min
[-29.031  -5.143]
agent1_energy_min, agent1_attention_min
[-28.273  -4.783]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -75.8222018869504, time: 310.091
agent0_energy_min, agent0_attention_min
[-26.542  -5.194]
agent1_energy_min, agent1_attention_min
[-22.97   -9.425]
39000 50Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-5__2018-07-15_11-12-39...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -168.60728146105575, time: 222.369
agent0_energy_min, agent0_attention_min
[-17.74774775 -17.59459459]
agent1_energy_min, agent1_attention_min
[-15.8028028  -15.93693694]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -343.80199618589506, time: 316.707
agent0_energy_min, agent0_attention_min
[-18.295 -17.847]
agent1_energy_min, agent1_attention_min
[ -9.505 -23.472]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -136.34580917304115, time: 319.15
agent0_energy_min, agent0_attention_min
[ -4.399 -41.197]
agent1_energy_min, agent1_attention_min
[ -7.307 -38.723]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -128.24651880131816, time: 314.638
agent0_energy_min, agent0_attention_min
[ -7.965 -39.319]
agent1_energy_min, agent1_attention_min
[-20.938 -26.358]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -118.92335838092656, time: 318.319
agent0_energy_min, agent0_attention_min
[-10.714 -36.652]
agent1_energy_min, agent1_attention_min
[-34.758 -12.966]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -105.32113981620738, time: 315.819
agent0_energy_min, agent0_attention_min
[-14.065 -34.576]
agent1_energy_min, agent1_attention_min
[-41.519  -7.236]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -96.39405653718624, time: 315.547
agent0_energy_min, agent0_attention_min
[-17.991 -30.956]
agent1_energy_min, agent1_attention_min
[-44.764  -4.42 ]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -90.19185898682231, time: 314.916
agent0_energy_min, agent0_attention_min
[-21.128 -27.904]
agent1_energy_min, agent1_attention_min
[-44.233  -5.368]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -96.09139060652977, time: 306.526
agent0_energy_min, agent0_attention_min
[-24.935 -24.023]
agent1_energy_min, agent1_attention_min
[-43.545  -6.093]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -90.00432234134992, time: 299.125
agent0_energy_min, agent0_attention_min
[-29.267 -17.662]
agent1_energy_min, agent1_attention_min
[-43.938  -5.692]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -83.75372320888992, time: 301.897
agent0_energy_min, agent0_attention_min
[-38.005 -10.504]
agent1_energy_min, agent1_attention_min
[-46.394  -3.319]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -82.33721352379268, time: 302.67
agent0_energy_min, agent0_attention_min
[-43.911  -5.341]
agent1_energy_min, agent1_attention_min
[-46.444  -3.183]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -99.1296055849341, time: 300.225
agent0_energy_min, agent0_attention_min
[-42.289  -6.901]
agent1_energy_min, agent1_attention_min
[-45.785  -3.995]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -79.71357078272905, time: 303.55
agent0_energy_min, agent0_attention_min
[-34.013  -5.448]
agent1_energy_min, agent1_attention_min
[-46.769  -3.052]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -87.02525668246122, time: 307.691
agent0_energy_min, agent0_attention_min
[-29.7    -9.976]
agent1_energy_min, agent1_attention_min
[-46.93   -3.012]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -74.81520595117861, time: 305.75
agent0_energy_min, agent0_attention_min
[-36.18   -8.175]
agent1_energy_min, agent1_attention_min
[-47.588  -2.358]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -73.71688368531868, time: 301.684
agent0_energy_min, agent0_attention_min
[-40.635  -7.279]
agent1_energy_min, agent1_attention_min
[-47.035  -2.904]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -76.90973296367945, time: 304.912
agent0_energy_min, agent0_attention_min
[-38.269  -9.331]
agent1_energy_min, agent1_attention_min
[-47.066  -2.735]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -79.28172769188355, time: 303.762
agent0_energy_min, agent0_attention_min
[-40.917  -8.498]
agent1_energy_min, agent1_attention_min
[-47.272  -2.323]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -71.53205502788356, time: 304.279
agent0_energy_min, agent0_attention_min
[-32.767  -9.212]
agent1_energy_min, agent1_attention_min
[-47.208  -2.679]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -72.07959545185176, time: 304.844
agent0_energy_min, agent0_attention_min
[-26.652  -6.26 ]
agent1_energy_min, agent1_attention_min
[-46.862  -3.111]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -92.99464963028136, time: 306.923
agent0_energy_min, agent0_attention_min
[-33.673  -8.124]
agent1_energy_min, agent1_attention_min
[-45.716  -4.187]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -82.69119461089832, time: 305.421
agent0_energy_min, agent0_attention_min
[-32.381 -10.612]
agent1_energy_min, agent1_attention_min
[-44.96   -4.492]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -116.5789965260839, time: 305.173
agent0_energy_min, agent0_attention_min
[-21.518 -19.188]
agent1_energy_min, agent1_attention_min
[-45.686  -3.254]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -73.48328603701805, time: 304.508
agent0_energy_min, agent0_attention_min
[-21.792 -13.988]
agent1_energy_min, agent1_attention_min
[-47.122  -1.711]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -77.74103842924626, time: 306.942
agent0_energy_min, agent0_attention_min
[-29.546  -3.557]
agent1_energy_min, agent1_attention_min
[-45.069  -4.29 ]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -72.62048450018962, time: 307.013
agent0_energy_min, agent0_attention_min
[-29.695  -4.759]
agent1_energy_min, agent1_attention_min
[-41.062  -1.069]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -82.06509982511092, time: 308.329
agent0_energy_min, agent0_attention_min
[-25.906  -8.599]
agent1_energy_min, agent1_attention_min
[-42.331  -1.162]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -94.07259724900452, time: 309.717
agent0_energy_min, agent0_attention_min
[-22.475  -8.943]
agent1_energy_min, agent1_attention_min
[-33.277  -7.508]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -85.87901453319995, time: 312.656
agent0_energy_min, agent0_attention_min
[-21.326  -7.533]
agent1_energy_min, agent1_attention_min
[-35.877  -4.599]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -77.74110387044561, time: 311.769
agent0_energy_min, agent0_attention_min
[-22.699  -4.529]
agent1_energy_min, agent1_attention_min
[-42.407  -2.462]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -69.686684534247, time: 310.494
agent0_energy_min, agent0_attention_min
[-22.582  -5.136]
agent1_energy_min, agent1_attention_min
[-39.754  -3.624]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -67.23259543591895, time: 308.315
agent0_energy_min, agent0_attention_min
[-22.4   -5.32]
agent1_energy_min, agent1_attention_min
[-37.4    -3.255]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -66.73339895092454, time: 307.253
agent0_energy_min, agent0_attention_min
[-21.561  -3.514]
agent1_energy_min, agent1_attention_min
[-37.414  -2.04 ]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -82.94277339318444, time: 309.639
agent0_energy_min, agent0_attention_min
[-20.443  -5.679]
agent1_energy_min, agent1_attention_min
[-37.786  -2.576]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -66.32697216602398, time: 308.361
agent0_energy_min, agent0_attention_min
[-25.283  -5.307]
agent1_energy_min, agent1_attention_min
[-35.668  -2.417]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -78.38308358635705, time: 306.718
agent0_energy_min, agent0_attention_min
[-19.336  -3.681]
agent1_energy_min, agent1_attention_min
[-34.335  -1.517]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -98.00837867466294, time: 310.016
agent0_energy_min, agent0_attention_min
[-19.2   -3.13]
agent1_energy_min, agent1_attention_min
[-37.932  -1.612]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -84.69507240422861, time: 309.883Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-3__2018-07-15_11-12-28...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -163.68967990106214, time: 216.247
agent0_energy_min, agent0_attention_min
[-14.78278278 -18.36036036]
agent1_energy_min, agent1_attention_min
[-16.44644645 -16.73173173]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -346.25917621001685, time: 309.123
agent0_energy_min, agent0_attention_min
[ -9.697 -26.384]
agent1_energy_min, agent1_attention_min
[-31.419  -7.709]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -126.89987712101563, time: 308.237
agent0_energy_min, agent0_attention_min
[ -4.694 -41.824]
agent1_energy_min, agent1_attention_min
[-35.827  -3.242]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -107.61742729832274, time: 308.211
agent0_energy_min, agent0_attention_min
[-13.837 -28.537]
agent1_energy_min, agent1_attention_min
[-42.011  -3.388]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -104.1393738979332, time: 307.849
agent0_energy_min, agent0_attention_min
[-19.862 -23.884]
agent1_energy_min, agent1_attention_min
[-45.052  -1.995]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -94.6140852313253, time: 308.749
agent0_energy_min, agent0_attention_min
[-17.345 -26.774]
agent1_energy_min, agent1_attention_min
[-47.623  -1.089]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -99.18709744322408, time: 309.491
agent0_energy_min, agent0_attention_min
[-20.131 -23.776]
agent1_energy_min, agent1_attention_min
[-47.135  -0.491]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -90.938644697277, time: 310.313
agent0_energy_min, agent0_attention_min
[-17.187 -29.504]
agent1_energy_min, agent1_attention_min
[-47.67   -0.536]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -97.74259798868725, time: 310.181
agent0_energy_min, agent0_attention_min
[-15.97  -30.951]
agent1_energy_min, agent1_attention_min
[-43.394  -0.109]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -89.82919930615176, time: 307.821
agent0_energy_min, agent0_attention_min
[-17.857 -30.171]
agent1_energy_min, agent1_attention_min
[-38.275  -0.211]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -94.15081022787216, time: 308.939
agent0_energy_min, agent0_attention_min
[-16.013 -30.401]
agent1_energy_min, agent1_attention_min
[-35.338  -0.301]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -116.1315598929346, time: 309.417
agent0_energy_min, agent0_attention_min
[-16.977 -28.902]
agent1_energy_min, agent1_attention_min
[-38.033  -0.268]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -88.7585085439087, time: 307.032
agent0_energy_min, agent0_attention_min
[-15.108 -32.949]
agent1_energy_min, agent1_attention_min
[-34.474  -0.38 ]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -85.56450227228028, time: 309.148
agent0_energy_min, agent0_attention_min
[-16.909 -31.617]
agent1_energy_min, agent1_attention_min
[-32.511  -0.255]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -82.00431777702947, time: 309.846
agent0_energy_min, agent0_attention_min
[-14.89  -33.576]
agent1_energy_min, agent1_attention_min
[-28.302  -2.962]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -95.5919679816071, time: 310.371
agent0_energy_min, agent0_attention_min
[-15.421 -33.111]
agent1_energy_min, agent1_attention_min
[-28.663  -0.211]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -84.83828539765729, time: 307.701
agent0_energy_min, agent0_attention_min
[-16.76  -31.741]
agent1_energy_min, agent1_attention_min
[-32.831  -0.378]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -80.87706467292368, time: 308.039
agent0_energy_min, agent0_attention_min
[-16.202 -32.457]
agent1_energy_min, agent1_attention_min
[-31.278  -3.906]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -73.40406461198847, time: 308.444
agent0_energy_min, agent0_attention_min
[-14.882 -33.655]
agent1_energy_min, agent1_attention_min
[-22.541  -7.354]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -77.40594099552376, time: 311.39
agent0_energy_min, agent0_attention_min
[-15.355 -33.364]
agent1_energy_min, agent1_attention_min
[-20.797 -14.916]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -72.87094898446247, time: 310.62
agent0_energy_min, agent0_attention_min
[-17.006 -31.578]
agent1_energy_min, agent1_attention_min
[-22.235 -10.268]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -73.61106910139584, time: 309.5
agent0_energy_min, agent0_attention_min
[-28.933 -19.598]
agent1_energy_min, agent1_attention_min
[-21.524 -17.454]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -106.94270589078957, time: 310.406
agent0_energy_min, agent0_attention_min
[-34.317 -14.572]
agent1_energy_min, agent1_attention_min
[-21.057 -12.344]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -82.1159472917348, time: 308.84
agent0_energy_min, agent0_attention_min
[-31.6   -18.168]
agent1_energy_min, agent1_attention_min
[-31.937  -7.244]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -74.86023507451779, time: 307.168
agent0_energy_min, agent0_attention_min
[-37.464 -12.34 ]
agent1_energy_min, agent1_attention_min
[-32.251  -6.428]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -80.63158530395621, time: 308.941
agent0_energy_min, agent0_attention_min
[-35.48  -11.278]
agent1_energy_min, agent1_attention_min
[-30.835  -7.198]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -74.51703034616564, time: 306.259
agent0_energy_min, agent0_attention_min
[-38.497 -10.993]
agent1_energy_min, agent1_attention_min
[-26.541  -6.099]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -91.93883020656067, time: 308.489
agent0_energy_min, agent0_attention_min
[-38.721 -10.429]
agent1_energy_min, agent1_attention_min
[-26.718 -11.439]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -72.1145229358704, time: 310.241
agent0_energy_min, agent0_attention_min
[-36.445 -11.104]
agent1_energy_min, agent1_attention_min
[-24.731  -7.087]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -72.52784124210685, time: 311.436
agent0_energy_min, agent0_attention_min
[-30.947 -12.689]
agent1_energy_min, agent1_attention_min
[-23.583  -6.713]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -65.72289579109359, time: 314.097
agent0_energy_min, agent0_attention_min
[-26.766 -16.696]
agent1_energy_min, agent1_attention_min
[-24.584 -15.359]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -95.46973261795182, time: 312.004
agent0_energy_min, agent0_attention_min
[-23.915 -22.832]
agent1_energy_min, agent1_attention_min
[-20.964 -13.614]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -81.01327257949674, time: 311.163
agent0_energy_min, agent0_attention_min
[-19.252 -26.368]
agent1_energy_min, agent1_attention_min
[-27.579  -9.651]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -72.53392270190871, time: 309.691
agent0_energy_min, agent0_attention_min
[-16.558 -28.887]
agent1_energy_min, agent1_attention_min
[-27.452 -16.36 ]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -84.40322473293784, time: 311.411
agent0_energy_min, agent0_attention_min
[-14.763 -31.288]
agent1_energy_min, agent1_attention_min
[-20.692 -24.526]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -62.76811100255574, time: 308.859
agent0_energy_min, agent0_attention_min
[-20.389 -27.204]
agent1_energy_min, agent1_attention_min
[-28.619 -15.451]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -73.78804716794323, time: 308.99
agent0_energy_min, agent0_attention_min
[-19.446 -27.582]
agent1_energy_min, agent1_attention_min
[-27.666 -17.392]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -64.28834325837515, time: 310.26
agent0_energy_min, agent0_attention_min
[-22.208 -23.58 ]
agent1_energy_min, agent1_attention_min
[-28.756 -16.083]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -71.52386869360272, time: 310.472Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer2_2agents-6__2018-07-15_11-12-49...
1000 50
steps: 49950, episodes: 1000, mean episode reward: -180.79068612647475, time: 219.781
agent0_energy_min, agent0_attention_min
[-15.86886887 -16.62862863]
agent1_energy_min, agent1_attention_min
[-16.4034034  -16.16416416]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -414.6468222525796, time: 317.899
agent0_energy_min, agent0_attention_min
[-11.451  -8.614]
agent1_energy_min, agent1_attention_min
[-11.001 -23.471]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -178.3799242577421, time: 312.597
agent0_energy_min, agent0_attention_min
[-22.42   -5.881]
agent1_energy_min, agent1_attention_min
[-24.104 -17.792]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -146.4656129619649, time: 315.12
agent0_energy_min, agent0_attention_min
[-17.483 -20.726]
agent1_energy_min, agent1_attention_min
[-44.222  -3.043]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -135.58194323230146, time: 312.098
agent0_energy_min, agent0_attention_min
[-22.601 -16.655]
agent1_energy_min, agent1_attention_min
[-43.121  -2.894]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -119.81454355422711, time: 310.824
agent0_energy_min, agent0_attention_min
[-39.996  -3.319]
agent1_energy_min, agent1_attention_min
[-46.123  -0.551]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -116.92536761556792, time: 314.497
agent0_energy_min, agent0_attention_min
[-39.577  -0.817]
agent1_energy_min, agent1_attention_min
[-46.286  -0.575]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -112.78224694684737, time: 313.766
agent0_energy_min, agent0_attention_min
[-41.906  -0.427]
agent1_energy_min, agent1_attention_min
[-47.437  -0.419]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -100.5930536566108, time: 312.93
agent0_energy_min, agent0_attention_min
[-46.113  -0.203]
agent1_energy_min, agent1_attention_min
[-48.892  -0.359]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -99.21577080061869, time: 309.803
agent0_energy_min, agent0_attention_min
[-44.478  -0.446]
agent1_energy_min, agent1_attention_min
[-49.387  -0.39 ]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -101.9029931267013, time: 311.778
agent0_energy_min, agent0_attention_min
[-44.356  -0.408]
agent1_energy_min, agent1_attention_min
[-49.4    -0.406]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -89.88376653904014, time: 311.618
agent0_energy_min, agent0_attention_min
[-43.924  -0.698]
agent1_energy_min, agent1_attention_min
[-49.287  -0.37 ]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -84.1750252133295, time: 311.945
agent0_energy_min, agent0_attention_min
[-44.636  -0.512]
agent1_energy_min, agent1_attention_min
[-49.31   -0.448]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -84.42187829223352, time: 313.162
agent0_energy_min, agent0_attention_min
[-45.958  -0.659]
agent1_energy_min, agent1_attention_min
[-49.223  -0.699]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -95.45996833299809, time: 312.925
agent0_energy_min, agent0_attention_min
[-47.171  -0.345]
agent1_energy_min, agent1_attention_min
[-48.967  -0.998]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -88.29453999686294, time: 315.339
agent0_energy_min, agent0_attention_min
[-47.624  -0.707]
agent1_energy_min, agent1_attention_min
[-46.628  -1.942]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -84.45949145845785, time: 308.535
agent0_energy_min, agent0_attention_min
[-48.59   -0.283]
agent1_energy_min, agent1_attention_min
[-44.971  -2.168]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -78.7009565542867, time: 314.774
agent0_energy_min, agent0_attention_min
[-48.231  -0.257]
agent1_energy_min, agent1_attention_min
[-43.48   -1.272]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -74.4578738316454, time: 313.889
agent0_energy_min, agent0_attention_min
[-48.322  -0.377]
agent1_energy_min, agent1_attention_min
[-46.973  -0.893]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -72.01615584481249, time: 312.45
agent0_energy_min, agent0_attention_min
[-48.513  -0.15 ]
agent1_energy_min, agent1_attention_min
[-46.905  -0.989]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -73.87967473339302, time: 312.437
agent0_energy_min, agent0_attention_min
[-48.652  -0.146]
agent1_energy_min, agent1_attention_min
[-43.626  -1.461]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -97.59972127525714, time: 311.745
agent0_energy_min, agent0_attention_min
[-48.67   -0.124]
agent1_energy_min, agent1_attention_min
[-43.938  -2.768]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -76.06821088663109, time: 311.358
agent0_energy_min, agent0_attention_min
[-49.232  -0.122]
agent1_energy_min, agent1_attention_min
[-47.506  -0.636]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -70.87713280826348, time: 310.312
agent0_energy_min, agent0_attention_min
[-4.9312e+01 -4.6000e-02]
agent1_energy_min, agent1_attention_min
[-46.411  -1.786]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -73.87928747008192, time: 310.913
agent0_energy_min, agent0_attention_min
[-42.399  -0.698]
agent1_energy_min, agent1_attention_min
[-43.835  -1.935]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -75.41809732640486, time: 310.804
agent0_energy_min, agent0_attention_min
[-34.962  -0.328]
agent1_energy_min, agent1_attention_min
[-48.06   -1.232]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -70.58376541338693, time: 305.834
agent0_energy_min, agent0_attention_min
[-38.798  -0.14 ]
agent1_energy_min, agent1_attention_min
[-47.265  -1.906]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -72.3664220593007, time: 302.604
agent0_energy_min, agent0_attention_min
[-38.802  -0.401]
agent1_energy_min, agent1_attention_min
[-46.33   -2.383]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -79.85931055984274, time: 302.735
agent0_energy_min, agent0_attention_min
[-39.423  -1.217]
agent1_energy_min, agent1_attention_min
[-41.24   -2.339]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -82.98666819080412, time: 304.546
agent0_energy_min, agent0_attention_min
[-37.204  -5.402]
agent1_energy_min, agent1_attention_min
[-45.033  -2.112]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -78.65144969347719, time: 303.757
agent0_energy_min, agent0_attention_min
[-43.227  -2.318]
agent1_energy_min, agent1_attention_min
[-39.727  -0.325]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -90.33238843991447, time: 302.689
agent0_energy_min, agent0_attention_min
[-41.065  -0.361]
agent1_energy_min, agent1_attention_min
[-38.309  -5.11 ]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -77.89140015339582, time: 301.46
agent0_energy_min, agent0_attention_min
[-43.488  -0.409]
agent1_energy_min, agent1_attention_min
[-38.084  -2.187]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -70.4066278595018, time: 297.857
agent0_energy_min, agent0_attention_min
[-40.186  -0.77 ]
agent1_energy_min, agent1_attention_min
[-33.085  -0.547]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -72.47439752323291, time: 302.296
agent0_energy_min, agent0_attention_min
[-44.487  -0.927]
agent1_energy_min, agent1_attention_min
[-27.437  -0.811]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -63.27290079447455, time: 301.521
agent0_energy_min, agent0_attention_min
[-46.307  -1.337]
agent1_energy_min, agent1_attention_min
[-28.224  -0.649]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -66.96025780188097, time: 299.488
agent0_energy_min, agent0_attention_min
[-44.417  -0.931]
agent1_energy_min, agent1_attention_min
[-23.034  -0.937]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -84.16337237574876, time: 304.053
agent0_energy_min, agent0_attention_min
[-41.546  -1.745]
agent1_energy_min, agent1_attention_min
[-23.467  -1.111]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -70.5206673313862, time: 308.612
agent0_energy_min, agent0_attention_min
[-21.792  -3.141]
agent1_energy_min, agent1_attention_min
[-14.923 -35.05 ]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -86.57803896757777, time: 308.441
agent0_energy_min, agent0_attention_min
[-20.959  -4.743]
agent1_energy_min, agent1_attention_min
[-16.163 -33.796]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.38 hr
 50
steps: 1949950, episodes: 39000, mean episode reward: -69.97972994788492, time: 308.104
agent0_energy_min, agent0_attention_min
[-37.011  -8.33 ]
agent1_energy_min, agent1_attention_min
[ -7.829 -40.962]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -62.76639213684375, time: 309.586
agent0_energy_min, agent0_attention_min
[-37.717  -6.348]
agent1_energy_min, agent1_attention_min
[-10.057 -39.682]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.39 hr

[-15.005  -1.61 ]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -75.0594506073518, time: 308.879
agent0_energy_min, agent0_attention_min
[-18.869  -1.01 ]
agent1_energy_min, agent1_attention_min
[-14.19   -1.451]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -84.95595903763935, time: 304.974
agent0_energy_min, agent0_attention_min
[-18.995  -1.443]
agent1_energy_min, agent1_attention_min
[-14.671  -2.395]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.39 hr
steps: 1949950, episodes: 39000, mean episode reward: -64.43494236558824, time: 305.516
agent0_energy_min, agent0_attention_min
[-16.571  -1.066]
agent1_energy_min, agent1_attention_min
[-27.524  -3.239]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -56.12374134876505, time: 304.684
agent0_energy_min, agent0_attention_min
[-17.167  -0.364]
agent1_energy_min, agent1_attention_min
[-25.966  -5.858]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.39 hr

steps: 1949950, episodes: 39000, mean episode reward: -66.7229480758842, time: 311.67
agent0_energy_min, agent0_attention_min
[-30.925  -2.237]
agent1_energy_min, agent1_attention_min
[-24.088  -1.874]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -60.59569808993569, time: 304.606
agent0_energy_min, agent0_attention_min
[-23.547  -5.146]
agent1_energy_min, agent1_attention_min
[-22.212  -1.748]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.40 hr

[-21.287 -16.433]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -85.70905977065951, time: 304.0
agent0_energy_min, agent0_attention_min
[-18.835  -0.91 ]
agent1_energy_min, agent1_attention_min
[-26.81  -15.346]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -80.82129577370439, time: 297.437
agent0_energy_min, agent0_attention_min
[-21.793  -1.607]
agent1_energy_min, agent1_attention_min
[-22.87  -18.622]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.40 hr

[-33.76   -1.645]
agent1_energy_min, agent1_attention_min
[-35.701 -11.672]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -86.26574601315696, time: 307.844
agent0_energy_min, agent0_attention_min
[-22.523  -0.045]
agent1_energy_min, agent1_attention_min
[-35.495 -10.79 ]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -67.40272374451422, time: 298.603
agent0_energy_min, agent0_attention_min
[-22.681  -0.063]
agent1_energy_min, agent1_attention_min
[-36.783  -9.204]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.40 hr

agent0_energy_min, agent0_attention_min
[-18.675  -6.51 ]
agent1_energy_min, agent1_attention_min
[-33.647  -1.43 ]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -68.39637102051837, time: 299.331
agent0_energy_min, agent0_attention_min
[-19.668  -5.072]
agent1_energy_min, agent1_attention_min
[-31.413  -3.152]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.40 hr

[-34.702  -1.428]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -80.03776064838176, time: 303.156
agent0_energy_min, agent0_attention_min
[-22.799  -0.078]
agent1_energy_min, agent1_attention_min
[-35.215  -2.524]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -72.78578139031309, time: 295.412
agent0_energy_min, agent0_attention_min
[-23.428  -0.045]
agent1_energy_min, agent1_attention_min
[-31.512  -3.602]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.40 hr

agent0_energy_min, agent0_attention_min
[-20.25  -23.803]
agent1_energy_min, agent1_attention_min
[-28.749 -14.265]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -81.36191902740873, time: 287.368
agent0_energy_min, agent0_attention_min
[-18.234 -23.01 ]
agent1_energy_min, agent1_attention_min
[-36.744  -5.46 ]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.41 hr
 50
steps: 1949950, episodes: 39000, mean episode reward: -65.3709629671428, time: 303.546
agent0_energy_min, agent0_attention_min
[-46.525  -0.125]
agent1_energy_min, agent1_attention_min
[-25.956  -0.991]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -65.6613495458864, time: 277.993
agent0_energy_min, agent0_attention_min
[-44.25   -1.371]
agent1_energy_min, agent1_attention_min
[-25.313  -0.472]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.40 hr

39000 50
steps: 1949950, episodes: 39000, mean episode reward: -72.85645727360462, time: 309.314
agent0_energy_min, agent0_attention_min
[-23.149  -4.359]
agent1_energy_min, agent1_attention_min
[-18.78   -0.424]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -60.42912568151507, time: 278.776
agent0_energy_min, agent0_attention_min
[-27.596  -3.855]
agent1_energy_min, agent1_attention_min
[-16.945  -0.35 ]
...Finished!
Trained episodes: 1 -> 40000
Total time: 3.41 hr
