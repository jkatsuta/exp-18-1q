python train.py --scenario wanderer1_4agents-1 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-2 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
python train.py --scenario wanderer1_4agents-3 --num-episodes 40000 --max-episode-len 50 --good-policy ddpg --adv-policy ddpg  &
Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-2__2018-07-12_00-27-45...
200 50
steps: 9950, episodes: 200, mean episode reward: -350.9068008221569, time: 79.12
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.35678392  -0.42211055  -9.50170854]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.09547739  -0.48743719 -12.5       ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.11055276  -0.49246231 -12.67005025]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.83919598  -0.50251256 -12.12954774]
400 50
steps: 19950, episodes: 400, mean episode reward: -356.11220527743995, time: 82.308
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.925   -0.345   -9.3728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.285   -0.485  -12.4455]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.01    -0.475  -12.2046]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.4     -0.5    -11.9138]
600 50
steps: 29950, episodes: 600, mean episode reward: -369.9075518677698, time: 81.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.405   -0.405   -9.0971]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.91    -0.48   -12.4842]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.615   -0.49   -12.4425]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.      -0.455  -12.2763]
800 50
steps: 39950, episodes: 800, mean episode reward: -335.9538896912457, time: 82.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.815   -0.34    -9.2249]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.84    -0.53   -12.2639]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.15    -0.55   -12.2969]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.055  -0.525 -12.354]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -376.88954842074105, time: 80.371
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.39    -0.365   -8.8628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.795  -0.52  -12.379]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.065   -0.465  -12.1704]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.055   -0.485  -12.3507]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1557.0690754716709, time: 121.983
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.91    -0.525  -15.6153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.67    -0.55   -13.9352]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.415   -0.515  -12.8634]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.91    -0.405   -8.6157]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -440.47005732363743, time: 125.378
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.41   -0.125  -3.3606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.045  -0.06   -1.7511]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.03   -0.645 -17.866]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.52    -0.405  -10.3315]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -371.4379345143745, time: 126.718
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.88    -0.25   -10.6588]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.29    -0.49   -12.8596]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.525   -0.535  -11.0014]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.475  -0.525 -15.377]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -346.3434000052749, time: 127.859
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.48    -0.26    -5.3127]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.335   -0.735  -19.6266]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.485   -0.615  -18.0533]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.625   -0.26   -10.6367]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -265.42527198631154, time: 126.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.83    -0.195   -5.6449]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.095   -0.48   -12.4707]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.47    -0.645  -18.7021]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.815 -0.4   -5.863]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -240.06558590031025, time: 126.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.79   -0.205  -4.6788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.59    -0.4    -11.9569]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.435   -0.38    -8.0555]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.6    -0.25   -1.2324]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -229.72137973445678, time: 125.207
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.335   -0.36    -8.6574]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.51    -0.44   -12.4797]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.145   -0.555   -9.2256]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.115  -0.305  -1.4957]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -228.32124939435238, time: 127.498
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.86    -0.355   -9.3681]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.455   -0.435   -9.2817]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.525   -0.585  -10.1124]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.365  -0.27   -1.5862]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -222.7728911108258, time: 126.069
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.89   -0.26   -4.4095]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.38    -0.34    -9.9178]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.195   -0.57   -10.1104]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.04   -0.29   -1.3606]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -183.73038938383837, time: 126.539
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.46    -0.35    -5.3235]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.71    -0.35   -10.4532]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.27   -0.725 -15.09 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.27  -0.375 -2.543]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -169.45478108247278, time: 127.941
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.565  -0.355  -2.3712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.12    -0.375  -11.6853]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.71    -0.82   -17.2038]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.405 -0.255 -1.005]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -156.74145942320035, time: 126.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.375  -0.285  -1.1416]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.37    -0.335  -11.3485]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.93    -0.77   -19.0839]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.1    -0.28   -1.3728]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -185.04024883737037, time: 127.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.015  -0.305  -0.9673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.785   -0.355  -11.2423]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.7     -0.845  -20.1122]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.745  -0.235  -1.0491]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -189.78594277841566, time: 126.928
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.955  -0.255  -0.8959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.255   -0.475  -15.1259]
agent2_energy_min, agent2_energy_max, agent2_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-2__2018-07-12_00-27-50...
200 50
steps: 9950, episodes: 200, mean episode reward: -350.97372619898715, time: 80.228
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.18592965  -0.54773869 -12.06603015]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.55276382  -0.52261307 -14.38572864]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.87437186  -0.48743719 -12.18261307]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.11055276  -0.47236181 -13.09366834]
400 50
steps: 19950, episodes: 400, mean episode reward: -323.6479126592883, time: 83.417
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.99    -0.43   -11.8161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.24   -0.585 -14.307]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.765   -0.43   -12.6404]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.88    -0.525  -13.1476]
600 50
steps: 29950, episodes: 600, mean episode reward: -347.7268131427788, time: 83.238
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.395   -0.495  -12.1021]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.95    -0.52   -14.5535]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.025   -0.53   -12.2151]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.3     -0.515  -13.3283]
800 50
steps: 39950, episodes: 800, mean episode reward: -325.1398369550388, time: 81.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.925   -0.415  -11.7304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.21    -0.575  -14.3013]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.33    -0.48   -12.4655]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.06    -0.505  -13.1507]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -325.76450842257344, time: 81.508
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.75    -0.51   -12.1634]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.775   -0.545  -13.9222]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.265   -0.505  -12.3025]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.205   -0.485  -13.3655]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1145.7701897927766, time: 128.613
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.68    -0.42   -10.9343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.5     -0.52   -13.9364]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.88    -0.72   -18.8341]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.565   -0.315   -7.9666]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -1363.7073088624322, time: 131.992
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.175   -0.525  -15.4836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.695   -0.44    -9.3071]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.13    -0.605  -10.3451]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.88    -0.65   -13.0446]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -571.4122041852258, time: 132.002
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.6     -0.435  -10.8625]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.495   -0.68   -13.4499]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.155   -0.53   -16.1338]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.055  -0.555 -12.556]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -318.84476679156944, time: 132.57
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.78    -0.65   -13.3186]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.75    -0.42   -11.9197]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.095  -0.275  -4.9428]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.115   -0.51   -14.3724]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -287.6182096793935, time: 131.353
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.725   -0.675  -17.5231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.32   -0.31   -4.9379]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.215   -0.36    -6.2257]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.94    -0.49   -14.8673]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -241.87098329343695, time: 128.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.46    -0.57   -16.0636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.005  -0.35   -5.2812]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.225  -0.23   -3.6138]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.295   -0.48   -13.9448]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -264.2591204503411, time: 125.759
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.25    -0.395  -11.4567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.005   -0.325   -6.4187]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.85   -0.24   -4.7076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.425   -0.485  -13.1104]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -202.22270901196728, time: 125.774
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.78    -0.39   -15.0041]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.795   -0.49    -9.9792]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.635  -0.175  -2.8106]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.36    -0.425  -13.6544]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -213.94680682277277, time: 123.798
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.57    -0.475  -16.9002]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.285   -0.33   -10.9829]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.785  -0.14   -2.5986]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.22    -0.505  -15.6751]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -201.38521584739624, time: 123.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.185  -0.465 -15.2  ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.425  -0.445 -13.1  ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.1    -0.09   -1.7756]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.07    -0.485  -15.3517]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -180.80799151725176, time: 124.27
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.52    -0.46   -18.0124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.835   -0.405  -13.1739]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.465  -0.225  -4.4592]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.425   -0.525  -17.3755]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -195.84807097619588, time: 125.632
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.985   -0.535  -18.6009]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.41   -0.44  -13.644]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.865  -0.135  -4.4331]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.465   -0.54   -19.2244]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -181.363058095614, time: 123.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.185   -0.51   -18.7607]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.055   -0.405  -12.9759]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.81   -0.16   -3.9607]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.835   -0.51   -18.7482]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -173.8137423577433, time: 124.807
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.485   -0.5    -18.9181]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.35    -0.37   -13.4682]
agent2_energy_min, agent2_energy_max, agent2_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-1__2018-07-12_00-27-43...
200 50
steps: 9950, episodes: 200, mean episode reward: -319.7963436850312, time: 77.774
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.26633166  -0.52261307 -11.88894472]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.68844221  -0.48241206 -12.11266332]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.53266332  -0.42211055 -10.76864322]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.12060302  -0.52261307 -13.29266332]
400 50
steps: 19950, episodes: 400, mean episode reward: -320.2654469986339, time: 80.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.945   -0.5    -12.3323]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.805   -0.465  -12.6859]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.385   -0.395  -10.6894]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.735   -0.515  -13.5493]
600 50
steps: 29950, episodes: 600, mean episode reward: -317.88839760177996, time: 80.953
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.63    -0.465  -12.0532]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.875   -0.485  -12.2519]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.01    -0.46   -11.0362]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.25    -0.515  -13.1589]
800 50
steps: 39950, episodes: 800, mean episode reward: -333.7810143542882, time: 79.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.385   -0.47   -11.9823]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.11    -0.49   -12.3176]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.095   -0.435  -10.7748]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.36    -0.51   -13.1504]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -324.7193093291846, time: 81.014
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.24    -0.525  -12.3982]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.825   -0.51   -12.6608]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.355   -0.42   -11.1919]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.49    -0.525  -12.7845]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1935.5729459630393, time: 123.494
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.745   -0.56   -19.1861]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.515   -0.465  -10.6409]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.09   -0.69  -15.463]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.28    -0.615  -18.0847]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -1021.1936810027769, time: 128.852
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.2     -0.185  -11.3687]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.715   -0.43   -10.3084]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.465   -0.61   -17.1944]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.15    -0.425  -13.4531]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -393.17557585294065, time: 130.355
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.09    -0.4    -10.6311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.855  -0.28   -7.11 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.245   -0.275  -12.2732]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.325   -0.57   -15.3296]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -312.1213682690913, time: 129.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.35    -0.47   -11.2694]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.89    -0.555  -14.5461]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.535   -0.36    -8.6303]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.8     -0.5    -19.4422]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -166.67134447290672, time: 129.998
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.12    -0.51   -11.0707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.75   -0.58   -5.6883]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.6     -0.53   -15.4583]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.975   -0.35   -11.3805]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -152.82982198066355, time: 128.858
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.61    -0.415  -11.1706]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.39    -0.555   -7.9129]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.915  -0.745 -20.121]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.4     -0.31   -14.9343]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -151.1023552390179, time: 129.22
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.53    -0.4    -13.6927]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.865   -0.57    -8.5901]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.495   -0.865  -23.3039]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.765  -0.275 -13.077]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -167.45695332381453, time: 129.704
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.71    -0.43   -12.7869]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.11    -0.46    -9.4767]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.36    -0.935  -23.4478]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.24    -0.28   -14.5286]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -128.039378119388, time: 127.859
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.865   -0.35   -10.2315]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.13    -0.515   -9.7502]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.63    -0.9    -22.8217]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.335   -0.3    -15.9875]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -124.51406134415927, time: 127.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.58    -0.31    -7.0754]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.28    -0.5     -9.2448]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.715   -0.94   -23.5339]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.875   -0.455  -18.0256]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -122.46334076667777, time: 128.192
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.665   -0.33    -5.9081]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.095  -0.49  -11.239]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.23    -0.965  -24.4353]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.65    -0.44   -16.9863]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -132.54310810369793, time: 129.304
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.065   -0.34    -4.9088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.235   -0.555  -12.6453]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.96    -0.93   -24.3304]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.21    -0.6    -20.5582]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -121.86112856087287, time: 128.012
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.88   -0.42   -5.3129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.075   -0.45    -9.6722]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.205   -0.945  -24.4511]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.56    -0.6    -20.7809]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -135.41493446847048, time: 128.872
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.47   -0.32   -3.7397]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.415   -0.405   -6.2392]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-2__2018-07-12_00-27-48...
200 50
steps: 9950, episodes: 200, mean episode reward: -381.77152534423556, time: 79.848
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.49246231  -0.44723618 -10.30201005]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.61809045  -0.61306533 -15.38      ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.89949749  -0.5879397  -15.54341709]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.00502513  -0.61306533 -14.09256281]
400 50
steps: 19950, episodes: 400, mean episode reward: -395.85866952830196, time: 83.181
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.405   -0.46   -10.3135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.69    -0.59   -15.3823]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.045   -0.595  -15.5548]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.935   -0.54   -13.9304]
600 50
steps: 29950, episodes: 600, mean episode reward: -394.73023003503886, time: 82.711
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.72    -0.455   -9.9069]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.915   -0.575  -15.5951]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.05    -0.605  -16.0967]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.805   -0.49   -13.3688]
800 50
steps: 39950, episodes: 800, mean episode reward: -383.6525751514532, time: 83.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.735   -0.46   -10.3319]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.78    -0.575  -15.4039]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.655   -0.68   -16.0002]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.68    -0.51   -13.7576]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -387.8453918240516, time: 82.785
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.295   -0.41   -10.2809]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.945  -0.6   -15.61 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.855   -0.535  -15.9071]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.465   -0.53   -13.7893]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -2182.180423714588, time: 129.389
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.67    -0.545  -14.4452]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.705  -0.295  -7.29 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.26    -0.255   -6.8356]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.485   -0.25    -6.4259]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -649.6107626449467, time: 133.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.28    -0.25   -11.7331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.475   -0.59    -8.5912]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.015  -0.755 -14.161]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.83    -0.52    -9.9199]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -308.10070264966174, time: 130.871
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.12   -0.01   -0.0793]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.22    -0.97   -24.2745]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.755   -0.795  -21.0023]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.3     -0.715  -18.1579]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -258.31523026887, time: 126.467
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.655  -0.05   -0.4628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.015   -0.68   -10.8526]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.27    -0.775  -20.2688]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.25   -0.615 -18.619]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -261.0614168869269, time: 125.72
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.715  -0.245  -2.5561]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.07    -0.62    -9.4607]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.115   -0.785  -20.2825]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.11    -0.59   -20.1147]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -269.87237012562395, time: 127.286
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.75   -0.25   -4.2457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.74    -0.68   -10.7219]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.62    -0.735  -21.6383]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.665   -0.645  -19.0922]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -290.31479465559516, time: 125.459
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.5     -0.355   -7.6739]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.38  -0.61  -5.123]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.81    -0.775  -21.8867]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.435   -0.47   -16.3307]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -276.43281750225935, time: 126.693
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.735   -0.4     -6.0564]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.045   -0.74   -10.6945]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.805   -0.69   -21.7057]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.82    -0.3    -11.1833]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -281.0624289405016, time: 127.596
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.97    -0.445  -10.0237]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.975   -0.79   -14.8747]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.555   -0.77   -21.1696]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.805   -0.46   -14.9055]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -277.96453947119653, time: 126.615
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.315  -0.21   -3.0462]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.35    -0.695   -9.3898]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.22  -0.8  -21.71]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.51    -0.54   -19.1097]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -288.7732706846241, time: 127.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.725  -0.22   -1.8781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.085   -0.72   -11.4621]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.17    -0.765  -15.4613]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.32    -0.6    -19.4117]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -223.82178793890984, time: 126.069
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.895  -0.175  -1.1139]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.285 -0.445 -5.139]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.91    -0.69   -15.7646]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.86    -0.59   -16.8154]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -244.63714300421088, time: 124.665
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.555  -0.16   -1.0489]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.71   -0.305  -2.8992]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.425   -0.645  -15.8377]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.26    -0.635  -16.0361]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -237.74984172200274, time: 127.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.685  -0.22   -2.0975]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.69    -0.405   -8.3697]
agent2_energy_min, agent2_energy_max, agent2_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-2__2018-07-12_00-27-52...
200 50
steps: 9950, episodes: 200, mean episode reward: -365.74849096848465, time: 80.809
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.3919598   -0.59798995 -15.29768844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.18592965  -0.52261307 -13.21557789]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.84422111  -0.49748744 -10.85949749]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.1758794   -0.52763819 -12.84120603]
400 50
steps: 19950, episodes: 400, mean episode reward: -391.49259393852304, time: 81.918
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.06  -0.56 -15.21]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.475   -0.52   -13.2866]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.4     -0.36   -10.4655]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.11    -0.425  -12.6494]
600 50
steps: 29950, episodes: 600, mean episode reward: -379.88686565771815, time: 81.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.86    -0.605  -15.0531]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.795   -0.465  -13.2305]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.48    -0.485  -10.6836]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.24    -0.515  -12.8153]
800 50
steps: 39950, episodes: 800, mean episode reward: -357.1799757334795, time: 82.421
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.65    -0.535  -14.8856]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.265   -0.51   -13.3989]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.965   -0.44   -10.4846]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.39    -0.44   -12.8189]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -370.7622442714391, time: 81.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.43    -0.535  -15.3331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.3    -0.485 -13.25 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.925   -0.39   -10.2793]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.03    -0.52   -13.1113]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1245.63611495762, time: 129.126
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.23    -0.93   -22.9263]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.525   -0.885  -21.3722]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.305  -0.34   -4.1547]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.01    -0.455  -12.9158]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -541.2420524118332, time: 132.587
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.425   -0.61   -15.6203]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.505   -0.395  -12.2005]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.925   -0.395   -8.8147]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.76    -0.43   -10.8994]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -360.4405479362004, time: 134.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.745   -0.715  -16.4976]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.54    -0.435   -8.5298]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.715  -0.28   -3.6607]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.655   -0.625  -15.3961]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -277.75423169039703, time: 133.563
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.3     -0.58   -11.6529]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.855   -0.49   -12.1615]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.87    -0.38   -13.6869]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.635   -0.395   -9.6527]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -206.0922494928657, time: 131.167
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.76    -0.285   -6.8508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.28   -0.25   -3.0069]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.615   -0.3     -5.7151]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.9    -0.115  -2.7558]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -189.5916831880764, time: 133.094
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.485   -0.38    -9.4164]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.795  -0.24   -2.3096]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.2     -0.335   -6.7076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.055  -0.22   -4.3404]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -177.42202332614283, time: 131.555
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.66    -0.595  -13.2056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.075  -0.31   -4.8334]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.335  -0.335  -2.7493]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.325  -0.265  -4.1385]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -183.30251423222856, time: 131.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.37    -0.53   -12.3494]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.265   -0.38    -7.1914]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.84   -0.355  -2.5026]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.51   -0.405  -5.3626]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -189.5017703770957, time: 129.237
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.015   -0.51   -10.4317]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.885  -0.285  -3.6222]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.59   -0.31   -1.9135]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.215   -0.535   -8.1828]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -166.16642583833593, time: 128.265
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.14    -0.635  -10.2336]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.705   -0.435   -7.9288]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.695  -0.31   -1.9307]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.46   -0.635  -9.842]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -165.78873358970043, time: 127.314
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.53    -0.64   -11.7796]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.07   -0.48   -8.278]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-2.825  -0.245  -1.4057]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.04    -0.675  -10.2338]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -163.80655431606394, time: 123.653
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.635   -0.75   -14.5473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.915  -0.43  -11.234]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.84   -0.38   -2.5238]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.96    -0.635   -9.1145]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -154.51370435859084, time: 124.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.575   -0.635  -13.9097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.275   -0.515  -11.8144]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.145  -0.27   -2.5904]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.37    -0.7    -11.4537]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -166.79927359571414, time: 125.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.72    -0.635  -13.5388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.82    -0.59   -14.1026]
agent2_energy_min, agent2_energy_max, agent2_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-2__2018-07-12_00-27-54...
200 50
steps: 9950, episodes: 200, mean episode reward: -330.9948220092758, time: 80.91
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.92964824  -0.46231156 -11.63708543]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.9798995   -0.42211055 -10.24582915]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.72864322  -0.51758794 -13.43417085]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.57788945  -0.48241206 -13.10974874]
400 50
steps: 19950, episodes: 400, mean episode reward: -321.59616741395, time: 82.296
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.15    -0.48   -11.7848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.32    -0.44   -10.5694]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.845   -0.505  -13.4709]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.975   -0.51   -12.7584]
600 50
steps: 29950, episodes: 600, mean episode reward: -324.20594272407726, time: 81.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.745   -0.51   -11.5542]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.37    -0.375   -9.9762]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.4     -0.54   -13.2256]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.425   -0.575  -13.1266]
800 50
steps: 39950, episodes: 800, mean episode reward: -328.1951792521164, time: 82.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.03    -0.495  -11.7083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.165   -0.44   -10.4414]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.395   -0.54   -13.3884]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.45    -0.515  -12.8817]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -329.21532169325053, time: 81.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.23    -0.545  -11.8792]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.74    -0.335  -10.0679]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.465   -0.53   -13.3901]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.515   -0.555  -13.0553]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -988.2198298261022, time: 127.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.44    -0.765  -19.1949]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.965   -0.545  -13.5676]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.3    -0.785 -19.312]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.79   -0.395  -6.455]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -442.99972453797005, time: 131.157
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.83    -0.22    -7.0973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.03    -0.5    -16.7122]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.415   -0.785  -19.6074]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.65    -0.48   -10.5509]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -346.0552901144605, time: 133.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.705   -0.535  -20.7273]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.95    -0.42   -14.6483]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.305   -0.455  -15.9632]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.16    -0.535  -11.6153]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -293.36538582586843, time: 131.155
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.99    -0.465  -18.2388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.05    -0.305  -14.3113]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.615   -0.575  -16.7943]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.38    -0.355   -9.9709]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -253.26174181279865, time: 132.015
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.93   -0.365 -10.044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.74    -0.345  -11.8661]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.39   -0.125  -2.4433]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.74    -0.41    -9.0425]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -236.42536122064658, time: 131.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.      -0.41    -6.4034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.775   -0.3    -10.8018]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.665   -0.27    -5.5386]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.255  -0.26   -4.0944]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -246.34457936828278, time: 129.578
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.49    -0.53   -10.4794]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.815  -0.29   -9.481]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.035   -0.465   -8.8817]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.285   -0.45   -13.0561]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -168.75493241349886, time: 130.767
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.765   -0.585   -9.6541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.68   -0.14   -3.1385]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.03    -0.585  -17.9278]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.58    -0.53   -16.7009]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -158.25828717419594, time: 131.012
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.54    -0.42    -6.6523]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.955 -0.07  -1.045]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.52    -0.85   -22.1081]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.035   -0.615  -18.6543]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -158.69479397814473, time: 130.742
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.065  -0.3    -5.2073]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.07   -0.095  -0.6686]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.855   -0.88   -22.9258]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.24    -0.705  -22.1078]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -139.61316013236876, time: 131.61
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.67   -0.265  -3.4029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.43   -0.05   -0.3147]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.265   -0.905  -23.7418]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.005   -0.77   -23.4916]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -130.033867800669, time: 129.802
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.715  -0.27   -2.6519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.295  -0.12   -1.8623]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.66    -0.885  -23.8554]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.605   -0.79   -23.8917]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -125.50975066022808, time: 129.508
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.36   -0.3    -2.4355]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.825 -0.125 -2.769]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.845  -0.915 -23.685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.99   -0.805 -24.203]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -133.43914739122187, time: 127.749
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.455  -0.355  -2.5941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.945  -0.085  -3.1546]
agent2_energy_min, agent2_energy_max, agent2_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-3__2018-07-12_00-28-00...
200 50
steps: 9950, episodes: 200, mean episode reward: -315.53756474191636, time: 80.525
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.70351759  -0.52261307 -14.05165829]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.23115578  -0.45226131 -11.44130653]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.28643216  -0.41708543 -10.35155779]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.70351759  -0.54773869 -12.17547739]
400 50
steps: 19950, episodes: 400, mean episode reward: -331.74552664536435, time: 81.71
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.61    -0.525  -13.4369]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.635   -0.46   -11.5804]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.795   -0.47   -10.5802]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.5    -0.48  -12.041]
600 50
steps: 29950, episodes: 600, mean episode reward: -335.4994407888825, time: 82.473
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.41    -0.54   -13.8404]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.645   -0.455  -11.6451]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.07   -0.435 -10.838]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.23    -0.51   -11.9674]
800 50
steps: 39950, episodes: 800, mean episode reward: -327.13242824364437, time: 83.02
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.52    -0.55   -13.4643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.075  -0.46  -11.98 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.365  -0.415 -10.431]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.605   -0.475  -12.1734]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -331.7963857910547, time: 82.634
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.015   -0.525  -13.6077]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.455   -0.475  -11.9598]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.345   -0.385  -10.7713]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.62    -0.465  -12.1643]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -878.6792038388677, time: 130.1
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.945   -0.66   -14.5841]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.13    -0.34    -7.8771]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.255   -0.285   -8.7877]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.49    -0.475  -10.7662]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -403.53377539324356, time: 130.262
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.37    -0.655  -17.3884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.925   -0.375  -10.7533]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.      -0.615   -9.0374]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.085   -0.3     -9.9951]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -266.54223236465987, time: 133.663
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.62   -0.72  -17.465]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.02    -0.23    -7.1744]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.955  -0.345  -4.4552]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.09   -0.45   -9.648]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -218.39562614406262, time: 130.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.04    -0.69   -14.3331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.485   -0.115   -5.7723]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.935  -0.335  -5.0247]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.875   -0.39    -9.9992]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -207.96538845519228, time: 132.184
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.72    -0.71   -11.8639]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.74   -0.035  -1.6487]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.15   -0.315  -3.8686]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.515 -0.22  -2.58 ]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -182.7819533405809, time: 131.728
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.2     -0.74   -10.2959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.545  -0.02   -1.7789]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.71   -0.31   -4.8985]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.37   -0.16   -1.9227]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -181.87587539952642, time: 131.855
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.825   -0.665   -9.2578]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.79   -0.045  -2.2035]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.39   -0.35   -3.8551]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.46   -0.34   -4.5184]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -176.8283656816888, time: 131.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.855   -0.61    -9.2627]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.97   -0.015  -1.1709]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.39   -0.38   -4.2331]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.87   -0.42   -5.5188]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -167.40704951287765, time: 130.984
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.055  -0.705 -10.826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.83   -0.015  -1.0885]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.83   -0.45   -4.3494]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.445   -0.46    -6.6838]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -163.2801075410555, time: 132.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.665   -0.56    -9.6363]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.98   -0.085  -3.2304]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.06   -0.48   -5.0092]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.715   -0.48    -7.8285]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -148.13775601824938, time: 131.552
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.415   -0.645  -12.2651]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.06   -0.045  -1.5213]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.345  -0.6    -5.0469]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.985   -0.54    -7.9194]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -153.2323382251667, time: 130.646
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.34    -0.615  -11.0668]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.37   -0.025  -1.1079]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.555   -0.64    -8.1924]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.895  -0.415  -5.3346]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -141.51748708740078, time: 130.593
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.69    -0.66   -12.2637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.605  -0.095  -1.2689]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.13    -0.605   -8.8269]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.01    -0.495   -5.8152]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -149.2925214220902, time: 131.136
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.06    -0.61   -12.8862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.935  -0.24   -3.7681]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.93    -0.54    -8.1871]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-3__2018-07-12_00-28-04...
200 50
steps: 9950, episodes: 200, mean episode reward: -380.15853232087727, time: 81.325
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.89949749  -0.54271357 -12.62954774]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.98492462  -0.31155779  -8.46361809]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.18592965  -0.49246231 -11.7998995 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.74874372  -0.38693467 -10.59557789]
400 50
steps: 19950, episodes: 400, mean episode reward: -372.52816865344704, time: 83.441
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.025   -0.46   -12.5094]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.585   -0.325   -8.3277]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.935   -0.49   -12.2313]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.535   -0.485  -10.6277]
600 50
steps: 29950, episodes: 600, mean episode reward: -354.18007346722254, time: 82.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.29    -0.405  -12.0817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.185   -0.38    -8.6891]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.005   -0.48   -11.8181]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.835  -0.455 -10.748]
800 50
steps: 39950, episodes: 800, mean episode reward: -368.45558252240977, time: 82.895
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.745   -0.545  -12.4458]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.135   -0.32    -8.0979]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.63    -0.455  -12.1593]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.61    -0.435  -10.6894]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -370.5037445627553, time: 83.123
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.1     -0.495  -12.6152]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.265   -0.385   -8.6445]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.34    -0.455  -11.9179]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.24    -0.46   -10.9876]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1124.7258635792837, time: 127.778
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.045   -0.61   -13.3233]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.91    -0.195   -8.5826]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.62    -0.74   -21.1678]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.975   -0.57   -15.1668]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -315.0024389391898, time: 132.194
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.045   -0.59   -10.0025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.42    -0.38    -8.9917]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.975   -0.365  -14.5493]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.56   -0.295  -8.984]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -297.2143339565411, time: 134.172
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.3    -0.34   -3.9545]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.275   -0.41    -9.1497]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.835   -0.35    -9.1818]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.35    -0.35    -8.9867]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -258.0437968987468, time: 131.951
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.9     -0.43    -7.8797]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.99   -0.135  -2.3253]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.07    -0.535  -11.1524]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.425   -0.34    -7.8983]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -202.55162028832677, time: 132.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.72    -0.485   -6.8749]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.215  -0.085  -0.1844]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.75    -0.555  -12.2723]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.935   -0.485  -14.4232]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -159.91394859590105, time: 130.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.785   -0.47    -8.3228]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.275  -0.05   -0.2036]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.7     -0.665  -16.2515]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.49    -0.61   -15.6962]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -140.95110383319846, time: 130.524
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.615  -0.495 -14.004]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.345  -0.06   -0.2682]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.895  -0.765 -17.878]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.645   -0.665  -15.7451]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -152.70908900618554, time: 130.507
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.425   -0.41   -16.0331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.34   -0.03   -0.2267]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.995   -0.735  -18.5317]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.27    -0.6    -14.2438]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -143.22176543966154, time: 129.536
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.59    -0.44   -14.9771]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.365  -0.075  -0.2689]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.96    -0.73   -18.3862]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.61    -0.51   -14.2631]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -138.72272400826716, time: 130.726
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.25    -0.47   -16.8938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.665  -0.08   -0.4518]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.975   -0.725  -18.0516]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.9     -0.66   -16.9642]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -131.21933174041914, time: 131.174
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.52    -0.365  -11.6973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.425  -0.08   -0.3521]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.81    -0.715  -17.5751]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.165   -0.72   -19.2938]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -148.12114432484293, time: 129.804
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.72    -0.3    -10.5586]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.265 -0.06  -0.227]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.47    -0.77   -15.8942]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.08    -0.855  -20.1789]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -122.18294569626316, time: 130.008
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.675   -0.33   -11.0093]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.235  -0.04   -0.1971]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.115  -0.655 -16.575]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.67   -0.775 -20.835]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -124.57444104821326, time: 130.365
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.04    -0.21    -7.0221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.425  -0.075  -0.3686]
agent2_energy_min, agent2_energy_max, agent2_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-3__2018-07-12_00-27-58...
200 50
steps: 9950, episodes: 200, mean episode reward: -371.34059324114077, time: 82.816
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.74874372  -0.48743719 -11.84884422]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.50251256  -0.50251256 -12.17386935]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.55276382  -0.59296482 -14.49075377]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.81407035  -0.56281407 -13.21628141]
400 50
steps: 19950, episodes: 400, mean episode reward: -367.82209685622024, time: 82.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.875   -0.51   -11.7327]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.16    -0.465  -11.9603]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.555   -0.54   -15.0762]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.765   -0.51   -13.2181]
600 50
steps: 29950, episodes: 600, mean episode reward: -388.1665099849005, time: 82.597
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.33    -0.41   -11.5354]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.84    -0.495  -12.2751]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.895  -0.475 -14.575]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.44    -0.52   -13.3838]
800 50
steps: 39950, episodes: 800, mean episode reward: -340.5001397239591, time: 82.684
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.12   -0.495 -12.056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.065   -0.465  -12.2648]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.125   -0.565  -14.7833]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.      -0.555  -13.2832]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -357.4345050812249, time: 83.13
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.875   -0.485  -11.7721]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.485   -0.5    -12.5873]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.855   -0.565  -14.5823]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.28    -0.46   -13.3032]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1207.5299489862318, time: 128.015
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.28    -0.385   -8.7153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.775   -0.69   -16.7199]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.34    -0.29    -6.7188]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.125   -0.36   -10.8206]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -542.2428015759104, time: 132.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.565  -0.405  -8.679]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.425   -0.375   -7.9349]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.84    -0.3     -7.3719]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.37    -0.68   -17.5048]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -415.3368157600461, time: 132.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.15    -0.54   -11.6967]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.54    -0.705  -16.7157]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.74    -0.315   -9.8194]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.135   -0.325   -8.8435]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -268.5824298143519, time: 131.207
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.795   -0.435   -8.0796]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.415   -0.6    -17.0693]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.075 -0.235 -3.193]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.875  -0.65  -13.265]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -200.95818678653694, time: 132.354
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.01    -0.375   -5.2003]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.96    -0.58   -10.4254]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.71   -0.12   -4.3738]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.62    -0.67   -21.1333]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -235.90287684354223, time: 133.524
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.25   -0.215  -2.3226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.94    -0.495   -7.8703]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.8    -0.105  -1.9011]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.26    -0.575  -19.7968]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -224.29092261548823, time: 131.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.685  -0.315  -5.0378]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.295   -0.565  -10.8206]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.16   -0.085  -2.1906]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.59    -0.5    -18.3405]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -216.09049469091522, time: 132.095
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.545  -0.205  -2.8312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.32    -0.585  -10.3519]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.42   -0.13   -2.0777]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.3     -0.445  -16.8268]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -217.98686287689924, time: 131.204
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.485  -0.3    -3.3612]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.875   -0.54   -10.5459]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.29    -0.155   -8.4911]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.94    -0.4    -13.4874]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -220.14285767433321, time: 130.763
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.73   -0.285  -3.5822]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.46    -0.44    -7.9928]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.955   -0.11    -6.7208]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.275   -0.44    -9.9323]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -219.88828387691194, time: 132.343
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.525 -0.365 -4.984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.435   -0.535  -11.2885]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.63   -0.14   -4.3727]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.135   -0.405   -8.1383]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -206.53584155629272, time: 131.068
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.47   -0.24   -4.9966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.555   -0.585  -13.1134]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.8    -0.105  -3.4125]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.33    -0.41   -10.0033]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -183.08905304578045, time: 129.855
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.12    -0.4     -8.2055]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.47    -0.555  -11.2545]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.89    -0.16    -6.5347]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.125   -0.44   -11.5435]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -188.40753723525714, time: 130.484
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.595   -0.405  -12.5643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.185   -0.565  -12.4906]
agent2_energy_min, agent2_energy_max, agent2_energy_avgUsing good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-3__2018-07-12_00-27-56...
200 50
steps: 9950, episodes: 200, mean episode reward: -382.6849814458152, time: 81.181
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.70854271  -0.53266332 -12.16090452]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.73869347  -0.36683417 -10.26532663]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.65829146  -0.43718593 -10.30271357]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.53266332  -0.57788945 -14.78613065]
400 50
steps: 19950, episodes: 400, mean episode reward: -347.8364073620692, time: 82.023
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.105   -0.525  -12.3685]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.9    -0.425 -10.429]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.565   -0.41   -10.2791]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.825   -0.495  -14.2832]
600 50
steps: 29950, episodes: 600, mean episode reward: -377.58527019301806, time: 82.054
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.715   -0.535  -12.8067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.73    -0.41   -10.0875]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.87    -0.35   -10.3173]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.255   -0.62   -14.6951]
800 50
steps: 39950, episodes: 800, mean episode reward: -356.3309405328707, time: 82.867
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.92    -0.475  -12.1857]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.84    -0.44    -9.9485]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.575  -0.365 -10.254]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.42    -0.52   -14.5885]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -358.8642213813805, time: 83.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.42    -0.48   -12.5265]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.555   -0.385  -10.2295]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.07    -0.405  -10.4645]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.505   -0.53   -14.5609]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1015.6547664576419, time: 128.388
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.61    -0.575  -14.2397]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.64    -0.485  -12.7792]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.815 -0.24  -4.006]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.655   -0.565  -14.9132]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -443.85810845913716, time: 131.863
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.28    -0.38    -6.1714]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.035   -0.64   -17.0359]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.635   -0.415  -11.6644]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.      -0.645  -16.1694]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -403.9712752334063, time: 133.897
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.41   -0.51  -15.952]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.235   -0.555  -12.7315]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.71   -0.275  -5.638]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.69    -0.285   -5.9184]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -207.34592431918142, time: 131.77
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.735   -0.57   -15.3926]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.805  -0.485 -10.185]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.805  -0.23   -3.4754]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.055  -0.16   -2.3477]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -224.0336408521473, time: 132.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.23    -0.285   -8.9694]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.345   -0.505  -14.2683]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-1.075  -0.135  -0.7168]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.96   -0.1    -0.7373]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -184.69673939367036, time: 133.701
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.155   -0.385  -11.2423]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.27    -0.475  -13.9846]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.68   -0.15   -2.0235]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.89   -0.08   -0.6945]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -164.1199080856386, time: 132.019
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.565   -0.395  -11.7889]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.66    -0.615  -16.1047]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-0.35   -0.09   -0.2745]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.985  -0.095  -0.6311]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -181.1916087027933, time: 131.258
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.6     -0.28    -6.9056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.99    -0.65   -17.8998]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-0.87   -0.105  -0.5628]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.85   -0.14   -1.0904]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -209.72765867578156, time: 132.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.14    -0.56   -14.4617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.755   -0.59   -15.5768]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-1.14   -0.085  -0.5869]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.065  -0.1    -0.7155]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -172.37477952988635, time: 131.707
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.085   -0.365  -10.5313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.315   -0.735  -18.2267]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-2.44   -0.115  -1.2571]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.01   -0.12   -1.4371]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -177.52479437724767, time: 132.784
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.68    -0.21    -5.7186]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.9    -0.725 -15.957]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.22   -0.12   -1.5624]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.2    -0.11   -1.2999]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -191.64139804266983, time: 131.074
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.345   -0.285   -8.2785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.375  -0.76  -18.575]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-2.265  -0.105  -1.1364]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.485 -0.12  -1.242]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -158.02787530308265, time: 130.383
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.865   -0.675  -16.9897]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.065   -0.715  -17.8926]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-2.835  -0.115  -1.4243]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.09   -0.07   -0.6408]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -170.85385916765918, time: 131.041
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.95   -0.26   -9.454]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.4     -0.755  -19.3322]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-1.58   -0.12   -0.9689]Using good policy ddpg and adv policy ddpg
Starting iterations of wanderer1_4agents-3__2018-07-12_00-28-02...
200 50
steps: 9950, episodes: 200, mean episode reward: -351.8319306559416, time: 82.216
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.46231156  -0.51758794 -13.82261307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.31658291  -0.42713568 -10.54482412]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.53266332  -0.46733668 -12.96040201]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.20603015  -0.50753769 -11.37678392]
400 50
steps: 19950, episodes: 400, mean episode reward: -364.3233829240767, time: 82.317
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.16    -0.535  -13.5719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.065  -0.415 -10.853]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.21    -0.545  -13.2676]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.02    -0.485  -11.1357]
600 50
steps: 29950, episodes: 600, mean episode reward: -359.34245739452, time: 83.123
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.02    -0.53   -13.6331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.105  -0.495 -10.847]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.38    -0.56   -13.3994]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.775   -0.425  -11.4479]
800 50
steps: 39950, episodes: 800, mean episode reward: -359.89631984477586, time: 81.656
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.235   -0.515  -13.6976]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.34    -0.455  -11.0796]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.17    -0.48   -13.3315]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.035  -0.425 -11.206]
1000 50
steps: 49950, episodes: 1000, mean episode reward: -364.52202772912017, time: 83.132
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.485   -0.53   -13.8066]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.41    -0.41   -10.6483]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.285   -0.46   -13.3577]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.125   -0.42   -11.1684]
1200 50
steps: 59950, episodes: 1200, mean episode reward: -1339.3519142172652, time: 128.382
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.51   -0.62  -16.213]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.86   -0.29   -5.1785]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.31   -0.435 -15.949]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.2     -0.56   -15.1012]
1400 50
steps: 69950, episodes: 1400, mean episode reward: -383.4421943870998, time: 131.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.695   -0.31    -6.9178]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.87    -0.495  -15.4727]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.935   -0.46   -11.6996]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.28    -0.3     -8.1086]
1600 50
steps: 79950, episodes: 1600, mean episode reward: -286.20763355494023, time: 134.795
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.92    -0.74   -20.4282]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.745   -0.36    -8.1509]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.45   -0.34   -5.3235]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.245   -0.59   -14.2701]
1800 50
steps: 89950, episodes: 1800, mean episode reward: -282.9658964220563, time: 132.093
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.1    -0.545 -19.473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.455  -0.3    -3.0021]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.73   -0.355  -3.2805]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.05    -0.8    -20.3327]
2000 50
steps: 99950, episodes: 2000, mean episode reward: -250.4564439994371, time: 132.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.715   -0.455  -13.8317]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.89   -0.175  -2.4541]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.57   -0.385  -2.7156]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.23    -0.61   -17.3679]
2200 50
steps: 109950, episodes: 2200, mean episode reward: -201.62621763746895, time: 132.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.33    -0.53   -10.3232]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.315 -0.155 -1.916]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-2.145  -0.225  -1.3161]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.35   -0.44  -13.919]
2400 50
steps: 119950, episodes: 2400, mean episode reward: -212.97546558668355, time: 132.536
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.055   -0.415   -6.8631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.445  -0.14   -1.0063]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.985  -0.315  -2.9145]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.72    -0.43   -12.6903]
2600 50
steps: 129950, episodes: 2600, mean episode reward: -242.39612530900087, time: 131.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.08    -0.345   -6.5006]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.47   -0.11   -0.8773]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.515  -0.345  -4.2978]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.17   -0.34  -15.999]
2800 50
steps: 139950, episodes: 2800, mean episode reward: -201.94865879683897, time: 131.031
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.795  -0.245  -5.3642]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.64  -0.165 -3.97 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.35   -0.275  -2.3208]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.23    -0.415  -15.6984]
3000 50
steps: 149950, episodes: 3000, mean episode reward: -204.43150845438387, time: 131.832
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.375  -0.415  -5.5279]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.01   -0.15   -4.0216]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.975 -0.28  -2.435]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.45   -0.39  -15.042]
3200 50
steps: 159950, episodes: 3200, mean episode reward: -193.30260870389247, time: 132.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.445  -0.26   -3.6707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.445  -0.215  -4.6116]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.415  -0.3    -3.6218]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.665   -0.375  -14.2776]
3400 50
steps: 169950, episodes: 3400, mean episode reward: -176.15559320353583, time: 132.308
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.665  -0.07   -1.0859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.54   -0.115  -3.6162]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.045   -0.31    -6.5637]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.525   -0.325  -14.3138]
3600 50
steps: 179950, episodes: 3600, mean episode reward: -178.95901375882167, time: 130.955
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.47   -0.19   -2.6164]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.12   -0.07   -3.1278]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.71    -0.315   -6.0602]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.87    -0.3    -13.3102]
3800 50
steps: 189950, episodes: 3800, mean episode reward: -169.73767942885456, time: 130.32
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.145  -0.17   -1.9372]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.125  -0.175  -3.3125]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.895   -0.275   -8.8189]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.625  -0.3    -7.434]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.795   -0.6    -20.5644]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -158.4286644395571, time: 125.007
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.895   -0.595  -18.9258]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.65    -0.375  -14.5246]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.845   -0.265   -6.5247]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.655  -0.63  -19.296]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -159.69926848583657, time: 125.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.575   -0.515  -16.6931]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.345   -0.42   -14.7907]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.02   -0.23   -4.5562]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.615   -0.475  -17.6876]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -182.09063621599762, time: 124.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.47    -0.445  -17.7859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.57   -0.375 -14.312]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.02   -0.255  -3.5833]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.35    -0.55   -20.1799]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -173.9396496959168, time: 124.475
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.695   -0.56   -18.7357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.025   -0.35   -14.4641]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.78   -0.275  -4.6997]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.665   -0.705  -22.5054]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -160.12146522492398, time: 128.647
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.525   -0.51   -17.4719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.07   -0.46  -16.817]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.605  -0.295  -4.2723]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.475   -0.575  -20.6256]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -168.20951535238643, time: 126.132
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.575   -0.545  -19.3654]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.005   -0.385  -14.9308]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.155   -0.345   -5.8158]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.885   -0.57   -20.8577]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -169.64702748463662, time: 128.158
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.08    -0.585  -20.3254]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.945   -0.405  -15.0204]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.365   -0.34    -5.6699]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.86    -0.635  -22.0994]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -153.74574921294345, time: 126.537
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.72    -0.495  -20.0532]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.455  -0.395 -15.109]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.805  -0.31   -4.5496]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.925   -0.7    -22.1725]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -171.97110343742182, time: 127.497
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.415   -0.575  -19.7914]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.855   -0.39   -14.4255]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.385  -0.255  -4.1798]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.035   -0.61   -19.7587]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -156.3200026012515, time: 125.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.005   -0.65   -19.2312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.705   -0.355  -11.4162]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.98   -0.37   -4.9713]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.82   -0.665 -21.042]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -161.48685614814437, time: 126.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.37    -0.58   -19.4938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.21    -0.43   -11.9788]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.855  -0.47   -6.1445]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.05    -0.705  -22.9157]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -143.1032442056828, time: 126.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.83    -0.54   -16.6673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.78   -0.39  -13.266]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.535 -0.3   -3.179]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.605   -0.745  -23.7248]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -156.5654988177272, time: 126.047
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.41    -0.64   -17.0589]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.275   -0.39   -14.3428]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.405  -0.45   -5.0105]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.735   -0.785  -22.2822]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -146.03652806168407, time: 125.744
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.75    -0.565  -15.1782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.69    -0.54   -15.8434]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.47   -0.46   -4.3366]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.885   -0.775  -22.1318]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -147.88375393003983, time: 125.665
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.465   -0.6    -15.7181]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.78    -0.485  -15.5612]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.26  -0.365 -4.272]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.8     -0.695  -21.9364]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -136.8499322528423, time: 126.492
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.925   -0.665  -15.3465]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.26    -0.44   -15.7396]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.58   -0.325  -3.3832]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.22    -0.74   -22.0099]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -144.9079114128286, time: 128.227
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.46    -0.71   -17.9415]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.165   -0.475  -16.5566]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.67   -0.4    -4.5996]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.52    -0.775  -22.6665]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -136.56988212053338, time: 124.913
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.805   -0.67   -16.6871]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.85   -0.455 -16.114]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.63   -0.345  -2.8593]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.51    -0.78   -21.9105]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -135.84144788077606, time: 126.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.98    -0.685  -18.4498]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.395   -0.535  -17.7247]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.07   -0.32   -3.0474]
[-40.8     -0.785  -20.5926]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.42   -0.35   -1.6404]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -192.95618105274946, time: 126.57
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.96   -0.465  -2.7933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.75    -0.43   -15.2201]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.055   -0.805  -21.6232]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.025  -0.25   -0.8032]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -165.57247032446526, time: 128.623
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.3    -0.4    -1.7979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.575   -0.49   -17.1166]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.17    -0.875  -22.6835]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.005  -0.265  -2.1065]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -179.48915161035046, time: 126.448
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.365  -0.37   -2.9573]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.655   -0.515  -17.5982]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.76    -0.89   -21.3731]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.935  -0.15   -1.3055]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -160.23790045817776, time: 127.61
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.13   -0.36   -3.1267]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.76   -0.495 -17.459]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.875   -0.865  -24.0407]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.825  -0.155  -1.2832]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -172.21707499142462, time: 126.789
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.43    -0.43    -7.2612]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.31    -0.41   -17.1622]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.1     -0.835  -23.6655]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.075  -0.175  -1.4013]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -155.4525966090493, time: 127.337
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.23   -0.32   -5.116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.505   -0.42   -15.0671]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.435   -0.815  -23.1506]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.63   -0.23   -2.9435]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -156.12719987198682, time: 128.676
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.745   -0.285   -5.5382]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.93    -0.53   -17.9025]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.705  -0.94  -24.08 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.68   -0.305  -4.4573]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -150.16146623491892, time: 128.001
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.485   -0.415   -6.4803]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.575   -0.54   -18.0822]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.545   -0.875  -23.7287]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.435  -0.225  -3.8202]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -137.75386099957387, time: 127.984
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.94   -0.285  -3.1055]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.735   -0.525  -16.1101]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.645   -0.85   -23.7594]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.06   -0.245  -3.8839]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -129.76414718679342, time: 127.957
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.885   -0.32    -4.7044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.54   -0.445 -14.605]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.89    -0.845  -23.8689]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.8    -0.225  -4.0862]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -139.23182419554246, time: 128.531
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.14    -0.315   -4.5036]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.53  -0.48 -14.32]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.475   -0.87   -23.7627]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.145  -0.165  -2.5568]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -139.93924881933788, time: 129.509
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.805  -0.31   -2.7441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.88   -0.52  -14.727]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.545   -0.88   -24.4905]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.875  -0.19   -3.8867]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -125.18082137929571, time: 126.921
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.035  -0.38   -3.8475]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.455   -0.695  -18.0981]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.25    -0.9    -24.2599]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.835  -0.165  -3.9125]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -128.65954122508788, time: 129.176
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.735  -0.455  -3.9156]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.59   -0.625 -16.349]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.725   -0.895  -24.5044]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.865   -0.24    -7.7131]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -119.84033492499107, time: 128.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.34  -0.295 -2.241]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.955   -0.63   -16.4399]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.605   -0.905  -24.3829]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.52   -0.09   -4.8108]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -123.20459285221027, time: 126.192
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.52   -0.235  -1.6459]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.955   -0.565  -16.8516]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.855   -0.915  -24.5933]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.58    -0.175   -5.5046]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -104.43143282811842, time: 128.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.1    -0.255  -2.1656]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.585  -0.62  -16.73 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.795   -0.95   -24.6572]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.245 -0.14  -5.303]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -127.80233543354625, time: 127.488
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.45   -0.285  -1.7376]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.575   -0.645  -18.9909]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.8     -0.945  -24.7076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.835  -0.185  -5.3045]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -124.21819648967032, time: 127.882
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.11   -0.315  -2.1498]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.44    -0.475  -17.1998]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.      -0.935  -24.7933]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.11    -0.89   -24.2459]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.34    -0.835  -24.4787]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -122.83135978437103, time: 125.162
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.57   -0.285  -1.9511]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.1    -0.14   -1.9624]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.24    -0.91   -25.0307]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.66    -0.88   -24.5562]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -110.43498871431973, time: 124.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.4    -0.23   -1.7029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.865  -0.145  -3.1243]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.695   -0.975  -25.2829]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.665   -0.875  -24.1174]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -123.1425339847826, time: 124.77
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.195  -0.105  -0.9003]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.615  -0.185  -3.2541]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.59    -0.975  -25.1963]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.71    -0.845  -24.0288]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -120.1834866749241, time: 123.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.315  -0.125  -1.3705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.805  -0.13   -3.1348]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.635   -0.98   -25.2393]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.585   -0.875  -23.4604]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -110.60274532366645, time: 123.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.955  -0.21   -1.4585]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.93   -0.29   -3.3406]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.66    -0.985  -25.2615]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.82    -0.835  -24.0294]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -114.97492277259687, time: 124.165
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.24   -0.17   -0.9649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.27   -0.335  -5.0848]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.84    -0.98   -25.3783]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.545   -0.885  -24.0452]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -109.37849625532291, time: 123.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.01   -0.11   -0.7128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.355  -0.395  -4.4065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.43    -0.975  -25.0845]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.69    -0.965  -24.6644]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -106.63675681338394, time: 124.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.225  -0.24   -2.1802]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.17   -0.385  -4.2484]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.575   -0.995  -25.1904]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.88    -0.935  -24.7686]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -97.99528276320079, time: 125.872
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.565  -0.245  -2.3593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.79   -0.295  -3.2984]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.595   -1.     -25.1878]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.455   -0.94   -25.0675]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -107.39447500020177, time: 124.016
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.995  -0.3    -3.2233]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.59  -0.3   -3.281]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.42    -1.     -25.0588]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.425   -0.95   -25.1019]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -103.74746074660885, time: 125.418
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.215 -0.26  -2.848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.855  -0.23   -3.3834]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.115   -0.995  -24.8119]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.605   -0.96   -25.2356]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -107.54204507684885, time: 125.264
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.185  -0.185  -0.9538]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.505  -0.175  -3.0274]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.53    -1.     -25.1401]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.58    -0.96   -24.5434]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -105.19293895046025, time: 125.393
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.155  -0.18   -0.9489]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.31   -0.16   -2.4001]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.39    -0.985  -25.0362]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.11    -0.98   -24.9008]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -105.5213710422468, time: 124.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.335  -0.3    -1.8329]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.725  -0.205  -2.1729]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.175   -0.96   -24.9198]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.29    -0.98   -25.0006]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -97.13679927608896, time: 124.478
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.92   -0.305  -1.6874]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.495  -0.185  -2.5583]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.69    -0.995  -25.2779]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.785   -0.93   -24.6055]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -105.77888383652584, time: 124.179
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.255  -0.215  -1.1052]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.02   -0.19   -2.2511]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.71    -0.985  -25.2946]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.825   -0.89   -24.5868]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -101.41882830971126, time: 126.913
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.475  -0.245  -1.2629]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.95   -0.17   -2.2441]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.76    -0.995  -25.3449]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.38    -0.94   -25.0162]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -91.05554706533434, time: 124.012
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.71   -0.31   -2.0925]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.315  -0.19   -2.3976]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.795  -0.97  -25.337]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.775  -0.985 -25.323]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -100.26683336517064, time: 124.402
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.21   -0.19   -1.0496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.115  -0.145  -1.6271]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.665   -0.96   -25.2474]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.55  -0.25  -2.301]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.755   -0.715  -11.0547]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -136.201276557261, time: 123.166
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.6     -0.715  -13.8747]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.27    -0.68   -16.5648]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.945   -0.43   -10.1226]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.21    -0.78   -13.9652]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -144.78070366309615, time: 125.225
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.61   -0.755 -15.344]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.205   -0.585  -15.7968]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.035   -0.395   -8.3455]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.23    -0.815  -19.5152]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -148.1399607030199, time: 124.26
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.      -0.805  -15.3291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.85    -0.59   -15.7184]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.38    -0.435  -12.7174]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.675   -0.71   -12.0471]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -143.5324088730428, time: 125.733
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.485   -0.72   -15.2601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.835   -0.565  -15.1573]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.69    -0.53   -15.5804]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.705   -0.765  -16.9637]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -137.19969049098495, time: 125.382
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.22    -0.765  -17.4438]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.475   -0.485  -15.8821]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.63   -0.54  -14.495]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.985   -0.755  -17.5216]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -141.25765628980537, time: 124.274
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.17    -0.675  -11.0719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.      -0.75   -20.7077]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.99    -0.55   -17.2241]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.455   -0.79   -19.1306]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -130.9970129515628, time: 125.503
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.13    -0.785  -15.7154]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.925   -0.695  -21.0125]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.845  -0.525 -18.764]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.135   -0.865  -20.9593]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -127.33627109283353, time: 126.641
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.335   -0.78   -15.3721]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.31    -0.78   -22.5818]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.295   -0.555  -18.6277]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.295   -0.83   -18.0039]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -129.61305719594995, time: 126.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.165  -0.795 -13.831]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.525   -0.785  -23.0425]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.75    -0.53   -16.9406]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.84    -0.855  -21.1283]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -119.16226596353346, time: 126.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.22    -0.795  -15.0188]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.215   -0.675  -23.1796]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.615   -0.42   -16.6296]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.49    -0.895  -22.6088]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -126.0395230706659, time: 127.404
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.885   -0.625  -10.1155]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.89    -0.905  -24.7507]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.835   -0.45   -18.2136]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.28    -0.76   -18.8242]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -129.4095202167242, time: 126.495
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.66    -0.67   -11.5425]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.925   -0.74   -23.7458]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.925   -0.405  -17.5489]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.395   -0.89   -22.3921]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -99.72572302100018, time: 125.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.51    -0.75   -14.4791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.21    -0.835  -24.1411]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.895   -0.59   -19.5253]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.16   -0.875 -20.129]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -110.52241590406368, time: 125.12
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.105   -0.86   -14.6609]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.925   -0.64   -22.5137]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.125   -0.705  -22.7481]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.585   -0.855  -21.7924]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -108.77131381061139, time: 125.669
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.21    -0.725  -12.7713]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.025   -0.81   -23.9031]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.34    -0.575  -20.7682]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.865   -0.79   -20.4721]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -112.96595823870095, time: 124.343
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.925  -0.715 -10.868]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.645   -0.725  -22.8263]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.82    -0.625  -20.7773]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.985   -0.95   -22.3861]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -109.51191719271868, time: 127.047
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.22    -0.775  -12.1206]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.005   -0.87   -23.8862]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.295   -0.565  -20.1795]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.475   -0.94   -22.9323]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -107.35077097133693, time: 125.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.39    -0.715  -14.1201]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.22    -0.84   -23.2441]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.625   -0.65   -21.1103]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.68    -0.975  -22.7667]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -101.39684261065435, time: 125.142
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.04    -0.8    -14.5899]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.645   -0.76   -22.6727]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.995   -0.685  -17.2592]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.38    -0.65   -14.6963]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -253.88526736418993, time: 126.899
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.685  -0.14   -1.5785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.345   -0.48    -9.0994]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.055   -0.71   -15.1299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.29    -0.67   -13.5172]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -225.44740154344683, time: 129.09
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.045   -0.325  -10.8176]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.12    -0.45    -7.6119]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.66    -0.705  -14.7821]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.46    -0.585  -12.6638]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -237.6166610568542, time: 126.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.075  -0.35  -13.177]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.705   -0.48    -9.9223]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.28    -0.765  -15.0373]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.69    -0.66   -13.7076]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -201.66630976061205, time: 126.598
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.425  -0.21   -3.2635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.895   -0.555  -14.8352]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.565   -0.77   -15.5371]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.03    -0.595  -12.5938]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -220.28885124216637, time: 128.064
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.48   -0.19   -4.3796]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.49    -0.545  -15.1098]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.475   -0.845  -16.0177]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.69    -0.595  -12.0952]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -206.43634836007163, time: 128.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.725  -0.15   -1.8468]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.53    -0.44   -13.6641]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.62    -0.8    -17.9724]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.64    -0.62   -14.9039]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -185.0707872035405, time: 129.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.415  -0.11   -1.2758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.725   -0.475  -15.5432]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.425  -0.76  -17.473]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.61    -0.735  -18.4851]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -195.80971859408797, time: 128.033
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.29   -0.05   -0.1978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.335   -0.375  -13.7515]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.215   -0.775  -20.2039]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.535   -0.655  -16.8745]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -196.2496506337697, time: 127.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.635  -0.11   -0.8512]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.5     -0.44   -12.5576]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.435   -0.86   -20.1869]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.835   -0.765  -17.5461]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -176.83817726322442, time: 128.275
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.365  -0.055  -0.2892]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.345   -0.52   -16.5084]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.275   -0.83   -20.2776]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.67    -0.79   -20.6185]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -186.43164826307745, time: 128.86
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.32   -0.185  -1.4367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.97    -0.475  -13.7531]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.32    -0.78   -21.2684]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.055   -0.735  -18.6048]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -193.90466755779812, time: 127.712
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.81  -0.145 -1.025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.32    -0.48   -13.6518]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.75    -0.81   -21.1606]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.135  -0.715 -17.052]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -207.22668131494925, time: 127.514
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.515  -0.19   -1.0743]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.515  -0.45  -12.721]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.175   -0.86   -21.5387]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.03    -0.665  -17.4586]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -187.8761996284292, time: 128.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.15   -0.14   -0.8271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.115   -0.495  -12.5431]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.955   -0.82   -22.9921]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.205   -0.78   -19.4623]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -189.12721267214175, time: 128.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.47  -0.195 -1.076]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.745   -0.525  -14.1367]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.03    -0.895  -22.7474]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.105   -0.75   -16.7925]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -170.17727955248674, time: 126.539
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.085  -0.315  -1.5674]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.55    -0.445  -11.0881]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.56   -0.88  -23.519]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.32    -0.69   -15.0923]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -154.5596088874378, time: 128.044
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.6   -0.27  -1.284]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.085   -0.475  -14.9214]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.19    -0.89   -22.3208]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.735   -0.68   -15.5033]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -153.2741910371346, time: 126.864
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.695  -0.19   -0.6085]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.195   -0.405  -14.2151]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.81    -0.89   -21.8313]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.185   -0.745  -17.8286]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -151.76278661425772, time: 127.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.435 -0.255 -1.154]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.07    -0.48   -15.9551]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.175   -0.885  -23.0584]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.11    -0.95   -23.9136]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.355  -0.595 -18.64 ]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -116.76075514246841, time: 130.209
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.82  -0.28  -2.978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.35   -0.44   -6.544]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.765   -0.91   -24.0669]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.605   -0.61   -20.2965]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -120.33510418846299, time: 129.023
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.68   -0.38   -3.5764]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.32    -0.53    -9.1102]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.38    -0.905  -23.8038]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.45    -0.615  -21.5546]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -114.02128159806227, time: 128.143
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.465  -0.455  -5.2039]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.65   -0.55   -7.663]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.055   -0.915  -24.2049]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.62    -0.62   -20.9767]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -119.25558861852805, time: 128.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.68  -0.455 -4.346]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.82    -0.47    -6.3845]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.93    -0.915  -24.7263]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.94    -0.625  -23.1334]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -107.47591731432902, time: 128.657
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.015  -0.43   -4.0911]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.585   -0.535   -6.8616]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.61    -0.945  -24.5375]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.3     -0.805  -24.2138]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -104.81763494164886, time: 128.605
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.005  -0.445  -4.4676]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.93    -0.6     -7.5299]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.175   -0.92   -24.2225]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.95    -0.88   -24.1678]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -116.99631734849063, time: 129.939
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.735  -0.455  -5.8357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.66    -0.57    -6.2821]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.645   -0.895  -24.5374]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.63    -0.815  -23.8729]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -111.91742799318823, time: 129.002
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.875   -0.565   -7.0351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.7    -0.535  -5.9936]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.525   -0.915  -24.4563]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.165   -0.82   -23.7295]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -107.58947279274598, time: 130.466
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.83   -0.495  -5.0404]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.415  -0.51   -5.3851]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.775   -0.935  -24.6178]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.405   -0.82   -24.3008]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -103.06550127909406, time: 128.235
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.365 -0.52  -5.109]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.29    -0.555   -6.7253]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.855   -0.915  -24.7307]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.33    -0.825  -23.1636]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -92.83151928808493, time: 130.005
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.42   -0.425  -4.0015]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.44    -0.685  -10.4437]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.975   -0.935  -24.7688]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.4     -0.77   -23.2297]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -99.16638472625097, time: 130.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.615  -0.49   -5.1887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.855   -0.65    -7.7056]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.31   -0.93  -25.006]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.86    -0.74   -23.1667]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -93.25636019115237, time: 129.819
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.915 -0.5   -4.448]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.205   -0.555   -6.4616]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.145   -0.945  -24.9313]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.195   -0.865  -24.7963]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -87.88426351147972, time: 127.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.315  -0.44   -4.0815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.575 -0.53  -6.014]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.52    -0.975  -25.1303]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.21   -0.835 -24.849]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -92.84726288226383, time: 128.616
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.205  -0.48   -3.5121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.575   -0.595   -6.8379]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.985   -0.945  -24.8251]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.175   -0.8    -24.7962]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -88.42386719652892, time: 129.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.185 -0.505 -4.922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.805   -0.5     -7.2645]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.505   -0.985  -24.5918]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.305   -0.865  -24.9536]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -90.03409417384528, time: 129.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.515  -0.485  -3.9839]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.36   -0.6    -6.5433]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.635   -0.99   -24.7203]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.715   -0.94   -25.2679]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -94.98994396190386, time: 128.756
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.395   -0.59    -6.3075]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.7    -0.55   -5.7922]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.795  -0.975 -24.804]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.62    -0.995  -25.3065]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -86.00871398078193, time: 127.722
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.375   -0.555   -7.3946]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.74    -0.585   -7.0717]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.835  -0.1    -1.8937]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -142.27813150562577, time: 133.319
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.555   -0.625  -16.7975]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.4     -0.785  -17.6736]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-2.67   -0.1    -1.3759]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.155  -0.04   -1.0057]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -127.42280205977622, time: 130.932
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.355   -0.505  -13.4521]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.63   -0.78  -17.729]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.11   -0.27   -2.9194]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.385  -0.03   -0.7144]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -142.5855770194029, time: 132.969
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.805   -0.635  -14.7264]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.57    -0.85   -22.3722]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.36   -0.235  -2.6642]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.435  -0.065  -0.8853]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -133.04921275811867, time: 133.005
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.975   -0.65   -17.9237]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.39    -0.88   -22.7664]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.175  -0.24   -2.6763]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.225  -0.125  -2.3157]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -129.938936420935, time: 127.864
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.275   -0.735  -18.8853]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.815   -0.935  -21.2986]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.41   -0.24   -2.3076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.835  -0.045  -0.9538]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -126.55125769530645, time: 128.849
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.26    -0.625  -16.6515]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.93    -0.905  -22.3052]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.11   -0.41   -7.706]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.62   -0.02   -0.3942]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -122.32985985077235, time: 128.157
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.16    -0.61   -16.3007]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.305   -0.91   -21.9885]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.66   -0.265  -4.2268]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.295  -0.005  -0.1296]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -121.07068987857716, time: 125.759
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.365   -0.73   -18.3689]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.25    -0.935  -22.1831]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.9    -0.25   -2.3853]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.535  -0.145  -1.6008]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -123.97672517409919, time: 125.416
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.72    -0.755  -17.7092]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.57    -0.935  -23.6618]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.625  -0.24   -3.5675]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.34   -0.01   -0.1984]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -138.35235218726876, time: 125.699
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.285   -0.735  -18.6195]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.03    -0.965  -22.6879]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.17   -0.295  -4.0507]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.945  -0.04   -0.5795]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -119.06029944657263, time: 125.65
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.915   -0.77   -17.2045]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.96    -0.975  -19.7853]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.08   -0.37   -4.0029]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.12   -0.02   -0.0898]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -104.20764282234981, time: 126.597
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.495   -0.76   -18.0806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.425   -0.965  -19.6782]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.295  -0.36   -3.8758]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.47   -0.045  -0.3031]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -107.64875562878444, time: 125.479
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.305   -0.76   -16.8801]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.64    -0.945  -19.6455]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.855  -0.435  -3.8784]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.535  -0.03   -0.3926]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -119.03515578637445, time: 127.125
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.715   -0.8    -16.2722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.445   -0.97   -19.8478]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.385  -0.32   -4.3309]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.72   -0.085  -1.1794]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -130.15413928626558, time: 125.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.96    -0.875  -16.8549]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.1     -0.975  -20.3098]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.525  -0.34   -3.2441]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.285  -0.02   -0.2467]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -114.80538678226813, time: 124.919
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.56    -0.82   -17.5879]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.515   -0.96   -17.1523]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.67    -0.385   -5.9167]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.44   -0.05   -0.3203]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -111.0502667403556, time: 125.343
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.74    -0.83   -18.8943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.91   -0.97  -18.759]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.44   -0.285  -3.0064]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.185  -0.015  -0.1637]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -103.21660483132615, time: 124.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.2     -0.805  -18.7887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.51    -0.975  -19.3132]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.72   -0.485  -5.6245]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.57   -0.065  -0.4653]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -113.82859808157045, time: 124.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.92    -0.73   -15.2719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.935   -0.96   -18.8382]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.28   -0.33   -3.4511]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.795   -0.27    -6.8127]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.895   -0.47   -11.3508]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -192.79469300255576, time: 131.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.95    -0.465  -15.2778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.8     -0.395   -6.5515]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.06    -0.14    -5.1739]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.09    -0.505  -10.2048]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -173.58171492581928, time: 132.512
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.94    -0.405  -14.0822]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.44    -0.48   -10.4293]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.54    -0.17    -5.3424]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.7     -0.545  -12.0108]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -175.63388006789458, time: 132.8
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.14    -0.59   -22.3978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.61    -0.375   -8.5134]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.255   -0.125   -8.5822]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.91    -0.385   -7.8281]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -187.29558678547707, time: 131.474
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.865   -0.435  -20.3494]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.2     -0.56   -13.4225]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.985   -0.215   -9.6898]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.23    -0.44   -10.5029]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -175.08890890865376, time: 130.875
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.785  -0.43  -19.704]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.37    -0.565  -13.2897]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.1     -0.165   -9.1894]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.745  -0.27   -6.204]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -178.61929132138997, time: 130.748
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.05    -0.525  -20.1229]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.955   -0.505  -12.8246]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.07    -0.25   -11.8304]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.885   -0.34    -8.4016]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -179.40471304098594, time: 133.418
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.235   -0.465  -19.9456]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.81    -0.475  -11.0219]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.      -0.225  -14.8993]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.26   -0.205  -4.6161]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -179.1037114252278, time: 132.555
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.4     -0.46   -19.3905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.99    -0.56   -11.0265]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.06    -0.19   -12.7371]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.28    -0.25    -6.3584]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -157.49062562624246, time: 130.958
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.7     -0.495  -20.1421]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.245   -0.45   -10.0387]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.575   -0.235  -13.5816]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.445   -0.185   -5.0635]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -174.5712734659219, time: 131.289
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.84    -0.73   -23.1909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.755   -0.525  -11.3783]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.095   -0.34   -16.3238]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.36    -0.265   -6.0751]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -158.94767952350495, time: 129.607
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.625   -0.66   -19.4307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.24    -0.43    -8.1415]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.03    -0.235  -11.3068]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.125   -0.375   -5.6038]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -144.22831183584682, time: 126.801
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.315   -0.725  -20.1707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.46    -0.48    -9.6827]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.48    -0.395  -17.0469]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.44    -0.38    -6.0287]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -155.82998000896248, time: 128.625
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.07    -0.73   -19.3887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.765   -0.43    -8.5436]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.35    -0.35   -14.3568]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.035  -0.34   -4.7348]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -149.42376829530994, time: 128.893
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.895   -0.61   -16.9145]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.11    -0.39    -6.6022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.055   -0.4    -16.1622]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.72   -0.325  -3.8919]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -152.70077555344892, time: 124.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.715   -0.565  -19.4935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.02    -0.39    -7.9877]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.99    -0.345  -13.0598]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.015   -0.41    -5.3799]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -148.5874382814403, time: 123.658
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.545   -0.71   -20.5842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.595   -0.51    -7.5292]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.535   -0.39   -13.9902]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.83   -0.375  -5.1654]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -143.24400875964793, time: 125.695
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.375   -0.695  -21.1528]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.995   -0.495   -8.3054]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.19   -0.485 -17.717]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.395  -0.305  -4.2938]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -143.9205984612981, time: 123.824
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.975   -0.75   -20.9564]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.73    -0.505   -6.3166]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.915   -0.43   -17.4956]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.265  -0.315  -3.6118]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -151.86073733864217, time: 123.934
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.535   -0.76   -21.7229]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.535   -0.49    -6.1248]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.26   -0.425  -4.8896]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -149.56284982269747, time: 131.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.     -0.705 -14.592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.19   -0.19   -4.5921]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.385   -0.525   -6.3935]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.145   -0.445   -7.0041]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -156.8579661372107, time: 132.196
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.135   -0.685  -12.7169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.075 -0.2   -4.447]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.905   -0.625  -10.8865]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.115  -0.385  -7.043]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -157.62904192907644, time: 132.497
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.49    -0.61   -13.1364]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.505  -0.16   -3.2884]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.99   -0.49   -6.348]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.61    -0.345   -5.1855]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -145.63009254695083, time: 131.236
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.59    -0.765  -14.2351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.305   -0.195   -5.6206]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.115   -0.67    -9.3787]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.01   -0.44   -9.527]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -133.7577394165481, time: 130.946
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.625   -0.73   -14.1984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.465   -0.25    -8.1614]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.575  -0.64  -11.895]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.555   -0.39    -7.1498]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -114.50889298617027, time: 130.531
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.725   -0.665  -14.9789]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.88  -0.07  -2.251]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.47    -0.68   -11.9612]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.995   -0.45    -8.4487]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -122.03605529326816, time: 131.095
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.095   -0.67   -14.7969]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.22    -0.24    -6.0083]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.975   -0.69   -12.3242]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.22    -0.505  -11.9589]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -130.33017918401106, time: 132.533
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.54   -0.615 -12.935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.92    -0.32    -7.3422]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.805   -0.7    -14.9644]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.755   -0.515  -11.8945]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -136.4685180083711, time: 128.816
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.355   -0.62   -16.9407]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.605   -0.295   -9.8515]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.935   -0.66   -13.4396]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.67    -0.56   -15.1838]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -121.67113593607084, time: 131.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.09    -0.675  -14.0134]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.295   -0.25    -7.7948]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.26    -0.705  -13.1434]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.335   -0.42    -9.4432]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -123.67758358079216, time: 131.881
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.34    -0.73   -12.7488]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.49    -0.24    -7.7306]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.23    -0.795  -13.9329]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.815   -0.57   -11.6372]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -123.19227809542653, time: 131.625
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.94    -0.7    -10.1835]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.96    -0.385  -10.7932]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.25    -0.68   -11.7302]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.045   -0.48   -11.8806]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -117.4427621399366, time: 130.529
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.945   -0.69   -11.7326]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.43    -0.415  -10.9082]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.715   -0.83   -17.7005]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.79    -0.375   -9.6089]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -107.28521258288049, time: 130.312
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.71    -0.665  -12.1395]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.03    -0.345   -9.8665]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.99    -0.77   -15.6117]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.415   -0.495  -12.1303]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -104.81948490203257, time: 129.731
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.105   -0.72   -10.6094]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.255   -0.34    -9.1609]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.235   -0.89   -16.1108]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.23    -0.46   -13.1591]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -116.52674909799552, time: 129.472
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.5     -0.675  -10.1984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.165   -0.5    -11.4721]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.4     -0.835  -14.1709]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.19    -0.54   -14.5144]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -117.00744586631384, time: 130.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.405   -0.72   -10.4403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.86    -0.395   -9.1863]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.935   -0.9    -16.2415]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.535   -0.625  -15.6111]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -101.1697661764906, time: 128.566
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.185  -0.74  -10.212]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.275   -0.36    -8.6044]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.095   -0.84   -15.8054]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.655   -0.56   -14.2432]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -103.09041062951816, time: 129.156
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.22    -0.775  -10.6858]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.56    -0.32   -10.4541]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.07    -0.77   -16.0286]
[-30.815   -0.735  -17.0454]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.195   -0.805  -19.9112]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -137.6907590400867, time: 131.47
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.14    -0.325   -9.3556]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.16   -0.135  -0.9059]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.74    -0.75   -17.3585]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.33    -0.725  -16.7109]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -143.02286642550632, time: 131.166
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.805   -0.27    -9.9782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.2    -0.03   -0.1738]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.605   -0.78   -18.2444]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.37    -0.655  -18.7643]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -122.09208902564411, time: 131.443
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.245  -0.385 -11.347]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.605  -0.055  -0.4994]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.21    -0.815  -16.8966]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.18    -0.78   -20.1026]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -113.7433514529999, time: 129.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.895   -0.4     -7.8285]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.99   -0.08   -0.7969]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.82   -0.87  -17.671]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.8     -0.835  -19.7645]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -108.47594978630623, time: 129.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.615   -0.325   -5.3699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.23   -0.015  -0.2031]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.87    -0.875  -18.1975]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.655   -0.82   -20.7499]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -110.99094185996252, time: 130.789
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.655   -0.285   -7.7708]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.755  -0.085  -0.6827]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.95    -0.89   -17.3047]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.495   -0.86   -23.3427]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -99.07044947401606, time: 131.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.755   -0.185   -4.7326]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.475 -0.06  -0.444]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.3     -0.85   -18.3843]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.005   -0.865  -21.7205]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -104.93826368766666, time: 131.942
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.725  -0.17   -2.5738]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.765  -0.22   -1.4664]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.305  -0.845 -18.12 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.7     -0.92   -23.0772]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -105.14706480632441, time: 130.887
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.87   -0.085  -1.1791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.51   -0.055  -0.4446]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.835   -0.85   -17.4174]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.135   -0.84   -22.9095]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -112.89939939566939, time: 130.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.655  -0.145  -1.2677]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.325  -0.165  -1.7565]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.72    -0.845  -17.5043]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.32    -0.795  -21.4289]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -102.11257828903243, time: 132.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.53   -0.145  -1.6583]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.37   -0.185  -1.2368]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.835   -0.915  -19.2099]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.655   -0.925  -21.5039]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -100.38170099223939, time: 130.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.41   -0.16   -1.0829]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.71   -0.225  -1.5122]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.355   -0.925  -20.6597]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.535   -0.95   -22.2817]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -97.63661570420122, time: 130.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.61  -0.15  -1.299]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.13   -0.15   -1.7527]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.255   -0.925  -19.7783]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.025   -0.94   -21.5756]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -95.65549749684966, time: 133.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.885  -0.23   -2.1003]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.13  -0.1   -0.954]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.895   -0.935  -20.8863]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.195   -0.93   -21.8519]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -99.93347003239347, time: 129.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.985  -0.145  -1.4619]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.25   -0.19   -1.8137]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.26    -0.94   -21.0956]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.57    -0.92   -20.9289]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -113.1344646798975, time: 131.164
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.22   -0.17   -1.2173]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.49  -0.255 -2.04 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.52    -0.97   -20.2527]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.34    -0.965  -22.2874]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -118.14138376136029, time: 130.233
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.445  -0.28   -2.9236]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.56   -0.26   -1.9041]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.325   -0.945  -19.5632]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.685   -0.985  -21.8745]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -106.06253136638628, time: 130.16
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.805  -0.305  -2.9267]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.325  -0.305  -2.0187]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.795   -0.955  -19.3638]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.02    -0.955  -19.6863]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -101.23306664431071, time: 130.9
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.515  -0.175  -1.3583]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.485  -0.255  -1.8399]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.205   -0.97   -18.5069]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.7     -0.45   -17.3322]
4000 50
steps: 199950, episodes: 4000, mean episode reward: -172.28663390164837, time: 133.415
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.495  -0.165  -2.4587]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.845  -0.18   -2.3819]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.63   -0.3    -6.802]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.725   -0.52   -16.1652]
4200 50
steps: 209950, episodes: 4200, mean episode reward: -164.25341805521876, time: 131.542
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.635   -0.33    -5.8352]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.79   -0.21   -3.3328]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.455   -0.36   -10.2944]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.815  -0.505 -17.616]
4400 50
steps: 219950, episodes: 4400, mean episode reward: -173.16917470064493, time: 132.165
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.46   -0.15   -3.3313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.015   -0.27    -8.3928]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.27    -0.285   -9.5049]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.07    -0.505  -18.3633]
4600 50
steps: 229950, episodes: 4600, mean episode reward: -169.95644113875832, time: 131.692
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.985  -0.16   -3.9243]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.28   -0.18   -4.4249]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.13    -0.33   -10.7713]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.595   -0.505  -18.9843]
4800 50
steps: 239950, episodes: 4800, mean episode reward: -174.68000038781372, time: 131.425
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.19    -0.25    -5.6494]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.315  -0.155  -4.4075]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.375   -0.44   -13.4702]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.545   -0.425  -14.0546]
5000 50
steps: 249950, episodes: 5000, mean episode reward: -146.76321509787232, time: 133.764
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.585  -0.185  -4.2225]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.015   -0.315   -5.7279]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.74    -0.41   -12.1771]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.38   -0.59  -21.128]
5200 50
steps: 259950, episodes: 5200, mean episode reward: -125.38884992077878, time: 133.358
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.61    -0.43    -7.4673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.74   -0.275  -3.5769]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.005   -0.465  -16.1925]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.235   -0.495  -17.7784]
5400 50
steps: 269950, episodes: 5400, mean episode reward: -127.6457885500956, time: 131.977
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.055  -0.21   -5.568]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.385  -0.305  -4.7582]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.97    -0.48   -13.9971]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.74    -0.6    -20.0404]
5600 50
steps: 279950, episodes: 5600, mean episode reward: -130.93299880641672, time: 132.219
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.665  -0.355  -8.302]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.86   -0.305  -4.1162]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.69    -0.565  -15.7086]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.53    -0.575  -20.3982]
5800 50
steps: 289950, episodes: 5800, mean episode reward: -131.03524392619462, time: 131.366
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.625   -0.335   -9.8187]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.39   -0.295  -4.7148]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.895   -0.595  -13.4384]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.85    -0.67   -21.5736]
6000 50
steps: 299950, episodes: 6000, mean episode reward: -126.75728732075372, time: 132.644
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.725   -0.325   -7.2982]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.695  -0.365  -5.8693]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.175  -0.67  -18.23 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.815   -0.7    -20.8551]
6200 50
steps: 309950, episodes: 6200, mean episode reward: -130.1934468771046, time: 132.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.115   -0.275   -7.5001]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.265  -0.265  -4.2301]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.06    -0.56   -19.2987]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.235   -0.51   -18.0114]
6400 50
steps: 319950, episodes: 6400, mean episode reward: -127.37570717073045, time: 130.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.02    -0.285   -7.3241]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.63    -0.405   -6.6139]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.07    -0.555  -16.2024]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.205   -0.64   -18.4583]
6600 50
steps: 329950, episodes: 6600, mean episode reward: -127.80984388340228, time: 130.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.325   -0.305   -9.0655]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.17   -0.32   -4.1594]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.785   -0.64   -19.5218]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.04    -0.65   -16.7867]
6800 50
steps: 339950, episodes: 6800, mean episode reward: -128.27483231372605, time: 132.073
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.835   -0.28    -6.9355]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.31   -0.37   -5.3758]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.84    -0.585  -19.3718]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.395   -0.685  -19.1647]
7000 50
steps: 349950, episodes: 7000, mean episode reward: -122.63768794654672, time: 130.442
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.405   -0.22    -8.2714]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.675  -0.445  -5.7346]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.265   -0.725  -21.1892]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.705   -0.55   -17.1245]
7200 50
steps: 359950, episodes: 7200, mean episode reward: -117.42094902054417, time: 132.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.035   -0.315   -7.3183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.88    -0.455   -8.1717]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.435   -0.78   -21.3356]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.335  -0.49  -18.881]
7400 50
steps: 369950, episodes: 7400, mean episode reward: -113.36405240717367, time: 130.267
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.23   -0.265  -7.864]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.395   -0.6     -8.1634]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.875   -0.695  -21.0616]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.95   -0.62  -18.639]
7600 50
steps: 379950, episodes: 7600, mean episode reward: -115.57017385880114, time: 130.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.425   -0.35   -11.6558]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.43    -0.53    -8.0292]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.48    -0.765  -22.0513]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.98    -0.73   -18.9582]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.785   -0.81   -22.5133]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -140.62335057870098, time: 125.063
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.93   -0.555 -12.444]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.535   -0.595  -17.3152]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.055  -0.44   -4.3109]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.935   -0.815  -22.6816]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -137.00480809466347, time: 126.882
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.465   -0.505  -12.8097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.245   -0.605  -19.6502]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.34   -0.33   -3.3983]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.44    -0.795  -22.4067]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -126.66653616465042, time: 128.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.64    -0.71   -16.5481]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.88   -0.52  -18.232]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.815  -0.29   -2.8435]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.965   -0.82   -23.2133]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -119.26800083833831, time: 126.873
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.185   -0.625  -16.0171]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.88    -0.6    -19.5325]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.135  -0.415  -4.4657]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.545   -0.76   -23.1112]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -107.75517206003092, time: 126.434
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.495   -0.625  -15.5339]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.32    -0.56   -17.4904]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.625  -0.37   -4.1415]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.935   -0.835  -23.9932]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -113.07907682094442, time: 126.725
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.515   -0.65   -17.0436]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.575   -0.52   -17.8342]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.775  -0.485  -6.6967]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.82    -0.735  -23.7235]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -117.20417050244706, time: 126.095
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.275   -0.65   -14.4418]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.075   -0.585  -20.4294]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.75   -0.505  -6.4401]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.41   -0.875 -24.372]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -124.76427424229475, time: 126.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.06    -0.58   -15.7047]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.49    -0.615  -18.8533]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.34   -0.48   -5.6489]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.845   -0.83   -24.0335]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -120.00195864813388, time: 127.936
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.46    -0.615  -15.1258]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.085   -0.67   -18.7004]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.95   -0.38   -4.8615]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.63    -0.865  -24.4304]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -112.0106435449873, time: 127.211
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.705   -0.66   -15.8337]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.79    -0.71   -19.9122]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.25    -0.39    -6.1343]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.635   -0.87   -24.4974]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -112.07123537093938, time: 125.922
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.02    -0.705  -17.3248]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.9     -0.65   -18.5599]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.505  -0.365  -5.7045]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.555   -0.91   -24.5284]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -118.96743742825345, time: 127.666
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.335   -0.665  -15.4888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.83    -0.66   -18.3738]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.08   -0.4    -4.8619]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.51    -0.8    -24.3167]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -113.9306351491247, time: 128.394
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.905   -0.615  -16.7958]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.015   -0.655  -17.5489]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.495 -0.375 -4.058]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.85    -0.83   -24.5486]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -114.47302821680931, time: 126.274
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.785   -0.695  -19.4809]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.545   -0.65   -18.2723]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.06    -0.425   -6.7745]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.14    -0.85   -24.1339]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -106.92924797199663, time: 127.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.45    -0.64   -19.2087]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.55    -0.685  -18.9823]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.56   -0.315  -3.4549]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.415   -0.83   -24.2311]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -112.459870088353, time: 127.279
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.835   -0.69   -15.1034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.895   -0.635  -18.9073]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.265  -0.38   -5.7003]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.31    -0.845  -24.2496]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -100.03576420044021, time: 127.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.19    -0.76   -21.0952]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.27   -0.68  -20.905]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.735   -0.485   -7.9594]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.915   -0.85   -24.5854]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -110.77916289600063, time: 127.403
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.41    -0.605  -17.2378]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.725   -0.755  -21.7523]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.55    -0.425   -8.0799]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.205   -0.895  -24.8289]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -112.63846670115967, time: 125.827
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.095   -0.625  -21.1615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.82    -0.61   -18.8288]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.6     -0.46    -7.4299]
[-47.67    -0.715  -23.6294]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.445   -0.95   -22.9536]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -103.12360065710487, time: 125.267
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.535   -0.735  -12.5248]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.335   -0.855  -24.0234]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.735   -0.67   -22.3433]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.705   -0.95   -21.0438]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -96.79807594698039, time: 127.257
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.39    -0.755  -13.7757]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.025   -0.88   -24.6367]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.955   -0.73   -23.8008]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.485   -0.98   -20.7744]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -102.62265507244283, time: 128.032
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.76    -0.765  -12.5748]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.13   -0.92  -24.722]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.55    -0.615  -22.8257]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.965   -0.94   -20.7909]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -99.6244929141039, time: 126.801
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.6     -0.79   -11.4746]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.98    -0.895  -24.6296]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.735   -0.77   -23.0346]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.985   -0.92   -23.4772]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -95.10228087217747, time: 125.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.61    -0.78   -11.7313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.685  -0.94  -25.225]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.465   -0.8    -24.1591]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.59    -0.965  -23.0955]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -85.18278674885543, time: 125.286
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.18    -0.785  -12.7432]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.57    -0.955  -25.1706]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.405   -0.7    -22.8074]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.675   -0.98   -22.7383]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -99.31219758671408, time: 127.172
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.465   -0.78   -11.7469]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.955  -0.995 -25.473]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.28    -0.675  -22.1154]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.77   -0.98  -21.783]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -91.41469609578799, time: 126.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.82    -0.87   -12.8026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.765  -0.97  -25.301]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.795   -0.7    -23.2237]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.87    -0.94   -22.6705]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -98.12566272882202, time: 125.739
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.53    -0.855   -9.8897]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.715  -0.96  -25.267]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.105   -0.81   -24.0592]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.455   -0.94   -22.1159]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -92.91185801048974, time: 125.069
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.795  -0.885 -14.084]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.74    -0.97   -25.2838]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.365   -0.82   -24.1718]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.845   -0.955  -22.2887]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -88.35785763792781, time: 126.115
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.945   -0.895  -11.1453]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.86    -0.965  -25.3833]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.945  -0.895 -24.688]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.905   -0.99   -21.5861]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -100.58952375789177, time: 126.677
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.44    -0.785  -11.5172]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.735   -0.99   -25.2984]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.38    -0.76   -24.1335]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.93    -0.995  -21.9918]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -89.73828553709173, time: 127.03
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.59    -0.82   -10.4945]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.765   -0.95   -25.2957]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.82    -0.795  -24.5008]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.365   -0.995  -22.3932]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -85.60955185875802, time: 127.04
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.415  -0.805 -11.208]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.42    -0.92   -25.0289]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.43    -0.68   -23.3884]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.695   -0.97   -21.9825]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -89.46977935751785, time: 127.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.735  -0.78   -8.964]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.65    -0.99   -25.2457]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.065   -0.605  -22.5528]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.42   -1.    -22.017]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -87.48037927265437, time: 128.103
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.225   -0.84   -10.8091]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.705   -0.98   -25.2708]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.225   -0.67   -23.3676]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.655   -0.995  -20.6363]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -84.30172800775038, time: 128.237
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.47    -0.86   -12.4839]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.555   -0.95   -25.1295]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.095  -0.655 -23.246]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.965   -0.98   -20.5469]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -89.15311126375003, time: 125.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.585   -0.82   -10.2144]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.66    -0.93   -25.2128]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.825   -0.76   -24.4839]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.45    -0.98   -20.7754]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -88.01208752136925, time: 126.673
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.145   -0.885  -10.6832]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.505   -0.92   -25.1032]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.715   -0.72   -19.7961]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -147.76302685072315, time: 127.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.02   -0.24   -1.4239]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.35    -0.36    -9.9944]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.965  -0.865 -20.774]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.85    -0.81   -20.8856]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -146.92243770974991, time: 127.838
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.45   -0.225  -1.2643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.79    -0.565  -15.7414]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.535   -0.785  -21.8955]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.475   -0.77   -19.8383]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -150.76559948971308, time: 128.764
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.96   -0.145  -0.8763]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.315   -0.59   -17.1394]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.895   -0.81   -19.3666]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.42    -0.805  -20.1653]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -147.82285499532813, time: 127.032
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.79   -0.185  -1.8655]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.33    -0.575  -17.5163]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.24    -0.76   -20.9743]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.135   -0.835  -22.0109]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -137.31936605343037, time: 129.709
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.505 -0.295 -2.04 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.235   -0.52   -16.2366]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.1     -0.84   -22.5277]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.73    -0.765  -21.6658]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -137.67461691771427, time: 127.759
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.48   -0.185  -1.2713]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.76    -0.58   -19.4067]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.63    -0.78   -21.1897]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.97    -0.715  -20.9974]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -140.0558454287852, time: 127.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.475  -0.165  -2.0391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.11    -0.565  -15.8728]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.76   -0.725 -20.936]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.89    -0.71   -21.4083]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -141.18267426836454, time: 128.51
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.535  -0.195  -1.9869]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.      -0.485  -14.1618]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.92    -0.865  -24.3465]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.285   -0.69   -21.0534]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -145.81049134237054, time: 128.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.015  -0.225  -1.5231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.385   -0.55   -16.0315]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.345   -0.745  -23.2666]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.905   -0.92   -24.1227]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -146.79807373687814, time: 127.948
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.595 -0.145 -1.043]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.195   -0.42   -13.0388]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.61    -0.795  -23.4271]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.02    -0.85   -22.6059]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -122.92794633722445, time: 127.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.725  -0.105  -0.5576]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.225   -0.635  -20.7011]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.975   -0.875  -23.0835]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.295   -0.78   -23.0678]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -120.88264253999947, time: 127.966
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.93   -0.19   -2.6507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.035   -0.47   -17.2823]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.6     -0.83   -21.9468]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.945   -0.775  -22.8332]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -124.96812034272158, time: 128.319
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.685  -0.22   -1.9449]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.145   -0.51   -18.3811]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.185   -0.855  -23.2752]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.635   -0.83   -21.8796]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -132.58881326372756, time: 128.619
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.57   -0.35   -4.3934]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.89    -0.49   -15.9627]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.96    -0.835  -23.6388]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.31    -0.75   -22.6284]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -122.77251672528459, time: 127.391
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.735  -0.27   -2.5454]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.395   -0.56   -19.7644]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.5     -0.84   -22.5396]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.855   -0.76   -22.8202]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -114.34008048424799, time: 128.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.895 -0.22  -2.379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.525   -0.535  -18.6323]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.84    -0.825  -23.5229]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.265   -0.885  -23.6567]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -119.38091034327128, time: 129.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.485  -0.375  -5.3029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.72    -0.66   -20.6861]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.32    -0.8    -23.7621]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.09   -0.88  -23.702]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -100.09223359464437, time: 128.158
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.1    -0.205  -1.5218]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.835   -0.695  -22.5396]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.05    -0.8    -23.3016]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.42    -0.855  -23.7022]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -106.99808067619507, time: 128.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.5    -0.305  -2.9105]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.545   -0.71   -22.6947]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.03    -0.825  -23.8099]
[-49.06    -0.985  -24.9113]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.63    -0.945  -25.2414]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -93.22049138398381, time: 127.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.975  -0.48   -4.6212]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.245  -0.67   -6.6041]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.97    -0.99   -24.8403]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.83    -0.965  -25.3591]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -86.2319045318694, time: 128.593
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.71    -0.6     -5.9298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.21  -0.615 -5.863]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.76    -0.975  -24.6978]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.095   -0.85   -24.7574]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -86.27523448278095, time: 129.045
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.815   -0.665   -7.6661]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.225  -0.54   -5.3511]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.665   -0.965  -24.6994]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.585   -0.93   -25.1849]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -89.86519541112823, time: 128.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.015   -0.55    -5.2096]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.32   -0.52   -4.0616]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.125   -0.975  -24.9216]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.985   -0.825  -24.6026]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -91.67959253258428, time: 128.946
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.015   -0.53    -5.7002]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.25  -0.49  -4.663]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.06    -0.93   -24.3027]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.68    -0.925  -25.2163]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -77.9422366897336, time: 127.855
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.1     -0.53    -5.7714]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.63   -0.365  -2.7948]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.31    -0.98   -25.0151]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.56    -0.88   -25.1115]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -82.9945423422951, time: 127.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.075   -0.58    -5.8562]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.635  -0.55   -6.4296]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.685   -0.96   -22.9864]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.38    -0.875  -24.9782]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -93.87403740757152, time: 128.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.915 -0.45  -3.358]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.78  -0.54  -3.955]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.04    -0.95   -22.8526]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.725   -0.92   -25.2833]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -88.60531135055577, time: 129.602
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.53   -0.435  -4.7914]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.5    -0.405  -5.1619]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.44    -0.945  -24.1609]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.765  -0.95  -25.306]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -83.40629563638925, time: 128.581
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.7    -0.55   -5.1544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.05    -0.545   -6.9878]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.39    -1.     -23.8077]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.91    -0.975  -25.4451]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -77.92339652723099, time: 128.474
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.5   -0.405 -3.644]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.02   -0.4    -5.0554]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.675   -0.925  -21.0227]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -0.985  -25.4098]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -81.22588911915325, time: 131.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.55   -0.525  -5.2493]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.985  -0.32   -3.6319]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.525   -0.99   -23.2485]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.78    -0.95   -25.3314]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -90.62791476026399, time: 130.196
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.41   -0.515  -4.3486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.17   -0.34   -4.7033]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.725   -0.99   -23.7129]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -0.995  -25.4362]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -80.29507691826005, time: 129.283
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.58   -0.515  -5.6386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.475  -0.385  -4.1073]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.94    -0.985  -23.9816]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.45    -0.79   -23.6586]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -73.16582364326827, time: 130.262
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.91   -0.52   -4.6621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.605  -0.34   -4.3161]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.23    -0.99   -24.0103]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.65    -0.92   -25.2067]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -83.27244687013444, time: 130.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.79   -0.525  -5.2699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.485  -0.325  -5.0111]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.24    -0.985  -23.1421]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.645   -0.925  -25.1995]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -70.76767662627856, time: 129.468
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.43   -0.445  -4.4924]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.46  -0.39  -5.684]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.91    -0.975  -23.5566]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.915   -0.99   -25.4392]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -95.44972011519845, time: 127.11
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.595  -0.515  -4.1837]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.455 -0.3   -5.184]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.08    -0.995  -23.5527]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.845   -0.975  -25.3792]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -76.61425759990611, time: 130.205
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.6    -0.485  -3.9943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.625  -0.42   -5.0621]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.255  -0.995 -22.669]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.56    -0.48   -17.3476]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.445 -0.305 -5.095]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -130.99108190829037, time: 125.462
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.205   -0.73   -20.4601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.255  -0.415  -4.5213]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.65    -0.49   -17.4478]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.23   -0.34   -3.9433]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -126.44936063412644, time: 124.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.815   -0.525  -18.4782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.88   -0.47   -7.022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.19    -0.58   -19.7902]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.62   -0.365  -4.2825]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -143.47201995531475, time: 125.549
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.505   -0.715  -22.2536]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.17   -0.395  -5.5764]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.36    -0.57   -19.9159]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.91   -0.385  -4.8134]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -145.23540945964936, time: 125.303
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.76    -0.585  -20.3141]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.08   -0.3    -3.2744]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.675   -0.595  -19.3933]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.135  -0.355  -4.4904]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -121.64675330181163, time: 123.696
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.315   -0.735  -23.0817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.345  -0.365  -4.3108]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.455   -0.67   -19.7122]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.18   -0.275  -4.0612]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -132.26057028333906, time: 125.462
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.975  -0.7   -22.771]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.665   -0.415   -6.2312]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.405   -0.59   -20.7197]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.655  -0.36   -5.4209]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -129.10004562614887, time: 125.191
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.105   -0.63   -22.6984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.315   -0.395   -6.5103]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.87    -0.69   -22.3022]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.695  -0.39   -4.0772]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -123.22379667560024, time: 126.659
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.16    -0.69   -23.0578]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.205  -0.335  -3.4525]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.99    -0.58   -21.7724]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.83  -0.285 -3.221]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -130.4249531419072, time: 126.884
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.87    -0.75   -23.9161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.03   -0.38   -4.9744]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.315   -0.62   -22.2213]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.02  -0.285 -3.207]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -119.45848965416052, time: 125.429
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.025   -0.73   -22.8423]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.465  -0.245  -2.4411]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.135   -0.685  -23.3128]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.58   -0.38   -4.9863]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -115.83056798295671, time: 126.183
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.01    -0.71   -22.3313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.32   -0.36   -3.9545]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.56    -0.62   -21.9251]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.87   -0.25   -3.6101]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -107.68507075975634, time: 127.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.22    -0.6    -19.4631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.705  -0.39   -4.4448]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.915   -0.785  -24.2444]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.225  -0.27   -3.2613]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -113.61378740810699, time: 126.775
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.725  -0.865 -21.259]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.04   -0.345  -2.9975]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.965   -0.745  -22.9134]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.245  -0.3    -3.8472]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -112.42043027698064, time: 125.494
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.02    -0.85   -20.4688]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.365 -0.355 -2.897]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.5     -0.705  -21.3667]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.18   -0.35   -3.4644]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -104.18330033756548, time: 126.445
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.945   -0.84   -21.2849]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.035  -0.47   -5.0128]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.73    -0.755  -22.9342]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.665  -0.235  -2.9927]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -113.62185565518381, time: 126.321
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.17    -0.83   -21.6484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.475 -0.46  -5.046]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.395   -0.895  -24.1731]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.48  -0.245 -3.018]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -105.33205320124162, time: 126.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.9     -0.835  -22.3874]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.71   -0.39   -4.3733]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.71    -0.815  -24.2359]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.465  -0.235  -2.4527]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -109.48957510290748, time: 124.778
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.535   -0.79   -21.6434]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.725  -0.33   -2.9731]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.59    -0.825  -23.5524]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.52   -0.36   -4.4085]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -102.0059709943096, time: 124.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.71    -0.945  -23.2616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.455  -0.38   -2.0732]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.895   -0.84   -24.4128]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.065   -0.945  -24.8314]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -102.56259577073364, time: 124.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.19   -0.335  -1.8805]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.56   -0.11   -1.1673]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.79    -0.99   -25.3519]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.825   -0.975  -25.3681]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -95.66048461207997, time: 125.614
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.77   -0.255  -1.5798]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.02   -0.19   -1.6456]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.91    -0.995  -25.4449]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.29    -0.995  -24.9803]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -98.79426472094727, time: 125.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.77   -0.285  -1.9922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.32   -0.185  -1.6491]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.775   -0.99   -25.3381]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.62   -0.995 -25.205]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -103.83753726342691, time: 125.154
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.11  -0.44  -3.966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.325  -0.265  -2.3878]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.725   -0.995  -25.3028]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.925   -0.995  -25.4488]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -101.03266147844762, time: 124.944
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.425  -0.405  -2.8095]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.52   -0.13   -1.2684]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.895   -0.99   -25.4404]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.915   -1.     -25.4615]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -97.58428721848603, time: 126.168
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.035  -0.475  -4.0108]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.965  -0.285  -3.2328]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.885   -0.99   -25.4287]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.945   -0.995  -25.4575]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -88.49062090474966, time: 124.675
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.53   -0.43   -2.9667]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.83   -0.205  -1.4809]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.88    -0.985  -25.4348]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.905   -0.98   -25.4434]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -89.70734103533422, time: 125.985
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.68   -0.43   -3.5357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.4   -0.235 -2.47 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.91   -0.99  -25.433]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.53    -0.975  -25.1401]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -93.32732584052384, time: 125.809
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.31   -0.38   -2.7541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.44  -0.195 -1.781]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.94    -0.995  -25.4655]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.44   -0.99  -25.088]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -99.04285828856514, time: 124.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.17   -0.305  -1.7762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.525  -0.29   -3.3309]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.9     -0.995  -25.4435]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.415   -0.97   -25.0542]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -95.54839114144637, time: 126.394
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.355 -0.4   -2.07 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.615  -0.295  -3.4711]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.925   -1.     -25.4616]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.68    -1.     -25.2693]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -88.76389088613134, time: 126.534
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.675  -0.42   -3.0367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.795  -0.345  -2.8379]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.86    -1.     -25.4035]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.915   -0.99   -25.4452]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -99.67384488353892, time: 125.944
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.01   -0.405  -3.1477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.08   -0.24   -2.7967]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.885   -0.985  -25.4272]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.605   -0.965  -25.1859]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -94.93301832939156, time: 125.417
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.44   -0.485  -3.5376]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.47   -0.24   -2.3397]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.825   -0.995  -25.3861]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.48    -0.96   -25.0865]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -94.14654002687537, time: 125.292
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.79   -0.435  -2.4433]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.69   -0.315  -3.1334]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.87    -0.99   -25.4103]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.875   -0.965  -24.6334]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -86.8224179160988, time: 125.832
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.875  -0.445  -3.1638]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.655  -0.22   -3.0175]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.845   -0.995  -25.3854]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.315   -1.     -24.9771]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -94.27642844689053, time: 125.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.23   -0.39   -2.6689]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.155  -0.27   -2.9285]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.81   -0.975 -25.352]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.665  -0.975 -25.237]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -90.27926568288774, time: 125.115
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.68  -0.38  -3.107]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.56   -0.295  -2.5246]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.78    -0.99   -25.3518]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.59    -0.98   -25.1816]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -97.96338116542523, time: 123.801
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.31  -0.515 -4.564]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.665  -0.215  -1.8949]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.87    -0.985  -25.3968]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.91    -0.995  -25.4371]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -90.96144766675299, time: 124.803
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.33    -0.715  -17.0939]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -108.43949465694539, time: 127.703
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.26    -0.745  -11.3441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.945   -0.32    -8.1411]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.63    -0.67   -13.8784]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.31   -0.41   -9.645]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -89.93996990092378, time: 127.227
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.28    -0.805  -10.0579]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.27    -0.38    -7.6876]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.51    -0.795  -15.0299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.42   -0.385 -10.53 ]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -98.96687632398056, time: 127.06
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.78    -0.8    -11.4751]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.9     -0.495   -9.1743]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.345  -0.845 -15.399]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.62    -0.545  -14.1233]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -102.13339032756551, time: 125.737
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.835   -0.67    -8.7422]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.145   -0.495  -12.8475]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.635   -0.88   -16.4478]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.475   -0.51   -14.3548]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -83.98934323362171, time: 125.668
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.535   -0.715   -9.3374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.37    -0.47    -9.9687]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.86   -0.92  -15.607]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.67    -0.51   -11.5836]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -86.3733401768937, time: 123.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.43    -0.83   -10.7744]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.79    -0.425   -8.3565]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.69    -0.79   -12.9685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.39    -0.505  -13.0769]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -93.34238813682227, time: 122.01
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.16    -0.81   -11.6591]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.945  -0.355  -5.9958]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.165   -0.845  -14.4025]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.395   -0.415  -10.1944]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -88.91337377803924, time: 127.249
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.81    -0.86   -10.1162]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.115   -0.39    -7.0043]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.91    -0.865  -14.3452]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.355   -0.545  -15.0204]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -97.83889869833313, time: 124.059
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.985   -0.825   -8.6724]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.98   -0.325  -5.0169]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.955   -0.87   -15.8834]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.105   -0.545  -13.2204]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -89.53749310213105, time: 124.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.545   -0.705   -7.7941]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.835   -0.445   -7.0293]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.15    -0.965  -17.3733]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.84    -0.495  -10.7355]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -92.51128951005376, time: 124.857
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.41    -0.755   -9.9027]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.59    -0.435   -8.5127]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.49    -0.94   -14.3545]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.965   -0.65   -12.7508]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -89.7270380033574, time: 124.848
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.445   -0.705   -8.0146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.835   -0.52    -8.7276]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.085   -0.92   -15.1293]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.6     -0.675  -12.4884]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -89.5183118616311, time: 125.72
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.42    -0.8    -10.1625]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.585  -0.54   -8.911]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.455   -0.925  -15.6143]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.7     -0.53   -10.5611]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -93.16468133589368, time: 124.697
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.75    -0.855  -10.3361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.265   -0.465   -8.8517]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.405  -0.955 -13.914]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.35    -0.605  -12.5776]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -96.60062839771086, time: 124.382
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.845   -0.81    -8.2161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.915   -0.435   -6.6826]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.74    -0.93   -12.9131]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.48    -0.54    -8.6439]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -78.86648689821504, time: 124.646
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.765  -0.82   -6.9497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.98   -0.335  -5.1216]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.595   -0.93   -13.9727]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.515   -0.54    -8.7695]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -89.10086267927781, time: 124.403
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.38    -0.81    -7.8954]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.875   -0.485   -7.8385]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.73    -0.92   -12.8139]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.675   -0.505   -9.6498]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -90.059814695343, time: 123.538
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.405  -0.72   -7.1548]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.72   -0.47   -7.711]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.075   -0.97   -14.8216]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.7     -0.66   -11.9333]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -85.37507604979834, time: 125.339
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.63    -0.695   -8.1291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.905   -0.495   -7.2825]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.335   -0.935  -12.8724]
[-5.815  -0.13   -3.7626]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -120.6560204008344, time: 127.839
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.71  -0.365 -2.623]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.385   -0.595  -17.8638]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.36    -0.945  -25.0517]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.685  -0.25   -4.9585]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -113.72292763630841, time: 128.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.145  -0.37   -2.7278]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.475   -0.63   -16.1102]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.47    -0.945  -25.1236]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.25   -0.23   -4.2194]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -108.22138222364414, time: 128.975
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.33  -0.345 -3.31 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.76    -0.525  -16.9383]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.315  -0.965 -25.03 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.03   -0.21   -4.9259]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -108.32574211241176, time: 126.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.48   -0.405  -2.4851]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.855   -0.515  -15.0389]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.045   -0.975  -24.8235]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.79   -0.26   -5.9888]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -118.70310297361155, time: 127.675
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.02   -0.305  -3.3492]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.83    -0.48   -14.0099]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.63    -0.97   -25.2364]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.685  -0.29   -4.4371]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -105.43994021673643, time: 128.166
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.79  -0.42  -3.675]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.155   -0.575  -18.4449]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.355   -0.97   -25.0486]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.815  -0.235  -5.8718]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -100.9100091711639, time: 127.614
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.56   -0.375  -2.4832]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.065   -0.52   -17.5741]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.465   -0.99   -25.1122]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.81  -0.255 -5.887]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -103.91700324492926, time: 128.183
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.32   -0.41   -2.9442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.185   -0.48   -14.8205]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.64    -0.99   -25.2369]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.56  -0.21  -3.714]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -102.066681800021, time: 129.482
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.085  -0.465  -3.3494]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.235   -0.49   -17.2633]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.425   -0.98   -25.1142]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.775  -0.27   -4.7572]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -107.0251846576533, time: 127.815
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.695   -0.52    -6.4115]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.455   -0.39   -15.6882]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.55    -0.98   -25.1467]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.02   -0.27   -3.8749]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -109.57505343987934, time: 128.628
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.2    -0.445  -4.4194]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.33    -0.46   -16.7361]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.57    -0.99   -25.1815]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.085  -0.385  -5.4598]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -99.39763030228718, time: 127.719
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.3    -0.43   -3.7544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.125   -0.5    -15.8622]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.585   -0.995  -25.1739]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.94   -0.23   -3.8902]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -115.86771332905919, time: 129.398
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.965  -0.39   -3.3997]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.845   -0.52   -14.9295]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.44    -0.98   -25.0695]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.075  -0.355  -5.2929]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -95.7892094406499, time: 127.362
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.38   -0.4    -4.7084]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.11   -0.47  -13.388]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.415   -0.985  -25.1133]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.905  -0.36   -5.1487]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -104.99831920686232, time: 127.336
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.635  -0.35   -2.8013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.175   -0.545  -20.0604]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.95    -0.99   -24.8931]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.875  -0.37   -4.6945]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -98.49612063683752, time: 128.108
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.095  -0.465  -3.2795]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.61    -0.665  -21.3309]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.565   -1.     -25.2247]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.5   -0.44  -5.119]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -99.5054575200826, time: 127.672
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.825  -0.345  -2.4661]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.725   -0.655  -20.2529]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.07    -0.995  -24.9524]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.135  -0.475  -5.2911]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -95.22881619998378, time: 128.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.945  -0.385  -3.7665]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.33    -0.46   -17.4133]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.645   -0.99   -25.2203]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.95   -0.365  -3.8317]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -95.65353964849503, time: 126.576
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.675 -0.33  -3.95 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.48    -0.59   -18.6701]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.425   -1.     -25.1405]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.46   -0.36   -5.1506]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -93.40257838677392, time: 127.249
7800 50
steps: 389950, episodes: 7800, mean episode reward: -104.3831471995377, time: 131.639
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.68    -0.35    -8.5534]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.13    -0.585  -10.7896]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.775   -0.83   -22.7892]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.195   -0.605  -19.7215]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -118.70167099488103, time: 131.48
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.85    -0.315   -7.4222]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.745 -0.38  -4.974]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.065   -0.84   -23.4534]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.93    -0.7    -17.7846]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -107.58140517552415, time: 132.374
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.685   -0.23    -7.4814]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.63    -0.47    -7.6868]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.21    -0.725  -21.5083]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.765   -0.615  -18.2331]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -108.84764974671408, time: 132.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.255   -0.25    -8.8917]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.785   -0.675  -11.8967]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.65    -0.815  -22.0724]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.23    -0.67   -18.7537]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -107.51308234469352, time: 131.315
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.655  -0.345  -8.826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.565   -0.605  -11.8115]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.505   -0.745  -23.0373]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.87    -0.77   -19.8522]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -106.58606956084306, time: 130.195
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.175  -0.375 -10.446]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.72    -0.69   -12.4325]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.23   -0.85  -23.473]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.32    -0.585  -17.6593]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -107.65447753924204, time: 130.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.52    -0.355   -9.2374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.365   -0.695  -13.4626]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.4     -0.85   -22.7766]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.69    -0.71   -19.9381]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -100.64452317629438, time: 131.066
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.085  -0.37  -10.333]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.595   -0.705  -10.6018]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.365   -0.875  -22.1595]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.475   -0.79   -19.3891]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -112.86366208268646, time: 127.988
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.095   -0.305   -8.9495]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.29    -0.735  -12.8976]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.6     -0.865  -23.3087]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.63    -0.615  -17.2185]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -112.12648290228617, time: 130.08
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.125   -0.21    -6.7766]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.105   -0.755  -10.2711]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.685   -0.85   -23.0469]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.025   -0.845  -19.3074]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -103.75926021783889, time: 128.237
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.635   -0.4    -11.3745]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.32   -0.725 -11.927]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.25    -0.89   -24.2454]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.65    -0.755  -19.3737]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -103.02145408653111, time: 127.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.325  -0.39   -9.653]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.11    -0.88   -17.5173]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.935   -0.875  -23.4304]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.14   -0.745 -18.923]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -105.38252761830714, time: 126.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.71    -0.43    -8.3273]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.535   -0.75   -12.5036]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.91    -0.875  -23.9621]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.86    -0.69   -20.2763]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -98.52578554954631, time: 127.373
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.15    -0.35    -8.4773]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.955   -0.65   -12.1577]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.27    -0.935  -23.7003]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.88    -0.79   -21.7551]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -99.03522733832888, time: 127.266
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.135   -0.315   -8.7103]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.615   -0.75   -13.9319]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.3     -0.905  -23.7253]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.31    -0.715  -19.2502]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -100.45869648489577, time: 124.729
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.155   -0.395   -9.1277]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.675   -0.87   -17.5792]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.295   -0.955  -23.3683]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.55    -0.71   -20.3903]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -112.43247567678016, time: 122.138
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.66    -0.36    -9.3239]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.235   -0.665  -13.4868]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.72   -0.94  -23.989]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.275   -0.63   -20.9337]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -107.46266077012376, time: 123.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.29    -0.315   -8.6102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.09    -0.74   -13.3335]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.605   -0.89   -24.0216]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.41    -0.705  -21.6315]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -109.947161308775, time: 124.299
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.065   -0.46   -11.8869]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.05    -0.75   -17.4062]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.375   -0.905  -23.5656]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.085  -0.015  -0.0644]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -112.49230208945079, time: 126.401
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.615   -0.92   -18.1807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.67    -0.98   -23.0255]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.24    -0.41    -6.1885]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.045  -0.005  -0.0379]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -110.18565310558024, time: 127.359
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.705   -0.87   -17.2812]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.185   -0.94   -22.9513]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.445  -0.43   -6.1356]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.24   -0.035  -0.1931]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -120.64132722045956, time: 126.89
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.96    -0.865  -18.4344]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.905  -0.955 -20.984]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.625  -0.345  -4.4603]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.105 -0.005 -0.082]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -108.11177259512247, time: 125.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.425   -0.885  -17.7848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.14   -0.95  -19.695]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.93   -0.335  -3.3652]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.155  -0.01   -0.0941]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -96.8999704398876, time: 126.042
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.79    -0.75   -17.8671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.995   -0.965  -19.5903]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.34   -0.305  -4.0726]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.12   -0.01   -0.0556]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -112.68601196387176, time: 126.688
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.795  -0.895 -16.008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.58    -0.985  -22.6826]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.545  -0.29   -4.3041]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.105  -0.005  -0.0821]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -94.22057691284233, time: 125.89
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.73    -0.96   -17.1125]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.055   -0.995  -18.6134]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.475  -0.26   -4.3741]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.075   0.     -0.0358]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -103.8935094735679, time: 127.393
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.265   -0.855  -15.0896]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.295   -0.98   -21.3205]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.79   -0.335  -4.5197]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.14   -0.005  -0.1134]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -104.52035141336837, time: 126.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.23    -0.645  -11.6845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.935   -0.98   -23.2475]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.715  -0.24   -3.3528]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.09    0.     -0.0724]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -101.47328536369663, time: 126.391
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.395   -0.61   -10.8021]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.71    -0.985  -22.5092]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.545  -0.305  -4.0183]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.605  -0.015  -0.3289]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -111.64111429227447, time: 127.189
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.29    -0.465  -10.2366]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.165   -0.99   -23.4456]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-2.865  -0.13   -1.9332]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.295  -0.025  -0.2073]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -113.49126235222296, time: 127.531
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.64    -0.59   -13.0942]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.385   -0.995  -22.3656]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.445   -0.45    -7.5228]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.305  -0.03   -0.2498]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -103.11002669198285, time: 127.857
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.08    -0.635  -10.8406]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.06    -0.995  -20.4371]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.89   -0.315  -3.3951]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.03    0.     -0.0179]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -95.41306652157698, time: 126.309
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.6     -0.745  -14.3249]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.43    -1.     -17.8789]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.46   -0.33   -3.1917]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.15   -0.02   -0.1135]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -99.84435042679786, time: 128.248
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.315   -0.845  -12.2698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.84    -0.975  -17.1275]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.82   -0.325  -4.3287]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.15   -0.005  -0.1052]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -102.44847549979663, time: 128.828
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.595   -0.715  -13.8998]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.11    -0.985  -18.6299]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.765  -0.255  -4.5188]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.55   -0.015  -0.4057]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -101.34550055514767, time: 125.932
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.24    -0.705  -11.9233]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.325  -1.    -19.971]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.81   -0.265  -3.7226]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.105  -0.005  -0.0651]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -102.17961314942738, time: 126.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.165   -0.5     -9.6116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.575   -0.995  -21.0572]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.85   -0.19   -2.8501]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.21   -0.05   -0.7586]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -105.05102195329574, time: 127.082
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.295  -0.78  -13.827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.405  -0.985 -20.826]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.69    -0.445   -6.0836]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.12   -0.02   -0.0975]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -120.63090760542454, time: 127.122
[-36.38    -0.975  -19.2896]
7800 50
steps: 389950, episodes: 7800, mean episode reward: -109.46041887699047, time: 130.88
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.94   -0.07   -0.9981]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.1    -0.14   -0.9713]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.17  -0.97 -19.62]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.51    -0.915  -19.0255]
8000 50
steps: 399950, episodes: 8000, mean episode reward: -96.84841262814479, time: 131.731
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.73   -0.205  -2.0902]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.64   -0.24   -2.0873]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.5     -0.95   -18.2564]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.36    -0.98   -17.8712]
8200 50
steps: 409950, episodes: 8200, mean episode reward: -107.03409408611762, time: 131.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.92   -0.21   -2.1357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.54  -0.3   -2.805]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.755   -0.985  -20.6801]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.435   -0.985  -19.7076]
8400 50
steps: 419950, episodes: 8400, mean episode reward: -100.3504344938431, time: 131.31
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.87   -0.165  -1.1934]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.795  -0.14   -0.7266]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.42    -0.98   -18.6022]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.135   -0.98   -21.0304]
8600 50
steps: 429950, episodes: 8600, mean episode reward: -102.94190173632686, time: 132.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.1    -0.165  -2.1775]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.06   -0.185  -0.9643]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.975   -0.985  -18.9789]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.435   -0.985  -21.9832]
8800 50
steps: 439950, episodes: 8800, mean episode reward: -107.76651124312474, time: 131.354
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.15   -0.28   -3.2579]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.145  -0.205  -1.0701]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.21    -0.95   -19.3196]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.97   -0.985 -22.903]
9000 50
steps: 449950, episodes: 9000, mean episode reward: -101.82598539942545, time: 129.449
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.06  -0.275 -3.02 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.995  -0.165  -0.9014]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.29    -0.95   -19.5859]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.215   -0.985  -22.9024]
9200 50
steps: 459950, episodes: 9200, mean episode reward: -89.82514892020623, time: 134.448
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.025  -0.425  -4.8473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.52   -0.07   -0.4788]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.78    -0.965  -20.4748]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.755   -0.95   -19.1787]
9400 50
steps: 469950, episodes: 9400, mean episode reward: -91.3256928034146, time: 131.22
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.585  -0.29   -1.3067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.32   -0.2    -1.2045]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.345   -0.985  -19.9785]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.52    -0.935  -19.3991]
9600 50
steps: 479950, episodes: 9600, mean episode reward: -84.75397796234745, time: 132.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.17   -0.235  -1.5269]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.555  -0.19   -1.3984]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.95   -1.    -19.647]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.61    -0.965  -18.0354]
9800 50
steps: 489950, episodes: 9800, mean episode reward: -94.76243503989322, time: 131.337
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.05   -0.28   -1.9496]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.885  -0.11   -0.7846]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.84    -0.97   -20.7387]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.78    -0.98   -17.3625]
10000 50
steps: 499950, episodes: 10000, mean episode reward: -94.19105164983206, time: 132.645
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.28   -0.23   -1.5711]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.54   -0.065  -0.4963]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.92    -0.98   -22.7306]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.245   -0.97   -19.3342]
10200 50
steps: 509950, episodes: 10200, mean episode reward: -84.35589163191564, time: 131.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.2    -0.28   -2.8253]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.015  -0.375  -2.5967]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.46    -0.995  -22.2814]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.51    -0.99   -17.7798]
10400 50
steps: 519950, episodes: 10400, mean episode reward: -94.66005214011486, time: 131.633
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.57   -0.35   -4.2339]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.58   -0.25   -1.4604]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.82    -0.985  -23.1176]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.57    -0.98   -16.7528]
10600 50
steps: 529950, episodes: 10600, mean episode reward: -98.2981835281826, time: 132.123
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.915  -0.255  -2.1079]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.035  -0.315  -1.8386]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.16    -0.99   -22.4746]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.105   -0.995  -20.4659]
10800 50
steps: 539950, episodes: 10800, mean episode reward: -93.77383337496147, time: 129.539
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.715  -0.22   -2.4331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.905  -0.15   -0.8383]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.885   -0.985  -21.9692]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.305   -0.99   -18.5711]
11000 50
steps: 549950, episodes: 11000, mean episode reward: -97.95090639039674, time: 128.667
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.535  -0.155  -0.9361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.15   -0.21   -1.7017]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.685   -0.98   -22.1856]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.165   -0.995  -19.3811]
11200 50
steps: 559950, episodes: 11200, mean episode reward: -91.28805320650874, time: 133.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.715  -0.15   -2.2154]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.06   -0.22   -1.7454]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.43    -0.965  -19.3944]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.555   -0.965  -16.6215]
11400 50
steps: 569950, episodes: 11400, mean episode reward: -92.11263827967025, time: 128.988
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.155  -0.195  -2.5341]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.035  -0.205  -1.7174]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.97    -0.985  -22.7492]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.59    -0.975  -15.4964]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -86.13874128577656, time: 129.744
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.74    -0.88   -24.5268]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -110.9508637045968, time: 127.253
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.275   -0.66   -19.1462]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.895   -0.6    -19.2613]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.625   -0.425   -8.4079]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.92  -0.84 -24.64]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -109.83978859666767, time: 126.761
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.37   -0.655 -17.675]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.645   -0.79   -21.7293]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.185  -0.415  -6.755]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.86    -0.915  -24.6205]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -97.8108068371193, time: 127.092
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.255   -0.765  -22.8934]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.81    -0.64   -20.7196]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.285  -0.38   -4.7901]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.165   -0.885  -24.7849]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -104.08362060953834, time: 128.027
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.195   -0.76   -22.4936]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.25    -0.665  -21.0729]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.07    -0.485   -7.4321]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.205   -0.94   -24.8934]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -100.28854908973727, time: 128.101
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.63    -0.71   -20.3124]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.285   -0.815  -21.0693]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.745  -0.41   -6.3222]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.1     -0.935  -24.8493]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -102.11377322861219, time: 127.159
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.245   -0.55   -13.0827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.785   -0.655  -20.5945]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.405  -0.425  -5.9517]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.43    -0.91   -24.3733]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -116.60261045087823, time: 127.286
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.635   -0.54   -16.8398]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.455   -0.7    -20.9562]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.775  -0.48   -6.8352]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.155   -0.935  -24.9053]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -109.57371307343827, time: 129.924
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.265   -0.53   -12.8401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.04   -0.66  -19.112]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.915   -0.455   -7.8104]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.85    -0.93   -24.6638]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -101.28042373692729, time: 129.443
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.725   -0.615  -17.3846]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.155   -0.725  -19.2426]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.68   -0.43   -5.3137]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.035   -0.915  -24.7703]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -108.77638598707435, time: 128.52
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.725   -0.665  -18.0379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.96    -0.78   -20.1951]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.695  -0.4    -5.3185]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.205   -0.94   -24.9447]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -105.09958409252958, time: 129.014
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.955   -0.79   -22.3403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.84    -0.74   -18.5637]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.175  -0.485  -6.6387]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.23    -0.935  -24.9375]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -105.11881108241276, time: 128.472
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.47    -0.685  -20.3577]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.67    -0.665  -17.6971]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.37   -0.38   -5.0255]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.065   -0.87   -24.7799]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -106.43604960693847, time: 128.488
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.375   -0.69   -18.1569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.03   -0.715 -20.397]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.195  -0.385  -3.9673]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.545   -0.935  -24.5457]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -109.28676298834569, time: 128.399
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.67    -0.585  -18.9983]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.885   -0.78   -21.4854]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.72   -0.435  -4.9656]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.35    -0.93   -24.3249]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -96.4103896963732, time: 127.337
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.115   -0.58   -18.9073]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.49    -0.775  -21.4519]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.715   -0.5     -7.2267]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.225   -0.975  -24.8975]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -98.12423087183873, time: 128.038
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.565   -0.695  -19.8909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.72    -0.805  -19.4192]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.025 -0.56  -6.59 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.005  -0.98  -24.816]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -100.17000973313057, time: 126.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.19   -0.57  -15.698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.52    -0.74   -20.2474]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.      -0.525   -6.9192]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.675   -0.95   -23.1497]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -112.19729394575539, time: 130.198
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.74   -0.73  -18.329]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.435   -0.81   -21.5795]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.92   -0.46   -5.7627]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.225   -0.94   -23.0319]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -105.78137132115867, time: 127.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.565   -0.685  -18.0607]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.43    -0.75   -21.4452]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.855   -0.735  -23.6975]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.765   -0.99   -19.8693]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -92.41505102534873, time: 126.152
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.975   -0.775   -9.9271]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.26   -0.935 -24.943]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.765  -0.7   -24.384]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.175   -0.99   -20.5649]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -86.08499952895862, time: 125.45
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.33    -0.89   -12.6352]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.155   -0.875  -24.7805]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.66    -0.75   -24.3549]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.325   -0.99   -19.3657]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -94.34427136582389, time: 128.527
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.965   -0.735   -8.9857]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.665  -1.    -25.274]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.215   -0.625  -23.9858]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.81    -1.     -20.9793]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -84.92538860372518, time: 126.737
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.89    -0.83   -12.4908]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.68    -1.     -25.2781]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.125   -0.76   -24.0215]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.225   -1.     -19.3985]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -86.79285564947477, time: 128.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.885   -0.765   -9.0487]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.49    -0.995  -25.1386]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.385   -0.675  -23.5314]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.73   -0.995 -20.176]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -85.07553511618998, time: 127.681
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.67    -0.945  -13.5539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.655   -1.     -25.2422]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.245  -0.725 -23.99 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.395   -1.     -20.0962]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -86.01777061338416, time: 128.53
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.64    -0.915  -13.7878]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.33    -0.995  -25.0218]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.85    -0.67   -23.0648]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.765   -1.     -19.8029]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -89.78175894886988, time: 128.058
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.285   -0.9    -15.6781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.695   -1.     -25.2827]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.135   -0.66   -22.7699]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.065   -0.99   -20.5705]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -81.5096770589552, time: 128.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.72    -0.835  -12.3405]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.735   -0.985  -25.3332]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.69    -0.8    -24.4429]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.235   -1.     -19.4093]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -86.3055623420476, time: 128.247
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.25    -0.61    -8.1266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.425  -0.94  -25.053]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.89    -0.815  -23.9666]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.95    -0.985  -20.7302]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -87.2701730469301, time: 127.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.715   -0.765   -9.1942]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.815  -0.995 -25.363]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.385   -0.75   -24.2246]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.845   -0.99   -19.3487]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -84.20763428455172, time: 127.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.7     -0.7     -8.0718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.84    -1.     -25.3885]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.925   -0.7    -23.7994]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.935   -1.     -18.7891]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -80.31349520949736, time: 128.86
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.385   -0.85   -12.9168]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.945   -0.985  -25.4615]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.7     -0.74   -24.3836]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.45    -0.99   -18.2758]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -81.06310865162511, time: 128.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.375   -0.845  -11.2269]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.935   -0.99   -25.4578]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.485   -0.785  -24.2746]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.575   -0.995  -19.3405]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -83.19210622553469, time: 127.589
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.79    -0.9    -14.4079]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.915   -1.     -25.4511]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.97    -0.765  -24.5851]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.48    -0.995  -20.2284]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -82.5617749564469, time: 128.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.095   -0.725  -10.4945]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.94    -1.     -25.4693]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.25    -0.845  -24.9012]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.895   -0.995  -20.3925]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -81.24115072473691, time: 126.062
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.37    -0.81   -12.1778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.92    -0.995  -25.4431]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.365   -0.84   -24.9581]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.055   -1.     -21.4173]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -90.19510329241872, time: 128.692
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.23    -0.675   -8.6343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.95    -1.     -25.4752]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.1     -0.835  -24.7701]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.855   -0.985  -19.7559]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -79.16466415232618, time: 127.191
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.125   -0.855  -11.2066]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.865   -0.96   -25.3916]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.015   -0.915  -23.0473]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -116.51288680826173, time: 127.689
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.975  -0.18   -1.2715]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.735   -0.67   -21.1329]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.355   -0.84   -23.8711]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.29    -0.8    -23.5133]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -109.12013609983914, time: 127.346
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.835  -0.355  -2.2123]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.08    -0.67   -19.8486]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.8     -0.83   -23.9701]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.755   -0.81   -23.9777]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -107.84905983024427, time: 128.042
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.975  -0.235  -1.4879]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.39   -0.785 -23.423]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.52    -0.81   -22.1535]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.11    -0.825  -22.7932]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -113.26996779520988, time: 128.502
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.84   -0.21   -1.4099]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.57    -0.725  -22.7107]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.12    -0.84   -22.1688]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.82    -0.935  -23.3907]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -105.00910244461178, time: 128.397
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.685  -0.11   -1.0319]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.7     -0.735  -23.5192]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.62    -0.72   -23.4117]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.555   -0.94   -23.3517]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -110.33446700319138, time: 127.203
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.195 -0.225 -2.353]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.4     -0.82   -24.1531]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.04   -0.795 -20.985]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.97    -0.89   -22.9404]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -108.59769721889131, time: 129.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.275  -0.275  -2.3755]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.19    -0.785  -23.9579]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.125   -0.765  -21.3554]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.85    -0.875  -22.3593]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -105.84305274097537, time: 126.718
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.49   -0.195  -2.0027]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.06    -0.88   -24.7202]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.285   -0.805  -24.3336]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.84    -0.935  -22.9146]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -104.32251402177906, time: 130.013
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.27   -0.205  -1.6552]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.705   -0.845  -24.3721]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.935   -0.86   -23.2281]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.355   -0.855  -22.9727]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -121.38479417147738, time: 129.179
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.2    -0.16   -0.9937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.88    -0.85   -24.5633]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.045   -0.815  -22.9337]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.45   -0.91  -21.778]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -94.29622666908304, time: 132.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.145  -0.125  -0.9357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.215   -0.82   -24.7904]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.35    -0.775  -22.7978]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.765   -0.895  -22.5013]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -109.22311007835896, time: 129.199
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.31   -0.26   -3.1194]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.1    -0.825 -24.696]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.84    -0.865  -24.1041]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.495   -0.89   -23.0092]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -110.79068033140804, time: 129.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.685  -0.18   -1.2594]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.13    -0.785  -24.7101]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.09    -0.86   -23.3901]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.315  -0.96  -25.038]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -101.49110567508181, time: 129.351
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.55   -0.115  -0.4979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.28    -0.785  -24.8136]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.885   -0.895  -24.2111]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.015   -0.975  -24.7542]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -98.20262998915474, time: 129.837
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.505  -0.215  -1.8669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.43    -0.845  -24.9912]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.925   -0.885  -24.6898]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.49    -0.93   -24.4666]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -109.13507013278661, time: 128.047
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.38   -0.275  -2.2641]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.58    -0.875  -25.1181]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.08    -0.85   -24.7928]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.155   -0.945  -24.8887]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -94.2685586761001, time: 128.752
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.165  -0.18   -1.0138]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.61    -0.905  -25.1637]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.08    -0.815  -24.7724]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.365  -0.915 -23.811]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -99.45334958821873, time: 128.527
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.295  -0.205  -1.1169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.485   -0.88   -25.0435]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.7     -0.79   -22.7818]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.835   -0.935  -24.6247]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -99.20015660083831, time: 129.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.69  -0.225 -1.45 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.54    -0.91   -25.1392]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.575   -0.88   -24.5306]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.56   -0.355  -2.1379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.495  -0.265  -2.3977]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.825   -0.99   -25.3806]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.76    -0.905  -23.9805]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -91.1800121058796, time: 125.749
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.775  -0.395  -3.0618]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.31   -0.22   -2.3617]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.87   -0.995 -25.413]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.085   -1.     -24.8423]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -84.28485860221672, time: 126.684
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.445  -0.395  -3.4296]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.595  -0.185  -1.9793]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.955   -0.995  -25.4685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.35    -0.98   -25.0312]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -89.1258314378234, time: 126.408
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.615  -0.53   -4.4137]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.515  -0.225  -2.4296]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.44    -0.98   -25.2206]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.55    -1.     -24.5517]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -88.44688073584707, time: 125.721
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.55 -0.48 -3.75]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.7    -0.31   -4.4954]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.855  -1.    -25.449]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.28    -1.     -24.9783]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -85.85571234328029, time: 126.243
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.13  -0.285 -1.868]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.81   -0.21   -2.5227]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.855  -1.    -25.408]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.84   -1.    -25.388]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -86.90568005725665, time: 127.499
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.29   -0.415  -2.7184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.935  -0.275  -3.8062]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.88    -0.995  -24.6424]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.705   -0.99   -25.2578]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -89.13622324635125, time: 126.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.62   -0.385  -2.2418]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.78   -0.31   -3.3015]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.735  -1.    -25.345]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.64    -0.985  -25.2114]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -80.68985203376327, time: 126.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.855  -0.34   -2.2567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.34  -0.385 -3.322]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.805   -1.     -25.3879]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.64    -1.     -25.2353]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -87.22561523512384, time: 127.101
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.305  -0.35   -2.0113]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.735  -0.3    -3.9361]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.84   -1.    -25.379]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.825   -0.98   -25.3916]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -85.76485381283126, time: 128.228
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.24   -0.46   -2.7733]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.385  -0.265  -2.9892]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.68    -0.995  -25.3524]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.735   -1.     -25.3223]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -84.44964859354828, time: 125.632
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.465  -0.415  -2.8312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.865  -0.245  -3.8999]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.74    -1.     -25.0146]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.625  -1.    -25.234]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -85.7553010056307, time: 127.05
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.185  -0.415  -3.2816]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.185  -0.22   -2.7312]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.61    -0.995  -25.2661]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -1.     -25.4282]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -89.31705888484514, time: 127.085
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.79   -0.44   -3.1267]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.57   -0.195  -4.0701]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.685   -0.99   -25.3713]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.9     -1.     -25.4308]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -80.012165078795, time: 128.21
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.895  -0.35   -2.4649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.395  -0.295  -4.9056]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.845   -0.995  -25.4299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.715   -0.97   -25.2873]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -84.445311820943, time: 126.679
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.325  -0.495  -3.5018]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.725  -0.355  -4.3414]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.76    -0.995  -25.3346]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.775   -0.98   -25.3349]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -92.54308994910232, time: 125.503
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.515  -0.38   -2.7771]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.39   -0.22   -3.0299]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.605   -1.     -25.2182]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.24    -0.94   -24.9723]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -80.71799684829449, time: 126.76
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.765 -0.525 -3.709]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.07   -0.29   -3.6195]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.89   -1.    -25.424]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.595   -1.     -25.2094]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -80.24019996736918, time: 127.055
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.53   -0.45   -3.7446]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.17   -0.265  -3.3158]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.915   -1.     -25.4521]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.515   -0.975  -25.1214]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -79.182553376534, time: 125.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.735  -0.445  -3.0996]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.76    -0.625  -10.8189]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -82.06768834346946, time: 124.912
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.13   -0.825  -7.967]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.09    -0.465   -7.3065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.055   -0.975  -13.1464]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.395   -0.68   -10.8224]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -88.90849516679708, time: 125.824
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.155   -0.85    -7.9187]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.155   -0.555   -7.5021]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.245   -0.91   -13.3463]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.435   -0.58    -8.8066]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -97.1741538479887, time: 123.992
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.7    -0.9   -10.142]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.105  -0.535  -9.292]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.365   -0.935  -13.7331]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.805  -0.415  -9.205]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -87.83735125635903, time: 127.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.6     -0.865   -8.6398]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.245   -0.68   -10.2614]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.165   -0.96   -15.8243]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.805   -0.64   -11.1333]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -87.02121796107987, time: 127.134
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.985  -0.755  -9.129]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.525   -0.675   -9.5946]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.255  -0.905 -15.857]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.995   -0.58    -9.3864]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -86.46153459975943, time: 126.794
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.38    -0.73    -7.8092]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.81    -0.625   -8.9141]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.36    -0.94   -14.2541]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.51    -0.655  -12.2783]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -85.21767308034487, time: 126.634
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.27   -0.655  -6.8594]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.11    -0.64    -9.0325]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.705   -0.965  -13.7303]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.01    -0.505   -7.7643]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -75.87957779780795, time: 126.068
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.4    -0.82   -7.3905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.095   -0.57    -7.8793]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.46   -0.955 -13.206]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.47    -0.535   -8.2315]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -94.44079958898857, time: 126.921
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.535   -0.86    -8.8367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.555   -0.515   -8.1722]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.395   -0.95   -14.5837]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.3    -0.565  -8.25 ]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -88.69754788814977, time: 128.685
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.415  -0.715  -6.4964]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.975   -0.6     -9.1084]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.77   -0.955 -15.086]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.2     -0.525   -6.9493]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -81.29154364391907, time: 127.068
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.895  -0.75   -7.3601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.87    -0.505   -8.6543]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.145   -0.97   -14.5405]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.28    -0.61    -9.2568]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -87.35095639104074, time: 127.572
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.025  -0.725  -6.5359]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.515   -0.595   -9.4956]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.265   -0.98   -12.9209]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.46    -0.515   -7.8598]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -81.07782780291069, time: 126.467
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.005  -0.7    -6.5239]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.47    -0.55    -7.9158]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.675   -0.975  -13.9104]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.355   -0.565   -7.8551]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -75.04348814424675, time: 127.747
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.665 -0.695 -6.937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.695  -0.425  -6.1502]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.185   -0.985  -12.6015]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.73   -0.465  -7.895]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -81.04104675111675, time: 126.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.845 -0.775 -7.078]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.995   -0.605  -12.3257]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.645   -0.955  -14.0533]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.23    -0.5     -7.6876]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -90.45655636243646, time: 128.327
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.27   -0.82   -7.4985]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.955   -0.585  -11.4197]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.745   -0.965  -15.6668]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.31    -0.575   -8.7402]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -82.04315272381585, time: 126.227
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.695  -0.82   -6.4138]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.515   -0.6    -11.6055]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.705   -0.965  -13.6786]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.735   -0.595   -7.9247]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -79.52645009744668, time: 127.874
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.62   -0.88   -7.1533]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.36    -0.81   -11.1756]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.975   -0.98   -13.6262]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.595  -0.6    -5.6484]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -80.1012446660716, time: 126.556
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.235  -0.755  -6.7742]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.25    -0.505   -7.1571]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.58    -0.975  -13.9415]
[-49.92    -0.99   -25.4417]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -79.59350769462809, time: 128.203
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.385  -0.555  -4.7214]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.49   -0.28   -4.1115]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.925   -0.98   -22.8181]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.74    -0.975  -25.2908]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -69.81184828809882, time: 128.546
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.3    -0.565  -5.6824]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.785  -0.285  -5.3382]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.02    -0.985  -23.9281]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.84    -1.     -25.4078]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -82.79443294896413, time: 128.34
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.3     -0.585   -6.5716]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.475  -0.335  -4.0125]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.165   -0.995  -22.2282]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.71    -0.975  -25.2806]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -76.49549524260145, time: 130.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.07    -0.585   -6.0564]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.09   -0.435  -5.1657]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.865  -0.995 -24.24 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.89    -0.99   -25.4138]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -80.675934011089, time: 129.803
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.455  -0.58   -5.8812]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.8    -0.39   -4.4255]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.13    -0.98   -23.3862]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -0.995  -25.4182]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -82.1967345627816, time: 129.637
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.81    -0.54    -7.3671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.42   -0.39   -3.4652]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.15    -0.98   -22.1326]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.68    -0.96   -25.2627]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -69.25309281492454, time: 130.496
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.765   -0.585   -6.4023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.585  -0.49   -5.0026]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.175   -0.995  -23.8645]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.885   -0.975  -25.4052]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -69.50104437096329, time: 129.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.64    -0.555   -6.9031]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.1    -0.29   -2.4763]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.73    -0.96   -22.8787]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -0.99   -25.4314]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -78.18691092667899, time: 131.471
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.82    -0.595   -7.4921]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.63   -0.355  -3.7223]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.16    -0.975  -23.6221]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.95    -1.     -25.4695]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -71.12186180724927, time: 129.68
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.445  -0.455  -5.0342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.28   -0.23   -1.8824]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.005   -0.965  -24.8122]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.925   -0.975  -25.4355]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -89.68527007151141, time: 130.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.145   -0.625   -9.1573]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.2    -0.34   -4.2747]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.515   -0.98   -24.5617]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.845   -0.985  -25.4227]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -69.58195699147718, time: 129.545
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.015   -0.47    -6.2985]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.435   -0.43    -6.8365]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.255   -0.975  -24.4235]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.925   -0.99   -25.4652]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -72.12277172382123, time: 128.615
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.145  -0.49   -5.8747]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.315  -0.395  -4.0543]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.45    -0.98   -23.5737]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.905   -0.985  -25.4374]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -68.95468279569032, time: 130.689
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.55    -0.665   -7.9089]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.535  -0.28   -1.9681]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.405   -0.985  -23.1537]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -0.975  -25.4223]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -83.9354263267712, time: 129.653
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.48   -0.575  -5.9873]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.915  -0.18   -1.5079]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.49   -0.99  -24.279]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.925   -0.995  -25.4684]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -82.84071485441974, time: 129.208
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.355  -0.56   -5.7644]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.575  -0.43   -3.4699]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.78    -0.985  -22.0664]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -0.99   -25.4165]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -83.15658802533395, time: 129.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.865   -0.615   -8.1221]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.745  -0.55   -3.8158]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.74   -0.97  -22.816]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.805   -1.     -25.4288]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -86.15279726357345, time: 129.176
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.15    -0.55    -6.5577]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.55   -0.545  -4.4756]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.7     -0.965  -23.0969]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.51    -0.995  -25.3102]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -74.01296441867186, time: 130.046
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.67   -0.56   -5.3909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.52 -0.56 -5.04]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.48    -0.96   -23.4698]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.26    -1.     -25.1674]
[-46.135   -0.81   -22.8208]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -104.1507980385438, time: 125.231
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.84    -0.435   -9.7486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.765  -0.735 -15.739]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.17    -0.865  -21.3884]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.45   -0.915 -22.032]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -93.50592530547465, time: 124.615
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.745   -0.47   -10.0844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.17    -0.605  -12.5312]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.245   -0.88   -20.7259]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.09    -0.89   -21.5697]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -100.64878110778545, time: 124.879
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.715   -0.3     -8.7344]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.85    -0.665  -15.3319]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.975   -0.865  -21.0137]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.18    -0.775  -19.3832]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -107.4922810930591, time: 127.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.63    -0.455   -9.9642]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.2    -0.875 -16.382]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.215   -0.875  -22.4824]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.535   -0.84   -17.7854]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -102.1480060524874, time: 123.99
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.45    -0.315   -8.7132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.515   -0.8    -13.4861]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.88    -0.86   -21.8754]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.875   -0.835  -18.5981]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -92.4335983475673, time: 125.971
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.945   -0.335   -8.6053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.425   -0.82   -13.6987]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.61    -0.865  -22.1508]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.42    -0.84   -19.9526]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -99.19347963414955, time: 125.553
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.455   -0.365   -8.7518]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.355   -0.6    -10.8338]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.155   -0.825  -20.9601]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.52    -0.78   -19.4824]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -92.31186203278004, time: 125.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.25   -0.35   -9.768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.015   -0.785  -14.3077]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.13    -0.915  -18.8182]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.23    -0.885  -20.1743]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -90.54059063373046, time: 128.012
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.38    -0.325   -7.2975]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.39    -0.655  -10.2385]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.05    -0.905  -19.7846]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.705   -0.915  -17.5692]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -94.0281814457661, time: 127.602
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.175   -0.36    -8.2069]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.27    -0.86   -15.2101]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.77    -0.915  -21.7362]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.235   -0.95   -16.8469]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -97.91238143609573, time: 125.763
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.575   -0.43    -8.1777]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.125   -0.8    -13.8223]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.625   -0.855  -19.6021]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.865   -0.915  -17.2339]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -97.7322038039903, time: 127.349
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.645  -0.43   -6.67 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.03    -0.93   -16.1525]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.85    -0.885  -18.8675]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.585   -0.865  -17.5857]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -98.16660001842894, time: 125.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.695  -0.37   -5.4712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.13   -0.8   -15.244]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.      -0.805  -20.4576]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.65    -0.91   -17.8876]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -97.9235823786628, time: 125.994
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.005  -0.35   -5.7231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.04    -0.845  -16.4108]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.44    -0.92   -17.5774]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.46   -0.9   -17.173]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -100.6833621860682, time: 128.355
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.955   -0.33    -7.8016]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.935   -0.83   -15.2373]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.42    -0.91   -20.5296]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.225  -0.955 -18.357]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -93.54221592407477, time: 125.086
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.74    -0.42    -8.8883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.89    -0.8    -13.7088]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.82    -0.91   -18.4481]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.395   -0.985  -19.7518]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -94.39612078441307, time: 126.748
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.95    -0.35    -7.5875]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.97    -0.845  -14.0621]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.825   -0.95   -18.5695]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.805   -0.985  -19.6841]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -99.96279142185205, time: 126.882
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.19    -0.38    -9.1143]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.965   -0.78   -14.2787]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.955   -0.96   -16.1678]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.98    -0.965  -18.0735]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -93.52885540384668, time: 127.026
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.23    -0.35    -7.3416]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.      -0.815  -12.1773]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.94    -0.95   -16.7632]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.99    -0.51    -6.6709]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.165   -0.58   -19.8238]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.565   -0.99   -25.1911]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.3    -0.305  -4.0144]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -92.1148157376554, time: 129.346
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.28   -0.34   -4.0415]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.78    -0.54   -20.8613]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.725   -0.99   -25.3104]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.725  -0.31   -3.6674]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -97.09826345084966, time: 129.366
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.49   -0.36   -4.9701]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.815   -0.515  -17.1065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.27    -0.995  -25.0718]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.875  -0.32   -5.3683]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -89.47164503638334, time: 129.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.23    -0.475   -6.3371]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.77    -0.69   -20.3365]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.1     -0.995  -24.6331]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.38   -0.44   -5.3174]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -81.97261386469648, time: 129.758
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.605   -0.47    -6.3022]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.195   -0.625  -19.9832]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.22    -0.98   -23.7159]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.375 -0.42  -5.495]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -95.6380702723205, time: 128.755
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.265   -0.39    -5.5593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.07    -0.595  -16.8008]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.645   -0.975  -23.6599]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.255  -0.465  -4.7271]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -87.11638473364852, time: 129.518
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.25    -0.45    -6.5307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.735   -0.6    -19.6486]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.465   -0.995  -23.9826]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.24   -0.465  -4.6733]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -82.13440238606265, time: 128.937
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.585   -0.48    -6.1844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.075   -0.765  -21.9895]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.485   -1.     -23.8664]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.015 -0.295 -3.841]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -82.75127220986592, time: 128.477
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.305   -0.4     -5.8112]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.565   -0.635  -18.8774]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.4     -0.99   -24.7734]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.815  -0.325  -4.3127]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -120.76251592864038, time: 128.283
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.205  -0.32   -4.1676]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.385   -0.61   -19.5741]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.005   -0.995  -23.7846]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.005   -0.455   -6.8318]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -76.98793905322592, time: 129.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.525  -0.44   -3.8679]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.945   -0.745  -22.4992]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.82    -1.     -24.0878]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.255  -0.42   -5.1775]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -89.94729106559659, time: 128.712
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.165  -0.405  -3.5595]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.685   -0.62   -19.9592]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.125   -0.995  -23.7877]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.455  -0.37   -5.0055]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -81.95623071395612, time: 129.842
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.94   -0.315  -3.5351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.64    -0.61   -19.6334]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.025   -0.98   -24.4723]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.44   -0.25   -3.0125]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -77.99194428452992, time: 128.621
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.625   -0.465   -7.4417]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.075   -0.73   -19.9688]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.735  -1.    -23.842]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.715  -0.34   -4.5616]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -79.5233873387967, time: 130.244
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.725 -0.48  -6.147]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.595   -0.775  -21.0775]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.145   -1.     -24.4907]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.97   -0.33   -4.2191]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -78.14431159312424, time: 131.311
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.67    -0.565   -6.6766]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.915   -0.655  -17.5874]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.91    -0.99   -24.3606]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.405  -0.53   -5.9335]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -86.71361860336285, time: 129.14
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.675  -0.42   -5.1096]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.65    -0.77   -20.7284]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.315   -0.995  -24.1678]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.96   -0.32   -4.9097]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -88.2597767059016, time: 128.961
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.14   -0.515  -5.1534]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.595   -0.755  -20.4495]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.985   -0.99   -23.1414]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.695  -0.47   -5.4551]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -84.1376668469889, time: 130.692
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.255   -0.395   -5.7444]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.06    -0.77   -20.5752]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.03    -0.995  -23.6455]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.95  -0.455 -7.013]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -83.64399958311417, time: 128.309
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.065  -0.29   -2.7978]
11600 50
steps: 579950, episodes: 11600, mean episode reward: -106.22026258608523, time: 125.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.32    -0.92   -21.3782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.29   -0.4    -2.7055]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.99    -0.73   -21.4617]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.72   -0.285  -2.1872]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -110.07161538682442, time: 127.437
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.7    -0.935 -22.808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.375  -0.395  -2.8008]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.085   -0.775  -21.8088]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.81  -0.355 -2.963]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -106.89827219660246, time: 126.305
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.875   -0.915  -21.9882]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.785  -0.42   -2.4457]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.365   -0.83   -21.3071]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.195  -0.345  -3.1225]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -103.76440520579261, time: 128.431
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.95   -0.93  -19.796]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.135  -0.44   -4.0468]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.58    -0.81   -22.2076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.485  -0.34   -3.7876]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -102.26345091172992, time: 126.596
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.265   -0.93   -21.6571]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.69   -0.375  -2.3916]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.54    -0.865  -22.9445]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.545  -0.34   -3.3767]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -102.58512210354722, time: 126.458
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.59    -0.97   -20.5883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.8    -0.4    -3.2086]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.45    -0.865  -22.8309]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.225  -0.355  -3.1811]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -102.78584498023461, time: 126.712
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.495   -0.98   -20.9188]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.635  -0.365  -2.8688]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.83    -0.845  -22.6417]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.585  -0.25   -2.0541]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -95.16365152178427, time: 128.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.5     -0.935  -20.3387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.77   -0.53   -4.4918]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.545   -0.875  -22.9295]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.325  -0.255  -2.5074]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -105.9646701253824, time: 127.541
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.64    -0.96   -21.2073]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.645  -0.38   -3.0834]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.98    -0.805  -22.4987]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.16   -0.42   -3.7318]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -89.222894679606, time: 127.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.11    -0.99   -22.0749]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.055  -0.455  -3.5473]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.065   -0.885  -21.4665]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.305  -0.36   -2.6239]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -94.14258185984826, time: 127.627
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.515   -0.93   -20.7049]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.14   -0.465  -3.5447]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.205   -0.895  -22.3602]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.635  -0.37   -2.9382]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -94.83323145608054, time: 128.373
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.82    -0.96   -21.0953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.07   -0.52   -4.1697]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.01    -0.89   -22.3032]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.37   -0.31   -2.6205]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -94.0351039024712, time: 127.684
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.53    -0.96   -22.1333]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.19  -0.51  -4.971]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.615   -0.92   -22.4818]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.345 -0.38  -2.723]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -96.81808686607368, time: 129.351
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.1     -0.885  -20.4292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.025  -0.455  -4.0361]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.965   -0.91   -22.0144]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.025 -0.375 -3.013]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -89.50750936106559, time: 128.293
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.76    -0.97   -20.5615]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.5    -0.43   -3.7067]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.015   -0.905  -20.4745]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.37   -0.34   -3.2381]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -99.88371983271838, time: 127.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.4     -0.93   -21.5218]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.42   -0.46   -3.7298]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.825  -0.88  -21.323]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.65   -0.37   -2.9051]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -99.31489011511057, time: 126.692
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.765   -0.96   -19.9661]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.74   -0.525  -4.6911]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.935  -0.915 -22.221]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.655  -0.245  -2.1991]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -100.56690926845845, time: 126.545
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.815   -0.99   -18.9074]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.245  -0.5    -4.5568]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.26    -0.89   -22.3366]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.32   -0.325  -2.7864]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -100.23317699018656, time: 127.492
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.99    -0.99   -21.7266]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.     -0.49   -4.2939]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.395   -0.86   -24.1117]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.935  -0.275  -2.4785]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -97.43893665667949, time: 126.188
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.93    -0.63    -9.5479]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.465   -0.99   -21.3363]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.375  -0.36   -3.8443]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.01   -0.005  -0.0084]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -115.74916712554663, time: 127.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.21    -0.76   -13.3296]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.695   -1.     -21.7058]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.955  -0.325  -3.6341]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.065  -0.01   -0.0445]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -107.85738853504876, time: 126.983
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.255   -0.76   -13.0695]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.06    -1.     -22.9389]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.475   -0.435   -7.7099]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.025   0.     -0.0143]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -103.71168576044231, time: 126.725
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.77    -0.795  -15.5371]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.84    -1.     -20.9598]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.57   -0.575 -10.487]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.075   0.     -0.0485]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -92.61867517430159, time: 127.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.585   -0.925  -15.4404]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.255   -1.     -19.1535]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.64   -0.4    -4.0745]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.03   0.    -0.021]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -103.77207592554923, time: 127.609
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.445   -0.875  -15.3081]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.525   -1.     -20.7022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.725  -0.43   -5.6031]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.075  -0.01   -0.0551]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -99.2465950105154, time: 127.966
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.185   -0.745  -12.7992]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.665   -0.975  -21.1914]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.81   -0.515  -5.6159]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.215  -0.015  -0.1409]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -101.82137664381179, time: 127.968
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.375   -1.     -15.2887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.955  -0.995 -19.573]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.265   -0.495   -8.3714]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.01   0.    -0.007]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -109.37725543693718, time: 127.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.515   -0.76   -12.0232]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.265   -0.995  -21.1272]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.04    -0.555   -7.2125]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.155 -0.01  -0.09 ]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -115.73880101768478, time: 128.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.815   -0.815  -13.7484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.165   -1.     -21.8734]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.915  -0.36   -3.9579]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.295  -0.055  -0.8743]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -99.3502535728272, time: 127.703
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.775   -0.72   -12.9365]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.35    -1.     -20.3367]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.295  -0.52   -5.1025]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02    0.     -0.0086]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -98.9300060727242, time: 128.986
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.455   -0.945  -14.7925]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.535   -0.995  -18.0504]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.955   -0.44    -6.5871]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.525  -0.01   -0.2631]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -95.29243282218904, time: 127.974
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.515   -0.965  -15.4723]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.94   -1.    -18.053]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.32   -0.465  -5.7139]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.155  -0.005  -0.0579]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -102.49500559312246, time: 129.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.685   -0.87   -14.4326]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.21    -0.995  -18.3089]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.905   -0.445   -7.3698]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.115  -0.01   -0.0806]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -104.09483778581301, time: 127.701
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.58    -0.795  -13.1055]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.31    -0.995  -18.3336]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.575   -0.485   -7.8934]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.06   -0.01   -0.0375]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -91.07647757060067, time: 127.136
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.335   -0.99   -15.3313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.3     -0.985  -15.7009]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.645  -0.455  -4.3093]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.165   0.     -0.0614]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -101.91673691994947, time: 128.336
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.73    -0.915  -14.3779]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.05    -0.995  -17.7926]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.985   -0.48    -6.1035]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.355  -0.01   -0.1606]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -92.46607136201986, time: 126.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.315  -0.915 -15.175]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.045   -1.     -19.9087]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.42    -0.525   -6.8514]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.015   0.     -0.0102]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -93.53254202449865, time: 127.508
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.28    -0.775  -12.6053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.695  -0.995 -19.939]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.595  -0.525  -4.2503]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02    0.     -0.0128]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -101.38488343163765, time: 126.587
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.66   -0.205  -2.8971]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.67   -0.175  -1.4384]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.02    -0.975  -22.2334]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.305   -0.98   -15.5686]
11800 50
steps: 589950, episodes: 11800, mean episode reward: -91.98998042296047, time: 128.744
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.97   -0.255  -2.5908]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.165  -0.205  -1.8729]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.23    -0.985  -22.4537]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.14    -0.99   -16.5533]
12000 50
steps: 599950, episodes: 12000, mean episode reward: -89.18207905349915, time: 128.034
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.565  -0.27   -3.7133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.365  -0.13   -1.1781]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.45    -0.985  -23.2574]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.91    -0.99   -17.6183]
12200 50
steps: 609950, episodes: 12200, mean episode reward: -88.3539204502165, time: 128.687
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.64  -0.275 -3.102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.22   -0.13   -1.0653]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.1     -0.985  -21.4833]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.625  -0.965 -16.469]
12400 50
steps: 619950, episodes: 12400, mean episode reward: -78.84154905575913, time: 126.458
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.11   -0.245  -2.7134]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.775  -0.095  -0.6939]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.25  -0.99 -18.68]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.24    -0.965  -14.6377]
12600 50
steps: 629950, episodes: 12600, mean episode reward: -87.82322624782974, time: 125.467
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.095  -0.21   -2.8514]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.255  -0.19   -1.1317]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.875   -0.985  -19.9392]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.735   -0.975  -13.8123]
12800 50
steps: 639950, episodes: 12800, mean episode reward: -92.44880452573915, time: 125.667
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.015  -0.19   -2.0616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.635  -0.245  -1.4688]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.515   -0.99   -19.4115]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.765   -0.985  -14.2645]
13000 50
steps: 649950, episodes: 13000, mean episode reward: -92.35614671238018, time: 125.013
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.755  -0.15   -1.0844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.98   -0.125  -0.8742]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.67    -0.97   -20.0227]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.825   -0.995  -14.4471]
13200 50
steps: 659950, episodes: 13200, mean episode reward: -85.02107538389613, time: 128.193
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.07   -0.12   -0.7142]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.825  -0.245  -1.6636]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.13    -0.975  -19.5899]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.995   -0.985  -15.5827]
13400 50
steps: 669950, episodes: 13400, mean episode reward: -83.45479334318253, time: 124.834
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.85  -0.075 -0.625]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.845  -0.36   -2.5301]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.19    -0.98   -23.2598]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.88    -0.995  -13.8663]
13600 50
steps: 679950, episodes: 13600, mean episode reward: -82.17492578655946, time: 125.937
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.725  -0.2    -1.9067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.035  -0.195  -0.9576]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.27    -0.995  -23.0773]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.845  -0.99  -13.722]
13800 50
steps: 689950, episodes: 13800, mean episode reward: -90.53462486227254, time: 125.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.725  -0.19   -1.2434]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.01   -0.17   -0.9225]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.85   -0.99  -20.969]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.355   -0.985  -15.1656]
14000 50
steps: 699950, episodes: 14000, mean episode reward: -102.21476574619872, time: 124.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.93   -0.265  -1.4949]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.255  -0.295  -3.0243]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.8     -0.995  -23.3949]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.33    -1.     -14.9653]
14200 50
steps: 709950, episodes: 14200, mean episode reward: -89.67520489665782, time: 125.599
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.93   -0.195  -0.7368]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.09   -0.265  -1.7445]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.325   -0.985  -21.8715]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.56    -0.995  -13.9238]
14400 50
steps: 719950, episodes: 14400, mean episode reward: -85.32518121135722, time: 126.045
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.175  -0.215  -1.4332]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.505  -0.23   -1.3568]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.61    -0.985  -21.2199]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.415   -1.     -14.6036]
14600 50
steps: 729950, episodes: 14600, mean episode reward: -93.69312140437454, time: 124.399
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.86   -0.155  -0.7235]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.4    -0.32   -2.0869]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.415   -0.985  -18.3532]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.615   -0.995  -15.2351]
14800 50
steps: 739950, episodes: 14800, mean episode reward: -84.85838111927023, time: 124.519
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.57   -0.235  -1.8623]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.835  -0.2    -0.7814]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.225   -0.985  -19.2609]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.695   -0.975  -12.6001]
15000 50
steps: 749950, episodes: 15000, mean episode reward: -80.75527774911887, time: 124.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.665  -0.205  -1.3077]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.29   -0.25   -1.2033]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.585   -0.985  -19.1455]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.23    -1.     -12.3687]
15200 50
steps: 759950, episodes: 15200, mean episode reward: -91.02681922179566, time: 125.256
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.91   -0.245  -1.4152]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.985  -0.135  -0.8529]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.21    -0.98   -19.3286]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.78    -1.     -12.6972]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -96.3848424046722, time: 124.725
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.03   -0.55   -5.8204]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.245   -0.93   -23.0146]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -111.04496116192946, time: 126.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.765   -0.565  -13.7046]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.925   -0.84   -21.6128]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.67   -0.455  -5.9216]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.345   -0.955  -24.4369]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -100.12496501846758, time: 127.224
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.23    -0.7    -18.3862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.72    -0.76   -19.0036]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.255 -0.445 -5.892]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.695   -0.915  -24.0122]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -98.58134530424844, time: 127.722
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.61    -0.7    -19.4914]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.945   -0.87   -21.3258]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.225  -0.495  -5.2951]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.08   -0.925 -24.785]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -106.64545698140027, time: 128.101
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.46    -0.67   -15.9386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.225   -0.82   -20.7008]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.045  -0.505  -6.4462]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.05    -0.89   -24.8004]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -102.59269850867815, time: 128.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.515   -0.685  -17.8413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.055   -0.865  -21.9065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.36    -0.47    -7.7616]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.05    -0.89   -24.2901]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -94.21673856397928, time: 127.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.15    -0.815  -21.8036]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.545   -0.9    -20.6579]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.18    -0.44    -7.1893]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.775  -0.85  -24.53 ]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -94.36870004865996, time: 128.245
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.505   -0.72   -20.4648]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.825   -0.82   -19.3801]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.235  -0.37   -6.2764]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.485   -0.85   -24.3932]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -99.61981619845092, time: 126.616
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.005   -0.71   -18.2723]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.91    -0.84   -19.8475]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.375   -0.55    -9.7986]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.47    -0.895  -24.4062]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -96.2479463176169, time: 127.275
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.195   -0.66   -16.9165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.265   -0.845  -20.4886]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.9     -0.525   -9.7001]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.48    -0.895  -23.7004]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -98.14923477288266, time: 128.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.105   -0.775  -22.0904]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.16    -0.85   -20.8793]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.225  -0.445  -7.19 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.615   -0.885  -23.7109]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -92.70758442764074, time: 129.177
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.995   -0.665  -17.3843]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.335   -0.81   -19.5793]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.075   -0.48    -7.9609]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.705   -0.95   -23.2954]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -89.90150690024664, time: 128.148
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.31    -0.675  -19.6053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.79    -0.77   -19.0707]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.79    -0.56   -10.8157]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.125   -0.9    -24.2877]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -90.57199557008144, time: 125.924
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.37    -0.59   -16.5436]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.235   -0.84   -20.4008]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.68    -0.475   -6.6218]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.86    -0.945  -22.9674]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -95.89158843531564, time: 127.123
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.49   -0.805 -22.787]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.32   -0.905 -21.908]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.185   -0.405   -7.0042]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.575   -0.885  -21.0319]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -85.58786644425201, time: 127.77
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.94    -0.58   -15.7618]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.63    -0.845  -19.7472]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.98   -0.55   -6.4832]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.945  -0.935 -23.263]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -97.23432035880134, time: 126.681
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.225   -0.68   -20.5229]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.295   -0.845  -21.2225]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.75    -0.5     -8.3027]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.365   -0.93   -23.2676]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -100.79740727959641, time: 126.861
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-40.24    -0.715  -19.6873]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.72   -0.83  -20.701]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.06   -0.43   -5.9965]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.105  -0.955 -23.499]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -88.92326779558054, time: 127.704
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.27    -0.86   -21.0956]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.37    -0.785  -19.0898]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.785  -0.525  -6.8432]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.105   -0.97   -22.8328]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -93.15531498800438, time: 128.576
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.8     -0.68   -20.3814]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.405   -0.87   -19.8545]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.185  -0.85  -24.785]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.545   -1.     -17.1368]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -82.40418600585753, time: 128.069
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.645   -0.775  -12.9284]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.875   -0.985  -25.4279]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.155   -0.84   -24.7721]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.82    -1.     -18.4732]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -83.99065979354283, time: 127.363
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.23    -0.865  -19.7909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.755   -0.94   -25.3086]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.8     -0.755  -24.4465]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.72    -1.     -18.4473]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -80.27473547896594, time: 127.562
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.86    -0.9    -18.7111]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.865   -0.98   -25.4022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.77    -0.745  -24.5174]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.67    -0.99   -18.8668]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -79.45170620616814, time: 127.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.645   -0.795   -9.9206]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.935   -0.99   -25.4591]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.61    -0.88   -25.1557]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.41    -1.     -20.5428]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -84.20671161457908, time: 129.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.37    -0.695   -9.7944]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.94    -0.99   -25.4627]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.345   -0.81   -24.9306]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.58    -1.     -19.4207]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -85.14851645359306, time: 127.989
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.51    -0.83   -12.7937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.77    -0.98   -25.3183]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.35    -0.825  -24.9306]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.675  -1.    -18.916]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -90.91614617414248, time: 129.418
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.64    -0.83   -12.4554]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.645  -0.995 -25.231]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.165   -0.82   -24.7792]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.82    -1.     -18.0041]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -81.95769959556296, time: 126.601
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.015   -0.64    -8.3165]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.125   -0.885  -24.7839]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.075   -0.85   -24.6968]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.96    -1.     -18.6785]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -88.73913556471445, time: 126.939
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.05    -0.875  -11.9483]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.24    -0.96   -24.9065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.695   -0.935  -25.2525]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.595   -1.     -19.4813]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -84.70382154985269, time: 129.182
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.55   -0.725  -8.666]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.47    -0.98   -25.0763]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.225   -0.815  -24.8117]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.87    -0.995  -19.1009]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -84.04028959784566, time: 127.895
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.09    -0.71    -8.4899]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.56    -0.98   -25.1565]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.495   -0.84   -25.0586]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.84    -0.995  -20.9176]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -99.03560840331097, time: 127.226
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.49   -0.665  -9.85 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.575   -0.97   -25.1917]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.505   -0.875  -25.1065]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.425   -1.     -19.5562]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -82.76606822032512, time: 125.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.455   -0.89   -13.6848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.71    -1.     -25.2884]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.405  -0.855 -24.999]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.355   -1.     -19.0206]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -87.19841633742791, time: 126.76
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.715   -0.885  -13.5318]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.41    -0.92   -25.0362]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.835   -0.965  -25.3748]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.1     -1.     -22.1573]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -75.35068862659716, time: 129.738
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.175  -0.83  -11.403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.63    -0.97   -25.2036]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.58    -0.86   -25.1307]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.465  -0.995 -19.468]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -84.39546185300395, time: 128.208
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.96    -0.805   -9.1833]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.55    -0.965  -25.1382]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.78    -0.94   -25.3382]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.575   -1.     -18.8042]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -76.82108792038679, time: 127.394
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.37    -0.82   -10.7191]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.69    -0.995  -25.2606]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.8     -0.865  -24.6486]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.24  -1.   -19.56]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -80.54351613417116, time: 128.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.965   -0.81   -10.5831]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.39    -1.     -25.0285]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.88    -0.835  -24.5797]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.815  -1.    -18.724]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -79.03767346040976, time: 127.69
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.33    -0.795   -8.9112]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.8    -0.53   -4.4923]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -71.96105291584354, time: 126.171
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.52   -0.84   -6.3222]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.14    -0.54    -9.5306]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.13    -0.975  -13.5656]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.165 -0.66  -6.849]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -81.58219095805636, time: 125.778
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.81   -0.755  -6.3925]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.71    -0.515   -8.0602]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.105   -0.95   -13.6493]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.295  -0.635  -5.8436]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -81.13229122848911, time: 125.742
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.14    -0.855   -7.8558]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.795   -0.54    -9.4439]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.7     -0.975  -13.6608]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.74   -0.635  -6.6158]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -78.05381598066528, time: 127.992
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.97   -0.82   -7.3973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.64    -0.635  -11.0022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.135   -0.935  -13.4751]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.345  -0.55   -5.6151]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -83.88095153192099, time: 128.505
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.095  -0.815  -7.4441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.505   -0.72   -10.9671]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.825   -0.965  -14.2569]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.32   -0.585  -7.01 ]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -80.5038391646997, time: 128.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.715  -0.725  -6.3553]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.215  -0.755 -10.067]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.765   -0.965  -14.0574]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.535  -0.585  -6.4686]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -87.79978342562652, time: 125.719
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.555  -0.775  -7.0826]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.415   -0.725  -13.1671]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.735   -0.96   -14.6007]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.44  -0.61  -6.551]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -81.44784182734254, time: 126.697
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.29   -0.815  -7.5202]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.84    -0.64   -10.3264]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.955   -0.975  -15.4049]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.465   -0.61    -8.5169]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -79.66687089378757, time: 126.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.685  -0.725  -7.4972]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.84    -0.69   -10.3267]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.12    -0.95   -14.3718]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.675   -0.615   -9.1834]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -79.51797237218825, time: 128.886
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.5   -0.78  -7.759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.165   -0.61    -8.4281]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.825   -0.955  -13.2449]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.24    -0.55    -7.4603]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -75.31749500806745, time: 126.749
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.325 -0.76  -6.911]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.605   -0.675   -8.5754]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.23    -0.99   -12.8305]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.535   -0.71   -10.1682]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -83.09374742146485, time: 125.219
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.45   -0.775  -7.4101]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.895   -0.74   -10.3192]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.15    -0.975  -13.7495]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.295   -0.745  -10.7353]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -79.77941881011415, time: 125.274
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.59   -0.72   -6.9421]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.205   -0.65    -9.0987]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.84    -0.965  -13.6166]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.615   -0.615   -8.8636]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -80.88606074495752, time: 126.418
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.485  -0.76   -7.3691]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.515   -0.745  -10.0812]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.72    -0.98   -14.4843]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.49    -0.6     -8.5068]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -88.5106991919466, time: 126.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.205  -0.815  -7.4565]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.48    -0.635   -9.6366]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.715   -0.965  -14.4978]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.09    -0.685   -9.3841]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -76.49831620986677, time: 126.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.39   -0.73   -5.3937]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.94    -0.66    -9.8255]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.105   -0.975  -13.0298]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.095  -0.56   -6.5068]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -75.73261480068996, time: 127.22
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.48  -0.835 -6.269]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.565   -0.65    -9.4598]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.39    -0.99   -13.0099]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.9     -0.585   -7.4932]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -86.22190162666108, time: 126.874
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.02   -0.84   -7.3419]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.77   -0.665 -10.768]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.885   -0.98   -13.6129]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.215   -0.505   -6.7046]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -80.29513013705886, time: 127.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.215  -0.835  -7.2681]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.075   -0.6     -9.3142]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.33    -0.97   -15.2868]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.49   -0.34   -5.2102]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.725  -0.985 -25.318]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.73    -0.965  -25.3003]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -81.82337861981972, time: 125.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.19   -0.415  -2.7618]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.225  -0.325  -4.3584]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.7     -1.     -25.2982]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.76   -0.99  -25.321]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -75.59758300341863, time: 126.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.54   -0.445  -3.4639]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.605  -0.285  -3.1631]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.845   -1.     -25.4054]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.915   -1.     -25.4573]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -78.19656022989504, time: 126.445
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.925  -0.39   -3.1827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.825  -0.34   -3.2759]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.155   -0.98   -25.0362]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.84    -0.95   -25.3647]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -81.67449798561306, time: 126.628
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.14   -0.37   -2.6569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.945  -0.26   -3.6268]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.455   -1.     -24.8407]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.77    -0.99   -25.3285]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -84.24703049642383, time: 126.765
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.895  -0.42   -3.1966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.98   -0.265  -3.1175]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.585   -1.     -24.5505]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.675  -0.99  -25.268]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -85.71726608575602, time: 126.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.335  -0.355  -2.6881]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.435  -0.305  -5.0226]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.915   -1.     -24.6867]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.67   -0.975 -25.233]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -89.12410853707192, time: 125.971
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.535  -0.37   -3.3968]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.54    -0.33    -5.9212]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.825   -1.     -23.7636]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.275   -0.99   -25.0255]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -82.21836879512, time: 126.633
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.965 -0.36  -2.503]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.215  -0.3    -3.4241]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.76    -1.     -22.4814]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.83    -0.995  -25.3906]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -74.73272584954877, time: 129.802
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.255  -0.465  -4.1953]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.805  -0.345  -4.6094]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.475   -0.99   -24.7904]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.815   -0.975  -25.3581]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -84.5633208455373, time: 128.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.78   -0.39   -3.1324]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.81   -0.32   -5.4621]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.97    -0.98   -23.3558]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -0.985  -25.4142]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -85.05464718277427, time: 127.426
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.04   -0.44   -4.0409]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.27   -0.31   -5.7675]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.645   -0.995  -24.9239]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.955   -1.     -25.4782]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -80.36211490700445, time: 127.728
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.85   -0.49   -3.9563]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.935  -0.3    -4.4367]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.375   -0.975  -24.7895]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88   -1.    -25.416]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -85.7231595663255, time: 127.097
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.785  -0.485  -4.5613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.745  -0.375  -5.6584]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.615   -0.99   -24.5445]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.89    -0.995  -25.4384]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -78.90695962302435, time: 128.281
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.755  -0.385  -2.9965]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.92   -0.28   -4.0562]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.17    -0.98   -25.0774]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.35    -0.985  -25.0517]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -79.8547961054137, time: 127.209
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.34   -0.51   -4.2067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.65   -0.335  -4.0831]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.73    -0.985  -23.6802]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.73    -0.985  -25.3137]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -74.74762272564219, time: 127.22
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.68   -0.47   -3.8863]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.9    -0.33   -4.0984]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.9     -0.995  -21.3002]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.865  -0.985 -25.428]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -76.15571985365182, time: 126.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.705  -0.555  -4.4592]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.48   -0.39   -5.7611]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.72    -0.98   -21.2287]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.79    -0.98   -25.3894]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -78.20307610995118, time: 127.594
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.36   -0.475  -4.2966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.575   -0.355   -6.7863]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.135   -1.     -22.0299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -0.99   -25.4212]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -77.79869032075788, time: 128.869
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.975  -0.52   -4.5805]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.935   -0.96   -23.3537]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -108.07487910331375, time: 128.173
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.215  -0.205  -1.0734]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.645  -0.895 -25.212]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.445   -0.845  -24.4205]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.01    -0.985  -24.0685]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -103.27821704945049, time: 129.491
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.7    -0.27   -1.4381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.395   -0.86   -24.9638]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.955   -0.89   -23.8637]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.55    -0.955  -23.2173]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -114.41180093098555, time: 127.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.255  -0.18   -1.0655]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.4     -0.85   -24.9662]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.66    -0.87   -23.7151]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.35    -0.905  -23.6829]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -106.05712690243297, time: 129.245
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.71   -0.32   -3.1161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.55    -0.825  -25.0883]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.475   -0.86   -24.0685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.38    -0.96   -22.9626]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -105.94428001559508, time: 130.039
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.14   -0.27   -3.0827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.635  -0.875 -25.186]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.085   -0.845  -22.4202]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.235   -0.95   -23.4917]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -97.88700608630333, time: 129.813
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.585 -0.195 -1.968]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.6     -0.91   -25.1661]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.135   -0.87   -22.5991]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.57    -0.96   -23.7297]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -96.86284697953153, time: 128.216
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.465  -0.225  -1.8728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.69    -0.95   -25.2486]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.01   -0.91  -21.924]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.74    -0.985  -24.5525]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -104.44935229704679, time: 128.262
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.84   -0.265  -2.0996]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.665   -0.9    -25.2043]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.495   -0.975  -22.6878]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.34    -0.95   -24.2762]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -98.92541105217714, time: 128.875
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.915  -0.22   -1.6196]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.675   -0.925  -25.2359]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.54    -0.94   -20.1985]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.345   -0.96   -24.3086]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -101.38460191051686, time: 132.916
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.24   -0.215  -1.8073]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.23    -0.93   -24.9345]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.06   -0.94  -21.387]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.085   -0.915  -24.7632]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -100.51715533327881, time: 129.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.86   -0.25   -1.5848]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.545   -0.91   -25.1454]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.665   -0.9    -24.5946]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.14   -0.91  -24.082]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -99.26261469199551, time: 129.55
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.825  -0.255  -2.5357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.785   -0.95   -25.3493]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.51    -0.955  -22.5703]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.795   -0.935  -24.5915]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -99.90075223189702, time: 127.449
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.36   -0.245  -2.2636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.795   -0.945  -25.3426]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.385   -0.965  -24.4837]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.85    -0.95   -24.6439]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -105.03082695552192, time: 130.47
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.39   -0.29   -3.9183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.705   -0.925  -25.2596]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.33    -0.945  -23.9892]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.785   -0.88   -23.8676]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -99.1214845119813, time: 129.222
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.965  -0.21   -3.2441]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.635   -0.965  -25.2316]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.705   -0.865  -23.0701]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.02    -0.95   -23.4425]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -105.3664487011011, time: 128.095
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.24   -0.2    -1.5608]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.73    -0.98   -25.3418]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.44    -0.885  -23.4692]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.13    -0.92   -23.3826]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -98.16831017702134, time: 128.367
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.43   -0.19   -1.0787]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.87    -0.99   -25.4198]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.54    -0.905  -23.3381]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.675   -0.925  -23.8182]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -96.73322889386581, time: 129.64
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.725 -0.16  -0.66 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.75    -0.985  -25.3283]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.755   -0.935  -24.6679]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.375   -0.91   -23.5702]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -105.33924878743372, time: 128.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.6    -0.205  -1.2632]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.825  -0.99  -25.396]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.27    -0.875  -24.3193]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.36    -0.95   -18.7868]
15400 50
steps: 769950, episodes: 15400, mean episode reward: -92.05641531569452, time: 124.875
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.395   -0.425   -9.1197]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.205   -0.84   -14.2805]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.045   -0.93   -17.7707]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.26    -0.96   -17.8938]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -91.712553663769, time: 124.724
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.73    -0.515  -10.2571]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.66    -0.815  -13.3922]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.335   -0.97   -18.2685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.145   -0.825  -15.7976]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -94.80786609453823, time: 125.786
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.625   -0.58    -9.1536]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.845   -0.81   -12.1953]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.105   -0.94   -18.5763]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.505   -0.87   -17.8342]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -86.57482057477817, time: 128.088
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.805   -0.5    -10.5527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.61    -0.895  -14.6842]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.795   -0.985  -16.7923]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.71    -0.92   -18.3855]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -91.75759633217069, time: 127.408
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.365   -0.45    -7.2791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.425   -0.87   -13.2284]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.445   -0.955  -17.5379]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.025  -0.94  -18.408]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -96.12314442462342, time: 127.517
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.56    -0.49    -7.9118]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.98    -0.85   -14.1432]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.505   -0.94   -15.4078]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.245   -0.88   -18.2324]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -93.67420382638804, time: 126.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.375   -0.52    -8.2306]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.635   -0.88   -14.2648]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.97   -0.905 -17.842]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.83    -0.83   -16.8973]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -92.69477758100929, time: 126.578
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.105  -0.44   -7.029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.415  -0.91  -15.007]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.815   -0.87   -16.7063]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.055   -0.87   -17.9448]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -93.17498479155569, time: 126.745
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.02   -0.44   -6.2263]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.695   -0.83   -14.4351]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.195  -0.88  -18.559]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.77   -0.745 -17.844]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -88.62380676394736, time: 128.246
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.98   -0.37   -5.3135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.685  -0.815 -11.413]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.15    -0.8    -16.9613]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.89    -0.91   -18.2265]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -87.54829570336852, time: 125.739
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.44    -0.495   -8.8295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.34    -0.83   -12.7968]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.16    -0.865  -17.6482]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.07    -0.89   -19.4657]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -91.7290805531463, time: 123.835
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.36    -0.41    -7.2089]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.055   -0.89   -15.1039]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.5     -0.855  -16.7183]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.695  -0.81  -15.693]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -87.00681758868095, time: 126.032
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.265  -0.475  -6.4808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.62    -0.91   -14.7887]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.44    -0.86   -16.5939]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.615   -0.92   -16.9593]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -92.64654026898697, time: 126.329
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.98   -0.45   -6.5648]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.415   -0.845  -12.3772]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.395   -0.935  -17.5668]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.31    -0.98   -16.7435]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -88.04144022812196, time: 127.193
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.78    -0.415   -7.1616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.665  -0.885 -13.75 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.62    -0.935  -19.5878]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.155   -0.955  -18.0168]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -94.65882528564978, time: 127.398
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.465   -0.425   -7.9566]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.785   -0.87   -14.3367]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.52    -0.935  -19.4963]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.305  -0.85  -16.752]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -86.80090037173339, time: 126.558
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.735   -0.575   -8.0781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.415   -0.82   -11.6774]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.15    -0.855  -16.7437]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.635   -0.96   -11.8218]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -80.93170744252124, time: 126.518
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.735  -0.415  -5.9295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.05    -0.875  -12.7779]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.04    -0.935  -18.2097]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.345   -0.95   -16.5061]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -86.71307795283427, time: 126.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.625   -0.49    -8.0581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.345   -0.93   -17.0508]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.015  -0.455  -5.2135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.8     -0.7    -19.1511]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.705   -0.99   -23.1762]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.71   -0.38   -5.6702]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -91.19043238787053, time: 127.693
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.585   -0.425   -6.2102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.83    -0.595  -19.2901]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.465   -0.99   -22.9929]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.05   -0.4    -5.1307]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -75.99268957372358, time: 127.942
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.465  -0.345  -2.9409]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.78    -0.67   -19.9105]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.095   -0.995  -23.2088]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.62   -0.405  -6.2165]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -92.90673523065236, time: 128.922
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.535  -0.375  -3.2246]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.655   -0.705  -20.9177]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.6     -0.985  -23.3342]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.61  -0.395 -5.52 ]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -87.38208263844183, time: 129.4
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.22    -0.54    -8.8132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.16    -0.795  -21.5307]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.055   -1.     -24.1386]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.905  -0.44   -7.825]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -89.84400152568922, time: 129.063
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.36    -0.6     -6.8625]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.145   -0.61   -18.1854]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.05   -0.985 -21.43 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.18   -0.46   -6.4942]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -79.06187208543777, time: 129.722
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.52   -0.62   -5.5735]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.9    -0.765 -21.682]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.735   -0.99   -22.5859]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.65   -0.345  -5.0188]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -82.20800164534545, time: 129.107
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.505   -0.67   -14.6419]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.485   -0.785  -22.3471]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.68    -0.985  -23.9299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.765  -0.4    -6.1118]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -88.39241409807283, time: 127.675
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.615   -0.61    -9.1675]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.735   -0.705  -22.4225]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.55    -0.985  -23.0109]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.575  -0.395  -5.9438]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -79.99488882039331, time: 130.839
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.565   -0.475   -6.1886]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.83    -0.865  -20.0143]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.905   -0.995  -23.6355]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.925   -0.5     -7.2586]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -85.1934003130241, time: 128.346
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.745  -0.375  -5.0183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.41    -0.9    -22.8412]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.805   -1.     -23.4662]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.905 -0.4   -6.054]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -89.61328206375369, time: 128.608
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.     -0.38   -5.0495]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.86    -0.81   -21.9449]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.925   -1.     -23.4654]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.48    -0.44    -6.7441]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -83.28470767957623, time: 127.557
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.335   -0.375   -6.8516]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.22    -0.85   -21.3796]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.38    -0.99   -22.6683]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.12    -0.48    -6.2444]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -93.23545465765199, time: 128.526
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.87    -0.43    -7.9976]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.385   -0.78   -19.8249]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.915  -0.995 -22.633]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.535  -0.43   -4.6954]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -83.13577714940136, time: 127.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.63  -0.425 -4.983]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.67    -0.83   -22.3528]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.255   -0.985  -23.0269]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.385  -0.415  -5.7469]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -79.47958626650379, time: 126.839
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.635   -0.52    -7.6501]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.62    -0.84   -20.2648]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.395   -0.98   -23.5934]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.7    -0.46   -6.6854]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -87.49424796509454, time: 129.174
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.51    -0.44    -6.0635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.155  -0.75  -19.383]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.175   -0.975  -23.6919]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.49   -0.295  -5.7756]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -84.4628375397158, time: 127.164
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.64    -0.455   -7.9905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.53    -0.78   -20.1796]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.57    -0.985  -24.1409]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.35   -0.445  -6.0409]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -92.73720565952, time: 128.193
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.11    -0.395   -6.7971]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.15    -0.825  -21.0744]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.13    -0.99   -23.9749]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.815  -0.505  -6.0899]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -79.82221222718438, time: 128.015
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.635   -0.49    -7.2309]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.85    -0.995  -20.3974]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.35   -0.525  -3.8259]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.18    -0.895  -22.6154]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.43   -0.325  -2.1376]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -96.0012108961123, time: 127.927
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.395   -0.975  -18.6845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.48   -0.52   -3.8822]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.89    -0.88   -20.1375]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.185  -0.41   -3.4067]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -93.68709865928201, time: 126.465
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.07    -0.93   -19.4318]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.415  -0.48   -2.9711]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.62    -0.86   -22.3206]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.58   -0.325  -2.9895]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -99.94433583626055, time: 126.63
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.32    -0.86   -18.4717]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.005  -0.45   -2.6658]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.35    -0.855  -22.0414]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.745  -0.35   -2.9849]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -96.0926951752837, time: 127.092
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.265   -0.93   -18.2859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.73  -0.565 -3.245]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.675   -0.815  -22.1439]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.045  -0.305  -2.3899]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -92.53992590148475, time: 128.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.405   -0.985  -20.1989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.42   -0.505  -2.9952]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.58    -0.88   -22.2017]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.06   -0.33   -2.5266]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -98.48245192236685, time: 127.332
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.34    -0.975  -18.5484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.495  -0.49   -2.9875]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.06    -0.885  -22.2346]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.99   -0.37   -2.5801]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -93.23539329249968, time: 126.733
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.615   -0.985  -15.7548]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.16   -0.475  -3.4416]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.695   -0.945  -19.4564]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.735  -0.375  -2.3794]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -95.73849983623714, time: 126.486
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.88   -0.98  -19.972]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.25   -0.555  -4.3544]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.455   -0.845  -19.1979]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.67   -0.42   -3.5616]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -94.3626013920251, time: 130.535
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.87   -0.955 -15.655]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.52   -0.575  -4.4941]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.44    -0.895  -21.0004]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.85   -0.355  -2.3474]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -90.65962181482901, time: 128.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.975   -0.99   -19.8149]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.745  -0.565  -4.0266]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.965  -0.935 -18.855]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.415  -0.52   -4.3027]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -106.14047115358258, time: 126.383
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.66   -0.99  -20.307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.04   -0.525  -4.0379]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.285   -0.955  -21.1294]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.795  -0.365  -3.0874]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -108.48050175766606, time: 126.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.865   -0.965  -18.8672]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.015  -0.53   -3.9955]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.435   -0.91   -20.5602]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.    -0.445 -4.077]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -94.11004543995642, time: 128.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.495  -1.    -17.694]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.36   -0.5    -3.7069]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.45    -0.92   -20.9533]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.3    -0.43   -2.7957]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -103.81485045890062, time: 128.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.79    -0.99   -17.1878]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.42   -0.495  -4.9612]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.625   -0.91   -20.8176]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.875  -0.36   -2.9198]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -96.71173449823556, time: 125.794
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.075  -0.99  -17.767]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.44   -0.49   -5.0643]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.3     -0.845  -19.2543]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.98   -0.46   -2.9364]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -87.29197100748843, time: 127.019
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.33    -1.     -18.5984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.71  -0.505 -3.948]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.88    -0.855  -18.5196]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.805  -0.435  -2.3658]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -108.3801983312078, time: 127.998
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.585   -0.99   -18.8429]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.975  -0.555  -4.8691]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.925   -0.815  -20.6604]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.24   -0.425  -2.7259]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -103.13300913219022, time: 128.391
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.665   -0.985  -17.9611]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.905  -0.515  -4.7746]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.375   -0.875  -18.6256]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.365 -0.435 -2.798]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -93.382170025023, time: 128.8
agent0_energy_min, agent0_energy_max, agent0_energy_avg

15400 50
steps: 769950, episodes: 15400, mean episode reward: -64.35892166108698, time: 127.419
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.935  -0.57   -5.3059]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.14   -0.51   -4.7828]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.61    -0.995  -22.5919]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.93    -0.995  -25.0274]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -77.21066651134598, time: 127.791
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.945   -0.66    -7.5122]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.565  -0.465  -4.2246]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.025   -0.98   -23.4753]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.905   -1.     -24.6309]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -81.25203556191033, time: 127.037
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.215   -0.615   -8.7288]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.985  -0.245  -2.1241]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.9     -0.985  -23.2148]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.82    -0.99   -25.3997]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -70.50215698010078, time: 127.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.14   -0.515  -6.718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.605  -0.31   -2.9505]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.87    -0.99   -20.6253]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.755   -0.995  -25.3742]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -78.00184553396562, time: 127.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.845   -0.755  -10.9313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.935  -0.46   -4.8496]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.69    -1.     -24.1172]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.775   -1.     -25.3744]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -71.69877496457097, time: 127.635
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.61   -0.59   -5.4785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.665  -0.385  -3.7672]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.7     -0.985  -24.5195]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.895   -0.995  -25.4465]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -72.4764253890386, time: 124.542
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.485   -0.595   -7.8185]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.58   -0.455  -4.2457]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.685   -0.98   -23.2127]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.815   -1.     -25.4123]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -75.09949092939641, time: 125.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.675   -0.605   -9.6977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.905  -0.54   -5.4327]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.76    -0.985  -22.6766]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.515   -0.985  -25.2019]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -66.96034602444428, time: 124.91
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.79    -0.59    -6.2782]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.455  -0.365  -2.6766]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.125   -1.     -23.2841]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.19    -0.9    -22.8264]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -75.8171325693204, time: 127.898
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.74  -0.47  -3.447]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.5    -0.31   -2.8825]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.325   -0.995  -22.6197]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.85    -1.     -25.4115]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -77.90377245593618, time: 124.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.105  -0.465  -4.0232]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.11   -0.4    -4.3762]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.285   -0.975  -23.7785]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.685   -0.995  -25.3018]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -83.11115409331069, time: 123.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.85    -0.54    -7.1603]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.045  -0.515  -5.0734]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.835  -0.995 -24.112]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.795  -1.    -25.367]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -66.64426900557443, time: 123.579
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.405  -0.56   -5.1392]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.22    -0.575   -6.3335]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.895   -1.     -23.7306]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.905   -1.     -25.4492]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -73.98483972643902, time: 125.301
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.94  -0.61  -6.387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.93   -0.465  -5.3776]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.53    -0.995  -21.8371]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -1.     -25.4399]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -85.17926547789017, time: 125.288
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.75   -0.655  -8.82 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.035  -0.365  -4.3369]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.075   -0.985  -22.1213]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -1.     -25.4417]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -69.48185040198109, time: 125.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.925   -0.73   -10.9526]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.045  -0.345  -3.8426]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.65    -0.985  -20.6257]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.745   -0.995  -25.3534]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -75.76021486579124, time: 125.766
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.9    -0.625  -6.3546]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.375  -0.47   -5.1925]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.02   -1.    -22.707]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.92   -1.    -25.458]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -75.53148262712979, time: 125.68
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.27    -0.65    -7.1255]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.01   -0.445  -4.6371]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.98    -1.     -22.7065]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.915   -1.     -25.4597]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -64.92673051767191, time: 126.178
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.675  -0.62   -7.68 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.21    -0.495   -9.5348]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.035  -0.995 -24.951]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.89    -1.     -25.4505]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -67.41659466054033, time: 126.271
[-25.865   -0.85   -14.4919]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.345   -0.99   -18.9585]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.27   -0.41   -4.2917]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.005  0.    -0.004]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -103.08108710690729, time: 126.111
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.31    -0.805  -13.1922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.72    -1.     -21.4836]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.19    -0.465   -6.0202]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.065   0.     -0.0315]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -98.11343631446181, time: 125.624
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.01    -0.975  -16.4852]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.43    -0.995  -21.0769]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.745  -0.54   -5.4611]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.015   0.     -0.0107]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -95.0903808752661, time: 129.031
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.69   -0.905 -15.788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.73    -1.     -21.3341]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.38   -0.47   -4.5272]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.045  -0.005  -0.0272]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -101.8313720090843, time: 128.654
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.975   -0.825  -15.4398]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.31    -1.     -20.5656]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.65    -0.55    -6.4895]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02   -0.005  -0.0124]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -105.60561615869833, time: 128.514
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.505   -0.95   -13.9055]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.425   -1.     -20.9948]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.585 -0.5   -5.141]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.045   0.     -0.0174]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -103.20620193119406, time: 126.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.35    -0.96   -15.3063]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.175   -1.     -22.0447]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.53   -0.405  -5.1353]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02    0.     -0.0088]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -88.8957714075587, time: 126.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.67    -0.925  -14.6486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.565  -1.    -20.775]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.095   -0.535   -7.6552]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.17    0.     -0.0743]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -92.3114932944267, time: 127.636
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.56    -0.76   -14.0719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.59    -0.99   -21.8691]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.07    -0.535   -6.4001]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.135   0.     -0.0502]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -100.8676274550708, time: 132.096
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.055   -0.875  -16.2216]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.245   -0.995  -20.2324]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.015  -0.54   -5.1485]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.895  -0.045  -1.6623]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -93.85977618618902, time: 129.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.975   -0.865  -14.7577]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.74    -0.995  -19.5893]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.82   -0.47   -5.8332]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.055  -0.015  -0.0386]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -96.50338855198774, time: 126.943
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.66    -0.975  -16.1889]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.52    -0.995  -18.6299]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.555  -0.47   -4.2995]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02    0.     -0.0087]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -91.27466232138093, time: 126.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.505   -0.99   -15.0065]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.51    -0.98   -20.5289]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.005  -0.525  -6.0053]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.085   0.     -0.0232]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -91.6329783555611, time: 129.748
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.415   -0.945  -14.9756]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.85    -1.     -18.9584]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.29   -0.5    -5.2643]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.025   0.     -0.0139]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -104.27720634997073, time: 128.576
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.44    -0.995  -14.6576]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.74    -0.995  -18.6457]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.965  -0.495  -6.8139]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02    0.     -0.0086]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -95.99874682691343, time: 127.679
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.725   -0.96   -15.2561]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.93    -0.995  -18.8092]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.185   -0.525   -9.6522]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.105  -0.02   -0.0865]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -87.76754602695874, time: 129.314
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.745   -0.995  -15.0357]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.135   -0.995  -19.2251]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.7     -0.575   -7.7013]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.355  -0.01   -0.1965]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -82.01858649718491, time: 127.475
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.7     -0.93   -15.8932]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.07    -1.     -17.4618]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.34    -0.485   -7.2556]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[0. 0. 0.]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -117.44243741521838, time: 127.846
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.68    -0.94   -14.5688]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.5     -0.99   -22.8299]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.915   -0.515  -10.0432]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02    0.     -0.0079]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -89.7248649165468, time: 130.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.74   -0.935 -13.945]
[-2.5    -0.3    -1.9436]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.405  -0.075  -0.3648]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.02    -0.985  -17.1287]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.59    -1.     -14.0693]
15600 50
steps: 779950, episodes: 15600, mean episode reward: -96.04976183185313, time: 123.299
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.095  -0.24   -1.6004]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.09   -0.22   -0.9913]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.36    -0.995  -20.7847]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.98    -0.995  -14.2924]
15800 50
steps: 789950, episodes: 15800, mean episode reward: -84.95564124225109, time: 124.822
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-0.835 -0.115 -0.649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.18   -0.235  -1.1042]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.315   -0.99   -17.3735]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.515  -0.995 -15.478]
16000 50
steps: 799950, episodes: 16000, mean episode reward: -89.34076047399758, time: 125.293
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.505 -0.19  -1.102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.825  -0.34   -1.6777]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.135   -0.995  -17.5868]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.145   -1.     -16.3659]
16200 50
steps: 809950, episodes: 16200, mean episode reward: -81.5485111682358, time: 127.592
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.255  -0.155  -1.0511]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.4   -0.31  -1.306]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.715   -0.975  -18.0569]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.465  -1.    -13.066]
16400 50
steps: 819950, episodes: 16400, mean episode reward: -95.8466254428439, time: 124.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.185  -0.14   -0.8444]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.205  -0.215  -1.0938]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.015  -0.975 -21.96 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.34    -0.99   -14.1464]
16600 50
steps: 829950, episodes: 16600, mean episode reward: -89.55640258305294, time: 125.645
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.8    -0.235  -2.0289]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.755  -0.255  -1.5323]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.71    -0.99   -18.1239]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.53    -0.99   -15.9804]
16800 50
steps: 839950, episodes: 16800, mean episode reward: -90.12132510059193, time: 124.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.345  -0.28   -2.4732]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.945  -0.355  -2.3937]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.11    -1.     -17.2815]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.805   -1.     -13.6659]
17000 50
steps: 849950, episodes: 17000, mean episode reward: -97.47932169283966, time: 125.862
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.07   -0.285  -2.3361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.565  -0.295  -2.1675]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.13    -0.975  -17.9097]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.925   -0.99   -14.8306]
17200 50
steps: 859950, episodes: 17200, mean episode reward: -90.83989523274249, time: 128.36
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.76   -0.21   -2.0524]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.8    -0.25   -1.5205]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.775   -0.985  -18.2556]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.965   -0.985  -15.5537]
17400 50
steps: 869950, episodes: 17400, mean episode reward: -84.48199077019541, time: 123.876
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.79  -0.345 -3.005]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.5    -0.31   -2.0919]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.575   -0.985  -16.2883]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.915   -0.985  -16.3561]
17600 50
steps: 879950, episodes: 17600, mean episode reward: -87.13973007929957, time: 125.359
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.675  -0.24   -2.0136]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.83   -0.33   -2.9917]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.895   -0.975  -18.8893]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.22    -0.995  -13.2313]
17800 50
steps: 889950, episodes: 17800, mean episode reward: -92.93269157538964, time: 123.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.18   -0.165  -1.6472]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.63   -0.4    -3.0891]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.55    -0.95   -16.8381]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.595   -0.995  -14.8573]
18000 50
steps: 899950, episodes: 18000, mean episode reward: -88.59540269526333, time: 124.393
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.72   -0.26   -2.7542]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.84   -0.35   -1.6847]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.64    -0.98   -16.9243]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.77    -1.     -13.3383]
18200 50
steps: 909950, episodes: 18200, mean episode reward: -82.16195479769331, time: 126.359
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.405  -0.44   -4.8342]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.08   -0.225  -0.9841]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.155   -0.955  -15.8493]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.455   -0.99   -13.8153]
18400 50
steps: 919950, episodes: 18400, mean episode reward: -86.1036694066977, time: 126.317
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.1    -0.345  -4.9388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.78   -0.11   -0.6528]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.785   -0.935  -15.3065]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.01    -0.99   -14.7578]
18600 50
steps: 929950, episodes: 18600, mean episode reward: -89.01196643279833, time: 126.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.76   -0.285  -3.0588]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.96   -0.29   -2.3077]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.64    -0.955  -17.3106]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.135  -0.975 -14.438]
18800 50
steps: 939950, episodes: 18800, mean episode reward: -90.36782241717562, time: 126.826
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.335  -0.295  -3.3591]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.39   -0.185  -1.1069]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.27    -0.925  -16.6978]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.63    -0.995  -12.5493]
19000 50
steps: 949950, episodes: 19000, mean episode reward: -86.12311482685077, time: 125.384
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.035  -0.31   -3.7853]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.445  -0.25   -1.8658]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.8     -0.985  -17.3742]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.135   -1.     -12.7085]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -86.85430716805955, time: 125.917
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.225  -0.37   -4.1084]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.685  -0.465  -6.0725]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.965   -0.925  -22.1147]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -104.23071595474053, time: 129.644
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.675   -0.76   -19.6995]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.19    -0.725  -19.5908]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.78    -0.53    -7.0837]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.7     -0.94   -23.2287]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -95.63429028350198, time: 127.486
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-46.655   -0.915  -23.4198]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.975   -0.825  -19.7065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.29    -0.47    -7.4747]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.675   -0.93   -24.0861]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -100.07247410058005, time: 128.199
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.63    -0.72   -21.5338]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.935   -0.77   -20.3453]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.225   -0.535   -8.9405]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.625   -0.93   -23.3088]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -94.57074878569523, time: 127.93
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.32    -0.675  -19.0578]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.78    -0.85   -19.9633]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.265   -0.595  -10.3908]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.29    -0.915  -23.4693]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -99.80435028200438, time: 126.91
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.95    -0.645  -16.7647]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.625   -0.82   -20.5054]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.805   -0.57    -9.3295]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.22   -0.93  -23.938]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -98.09481505187593, time: 128.487
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.265   -0.715  -21.5943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.67    -0.8    -19.1101]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.42    -0.495  -11.3402]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.255   -0.935  -23.8704]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -94.25600290623757, time: 126.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-42.43    -0.795  -20.8919]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.59    -0.805  -21.0526]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.605   -0.585   -9.7933]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.945   -0.965  -24.7884]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -88.30691686302522, time: 126.504
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.205   -0.73   -22.1686]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.46    -0.865  -20.5672]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.92   -0.45   -6.3212]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.255  -0.96  -24.557]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -91.85288062521478, time: 128.18
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.765  -0.745 -22.699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.735   -0.86   -21.3454]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.75    -0.53    -8.6581]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.095   -0.945  -24.4521]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -95.16146086932156, time: 128.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.98    -0.93   -24.0633]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.93    -0.775  -21.5936]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.015   -0.555   -8.6875]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.04    -0.945  -24.3023]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -108.73963449783386, time: 130.079
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.2     -0.65   -18.3163]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.705   -0.74   -19.9924]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.3     -0.53    -8.9434]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.985   -0.96   -24.4367]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -94.49250641252527, time: 129.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.68    -0.685  -21.9844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.1    -0.72  -20.179]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.39    -0.575   -9.1707]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.155   -0.985  -23.9252]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -91.50220519857209, time: 127.874
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.015   -0.78   -23.4154]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.345   -0.78   -21.0781]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.26    -0.525   -8.6836]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.165   -0.975  -23.3549]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -90.62575745926985, time: 126.218
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-47.855   -0.785  -23.8737]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.345   -0.825  -21.5776]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.285   -0.62   -11.3242]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.33    -0.955  -23.4507]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -91.483386641018, time: 129.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-45.64    -0.745  -22.3808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.905   -0.97   -22.3297]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.93    -0.74   -12.8769]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.46    -0.995  -24.7491]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -94.13142640401409, time: 130.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.02    -0.63   -20.3134]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.07    -0.975  -23.6969]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.96    -0.735  -12.3386]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.27    -0.99   -24.7405]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -89.04339764713978, time: 129.602
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-44.41    -0.71   -21.2846]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.84    -0.99   -23.1182]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.06    -0.825  -14.1877]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.515   -1.     -24.8445]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -87.6566570783195, time: 128.244
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.665   -0.675  -20.8157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.01    -0.94   -21.1876]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.37    -0.765  -12.1126]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.725   -1.     -24.4979]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -86.8495617950648, time: 129.684
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.285   -0.69   -20.5335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.69    -0.935  -23.3838]
[-49.775   -0.995  -25.3371]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.62    -0.88   -25.2001]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.405   -1.     -21.0309]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -80.47522067962299, time: 129.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.645   -0.785  -10.4785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.605   -0.985  -25.1727]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.795   -0.915  -25.3315]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.68    -1.     -17.2392]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -77.15892818278053, time: 128.406
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.8     -0.715   -8.2719]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.745   -0.995  -25.2929]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.275  -0.82  -24.886]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.02    -1.     -18.9957]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -76.57544348816455, time: 128.312
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.915   -0.795  -11.2283]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.37    -0.985  -24.9903]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.635   -0.895  -25.2077]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.12    -1.     -18.9279]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -80.93568778064471, time: 127.957
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.42    -0.815  -11.9913]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.725   -0.995  -25.2859]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.77    -0.935  -25.3292]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.425   -1.     -20.3657]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -82.15850334541709, time: 128.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.295   -0.725   -8.1822]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.565   -1.     -25.1514]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.66    -0.86   -25.2201]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.195   -1.     -19.4973]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -72.2318527999455, time: 128.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.46    -0.725   -9.6023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.785   -1.     -25.3342]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.855   -0.96   -25.3903]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.12    -1.     -17.9143]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -82.56430663304555, time: 128.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.355   -0.86   -14.3223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.675   -1.     -25.2571]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.765   -0.945  -25.3178]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.17    -1.     -18.2516]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -81.33249523735995, time: 128.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.06    -0.925  -15.1859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.46    -1.     -25.0923]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.965   -0.81   -24.6044]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.      -1.     -18.5623]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -73.61802759940988, time: 126.801
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.91    -0.91   -14.3895]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.73   -1.    -25.284]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.79    -0.96   -25.3574]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.535   -0.995  -18.9122]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -83.91156897764246, time: 127.766
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.36   -0.79  -10.104]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.575   -1.     -25.1795]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.64    -0.94   -25.2456]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.745   -1.     -18.2192]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -86.27178382612627, time: 130.553
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.95    -0.805   -7.9811]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.67    -1.     -25.2621]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.605  -0.985 -25.213]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.425   -1.     -18.4904]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -75.77998706921277, time: 128.475
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.225   -0.865   -8.3712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.89    -1.     -25.4159]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.795   -0.98   -24.6867]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.715   -1.     -19.3864]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -73.97009012667687, time: 127.58
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.41    -0.9    -10.8088]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.775   -1.     -25.3299]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.52    -0.99   -25.2302]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.89    -1.     -16.7982]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -74.44511224722493, time: 127.902
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.425   -0.685   -7.4854]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.66    -1.     -25.2456]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.65    -0.995  -25.2434]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.175   -0.995  -18.4776]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -78.50571397191045, time: 130.406
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.16   -0.81  -10.617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.8     -1.     -25.3482]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.63    -0.99   -25.2566]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.11    -1.     -18.4838]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -74.88577507339116, time: 131.19
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.74    -0.805  -10.1584]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.84    -0.995  -25.4039]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.765   -0.995  -25.3514]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.445   -1.     -17.3862]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -72.2057286475183, time: 128.669
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.43    -0.805  -10.7203]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.905   -0.995  -25.4382]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.71    -1.     -25.2979]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.615   -1.     -14.7789]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -99.75964001074895, time: 129.094
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.03    -0.75   -10.9825]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.885   -0.995  -25.4203]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.62    -0.995  -25.3066]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.435   -1.     -16.2568]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -83.73539797263419, time: 128.998
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.955  -0.315  -5.1522]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.965   -0.99   -22.2921]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.81    -1.     -25.4001]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -80.39060112936409, time: 126.645
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.13   -0.49   -4.0047]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.59   -0.345  -4.5346]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.15    -1.     -21.2855]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -1.     -25.4245]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -78.28474449147573, time: 127.652
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.05   -0.39   -3.7203]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.255  -0.28   -4.3669]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.005   -1.     -22.0861]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -0.99   -25.4216]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -80.26633377986944, time: 126.743
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.44   -0.545  -5.6621]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.33   -0.36   -5.9183]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.54    -0.97   -24.9014]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.91    -1.     -25.4661]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -83.5797224275978, time: 126.089
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.685  -0.525  -6.2365]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.915  -0.355  -6.2085]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.64    -1.     -24.4906]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.755   -0.985  -25.3337]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -77.43848293810595, time: 128.631
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.04    -0.6     -6.8748]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.92   -0.31   -5.4492]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.635   -0.98   -23.4919]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.705  -0.945 -25.293]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -72.61401177542938, time: 125.332
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.18   -0.545  -4.5529]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.755  -0.325  -4.4939]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.185   -0.99   -21.3158]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.5     -0.965  -25.1085]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -78.30355480878988, time: 127.623
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.035  -0.525  -5.1759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.605  -0.37   -6.3221]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.3     -1.     -21.8726]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.77    -0.99   -25.3176]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -78.82156562827141, time: 127.804
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.33   -0.475  -4.6328]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.85  -0.31  -4.734]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.785   -0.995  -21.4711]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.655  -0.995 -25.247]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -68.40670834652725, time: 126.417
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.975  -0.55   -6.2003]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.24   -0.39   -4.9728]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.72    -0.98   -21.3691]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.62   -0.995 -25.198]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -76.20402870881414, time: 129.242
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.175   -0.535   -6.6508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.125  -0.37   -4.1945]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.47    -0.99   -22.3768]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.495   -0.99   -25.1336]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -77.5190878145737, time: 128.88
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.44   -0.555  -5.7334]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.355   -0.355   -6.2928]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.8     -0.99   -21.3218]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.89    -1.     -25.4376]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -77.35672737724997, time: 127.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.525  -0.525  -4.7594]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.185   -0.365   -6.3476]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.035   -0.975  -20.4819]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -0.98   -25.4049]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -74.72253719941092, time: 127.002
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.23   -0.515  -5.5125]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.23    -0.425   -6.7757]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.91    -0.995  -20.5255]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.835  -0.975 -25.389]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -75.35638021876213, time: 127.6
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.74   -0.52   -5.5806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.37    -0.55    -7.5465]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.155   -1.     -20.5188]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.905   -0.995  -25.4355]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -82.88982124648908, time: 129.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.16   -0.565  -6.7989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.745   -0.495   -6.9211]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.39    -1.     -21.6924]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.86    -1.     -25.4105]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -73.34241084949673, time: 127.201
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.075   -0.58    -7.4207]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.89    -0.655   -9.7188]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.76   -1.    -21.658]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.935   -0.99   -25.4591]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -75.62500448836477, time: 128.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.445   -0.675   -8.3473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.825   -0.745   -9.3366]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.865  -1.    -21.707]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.88    -1.     -25.4276]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -76.71230450890843, time: 128.799
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.125   -0.715   -8.2948]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.315   -0.78   -10.1243]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.5     -0.99   -22.0321]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.83    -1.     -25.3848]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -74.46865641841323, time: 127.211
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.53    -0.825   -9.3232]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.085   -0.535   -6.8478]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -75.68792511608258, time: 129.385
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.41   -0.79   -6.7814]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.805   -0.645   -8.5675]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.85    -0.965  -13.6187]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.84    -0.58    -8.1978]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -88.9513260993125, time: 127.236
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.765  -0.71   -7.3519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.11    -0.665  -10.0726]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.62    -0.995  -14.6237]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.35    -0.75   -10.7178]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -83.36591491960905, time: 127.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.48   -0.785  -6.9196]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.345   -0.77   -11.0504]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.14    -0.96   -14.2579]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.025   -0.715   -9.1328]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -71.84491203707154, time: 127.561
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.285  -0.825  -7.3219]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.86    -0.705   -7.6256]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.01    -0.95   -13.7525]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.09    -0.76    -9.2183]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -83.21798998670997, time: 126.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.04   -0.765  -7.1768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.845   -0.59    -7.6429]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.065   -0.975  -15.8805]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.875   -0.67    -8.0367]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -84.60082174310037, time: 128.008
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.47   -0.655  -5.9613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.63    -0.675   -7.9031]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.315   -0.98   -15.0304]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.095   -0.685   -7.7208]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -82.22768412820031, time: 126.953
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.41   -0.745  -6.6567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.89    -0.74   -10.6184]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.18    -0.975  -16.9595]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.055   -0.625   -7.9231]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -93.6205603047879, time: 127.022
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.875  -0.69   -6.7025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.305   -0.7     -8.5801]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.965   -0.98   -17.6519]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.09    -0.78    -9.9933]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -77.2512753342991, time: 126.511
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.31    -0.845   -9.0044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.655   -0.795  -10.5752]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.89    -0.98   -13.8808]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.25    -0.745   -9.6559]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -74.31944664496704, time: 128.064
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.085   -0.865   -8.2089]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.23    -0.785   -9.4897]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.545  -0.985 -13.417]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.19    -0.655   -9.0314]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -85.14197574448761, time: 129.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.01    -0.725   -7.9939]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.75    -0.8    -10.0963]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.94    -0.99   -15.8311]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.085  -0.69  -10.164]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -81.89138633234141, time: 127.968
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.515   -0.7     -7.9673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.17    -0.815  -10.2073]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.175   -0.99   -15.0933]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.32    -0.77    -9.2006]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -71.32715964020846, time: 127.114
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.905   -0.785   -7.8072]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.06    -0.915  -10.9474]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.18    -0.98   -13.0548]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.995   -0.75    -8.4718]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -81.63302358666996, time: 128.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.39    -0.79    -7.9839]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.67    -0.935  -11.4389]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.79    -1.     -13.8046]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.93    -0.7     -8.4749]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -79.70480438838135, time: 129.273
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.43    -0.87    -8.7015]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.635   -0.935  -12.5978]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.695   -0.98   -14.1814]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.445   -0.755  -10.3515]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -82.07023051352604, time: 127.625
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.42    -0.865   -8.1924]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.685   -0.96   -12.9291]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.96    -0.995  -14.0685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.325   -0.85    -9.9813]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -81.06997904809944, time: 128.986
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.49    -0.9     -9.4707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.8     -0.925  -11.9881]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.35    -0.985  -14.1748]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.965   -0.81    -9.4711]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -72.20169620877058, time: 128.041
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.125   -0.84    -8.6394]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.855   -0.965  -11.9255]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.115   -0.985  -13.1618]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.52    -0.805   -9.8281]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -78.87407902498524, time: 126.81
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.015   -0.89    -9.4668]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.04    -0.97   -13.6377]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.565   -0.985  -12.6945]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.75    -0.97   -23.9168]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -102.1646498823572, time: 131.683
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-1.405 -0.21  -1.22 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.715   -0.985  -25.3419]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.86    -0.97   -24.7474]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.895   -0.965  -24.8013]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -105.51713027340446, time: 129.83
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.64   -0.29   -2.4478]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.805   -0.975  -25.3785]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.52    -0.98   -25.1477]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.285   -0.985  -24.9997]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -113.78329461139026, time: 127.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.195  -0.24   -1.6787]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595  -0.965 -25.266]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.625   -0.95   -24.0237]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.405   -0.96   -24.4053]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -104.28804897625116, time: 127.901
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.88   -0.26   -2.0709]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.775   -0.975  -25.3627]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.295   -0.88   -24.9254]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.32    -0.94   -24.3126]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -105.86173368795606, time: 129.153
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.595  -0.295  -2.4552]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.88    -0.965  -25.4022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.59    -0.905  -25.1418]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.455   -0.93   -23.2576]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -97.92224446338952, time: 129.67
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.785  -0.37   -3.3874]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.815   -0.985  -25.3894]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.285   -0.965  -25.0319]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.92    -0.98   -24.7285]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -92.7609342998435, time: 128.339
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.81   -0.29   -2.5703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.92    -0.955  -24.8393]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.43    -0.96   -24.5228]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.195   -0.95   -23.8582]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -103.07410785803027, time: 129.629
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.175  -0.32   -2.3883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.785   -0.945  -25.3313]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.16    -0.97   -24.4943]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.925  -0.945 -23.054]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -97.59721684894896, time: 129.143
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.88   -0.33   -2.9736]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.895   -1.     -25.4387]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.385   -0.99   -24.4194]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.405   -0.995  -24.1986]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -110.36768558330328, time: 129.853
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-2.605  -0.235  -1.9852]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.91    -0.985  -25.4429]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.39    -0.99   -25.0788]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.635  -0.96  -23.526]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -117.7944149439006, time: 129.589
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.96  -0.255 -2.455]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.96    -0.99   -25.4747]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.3     -1.     -25.0354]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.37    -0.845  -23.5849]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -110.4701797564952, time: 130.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.585  -0.335  -4.0048]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.905   -0.985  -25.4387]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.3    -1.    -25.041]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.745  -0.965 -24.012]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -90.98284232491994, time: 131.066
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.34   -0.305  -2.6962]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.805   -0.975  -25.3516]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.93    -0.99   -24.7801]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.83    -0.96   -24.7534]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -93.16610732833949, time: 130.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.435 -0.32  -2.671]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.715   -0.99   -25.3141]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.53   -0.96  -24.531]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.3     -0.91   -24.4811]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -94.42116538872881, time: 131.023
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.73   -0.285  -3.0114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.795   -0.97   -25.3385]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.2     -0.995  -24.9359]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.275   -0.975  -25.0384]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -93.55226038628011, time: 131.514
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.24   -0.205  -2.5227]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.785   -0.985  -25.3507]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.79    -0.995  -24.3639]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.925   -0.975  -24.8373]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -96.2292699983469, time: 129.851
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.95  -0.33  -2.481]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.835   -0.995  -25.4181]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.505  -1.    -23.731]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.62    -0.97   -24.7459]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -90.86587548402007, time: 127.573
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.98   -0.44   -4.2299]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.865   -0.985  -25.4322]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.365   -1.     -25.1045]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.05    -0.97   -24.5674]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -97.13775837672048, time: 127.929
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.54   -0.305  -3.7672]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.81   -1.    -25.404]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.145  -1.    -24.974]
[-23.095   -0.835  -14.5609]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.57   -0.945 -13.575]
19200 50
steps: 959950, episodes: 19200, mean episode reward: -101.18157138866783, time: 127.866
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.11   -0.42   -6.2989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.765   -0.93   -15.2484]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.8     -0.93   -14.1299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.4     -0.925  -12.8913]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -83.246886441987, time: 128.281
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.595   -0.62    -8.0655]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.065   -0.88   -12.9987]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.17    -0.895  -13.9065]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.19    -0.96   -12.4067]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -82.69437338968757, time: 126.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.405  -0.535  -6.7924]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.075   -0.955  -14.8305]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.625   -0.905  -15.4911]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.8     -0.945  -12.1696]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -88.52133759924227, time: 127.105
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.315  -0.465  -5.8934]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.935   -0.93   -15.5635]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.295   -0.965  -15.8844]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.69    -0.965  -11.6482]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -90.80403304379635, time: 127.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.77    -0.47    -7.2595]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.085   -0.93   -14.6325]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.795   -0.905  -14.5133]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.435   -0.945  -12.5564]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -89.57608552365672, time: 127.1
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.37   -0.5    -6.0281]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.375  -0.92  -13.713]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.115   -0.91   -14.4338]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.215   -0.965  -12.2907]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -87.42836523205305, time: 127.284
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.64    -0.565   -7.9375]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.275   -0.96   -15.6274]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.015   -0.9    -13.5914]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.015   -0.94   -12.4352]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -89.98130699009037, time: 126.464
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.615  -0.465  -7.121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.235   -0.945  -15.6148]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.495   -0.91   -14.4727]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.695   -0.975  -14.5633]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -85.32578277816782, time: 126.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.37    -0.55    -8.1102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.185   -0.9    -14.2968]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.505   -0.985  -14.2717]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.84    -0.89   -11.6696]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -81.15621387525533, time: 129.016
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.44    -0.555   -8.4737]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.56    -0.895  -13.6841]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.67    -0.925  -14.4899]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.715   -0.85   -11.0345]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -83.94208571567677, time: 127.777
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.435   -0.645   -9.0429]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.015   -0.935  -14.9952]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.045   -0.925  -14.1315]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.35    -0.845  -10.8637]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -85.88626978709937, time: 127.227
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.71   -0.67   -9.914]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.375   -0.925  -13.7905]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.555   -0.92   -14.6912]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.09    -0.92   -11.1459]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -82.69327943548718, time: 127.035
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.785   -0.56    -8.9067]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.69    -0.96   -15.1667]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.58    -0.975  -14.4765]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.985  -0.945 -10.367]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -79.38118118212887, time: 129.02
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.62    -0.695  -10.3622]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.26    -0.89   -12.9713]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.64    -0.985  -13.5568]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.11    -0.945  -11.0443]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -77.94390915696569, time: 127.694
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.45    -0.685   -8.2901]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.095   -0.97   -12.4407]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.6     -1.     -13.9443]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.455   -0.96   -10.8438]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -79.18151182731386, time: 128.95
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.585   -0.83   -10.9081]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.975   -0.92   -12.0173]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.755   -1.     -14.3732]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.79    -0.995  -10.6576]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -79.97451201969714, time: 128.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.115   -0.885  -11.2362]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.565   -0.975  -12.1948]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.66    -0.995  -13.1094]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.855   -0.985  -11.2408]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -79.37661263595196, time: 125.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.92    -0.875  -12.2705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.54    -0.965  -11.7571]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.88    -0.985  -12.6191]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.46    -0.95   -10.7559]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -68.64148666042064, time: 127.712
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.485   -0.815  -10.4396]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.565   -0.935  -11.4355]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.015   -0.745  -19.3244]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.525   -1.     -22.9932]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.04   -0.425  -5.9658]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -87.75337175379762, time: 127.167
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.28    -0.64    -7.4102]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.08   -0.925 -20.774]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.235   -1.     -23.0648]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.36  -0.56  -6.248]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -81.91752600756426, time: 124.478
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.945   -0.62   -10.9514]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.89    -0.845  -19.6108]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.555   -0.99   -23.4675]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.305  -0.485  -5.7191]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -86.1285092562155, time: 125.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.495   -0.515   -8.4913]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.045   -0.815  -20.2326]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.975   -0.99   -23.9025]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.86   -0.465  -6.4467]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -86.42433349085427, time: 126.451
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.94   -0.465  -5.1314]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.86    -0.855  -21.8468]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.68    -0.995  -24.1294]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.995   -0.59    -9.2594]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -91.33906589465121, time: 125.951
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.46    -0.44    -5.2606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.86    -0.685  -18.9985]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.345   -0.975  -24.5972]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.22   -0.505  -6.0878]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -85.31901222700589, time: 123.85
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.025  -0.415  -7.605]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.365   -0.815  -20.8292]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.155   -0.995  -24.3701]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.23    -0.555  -10.8613]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -86.40072569341064, time: 124.902
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.275   -0.455   -6.2391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.225   -0.795  -20.8144]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.155   -0.99   -23.7939]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.47    -0.605  -11.1568]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -89.45431771788488, time: 125.053
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.53    -0.31    -6.0374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.135   -0.87   -23.2025]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.635   -0.975  -24.1096]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.71    -0.555  -11.0435]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -90.99416784104204, time: 124.161
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.07   -0.31   -3.6476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.925   -0.765  -22.2314]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.41    -1.     -24.5271]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.52    -0.505  -10.3749]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -80.72561590815681, time: 126.918
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.645   -0.345   -6.2092]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.92    -0.755  -21.3827]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.835   -1.     -24.3094]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.495   -0.56    -9.7135]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -76.0396694448674, time: 125.551
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.97    -0.525   -6.5798]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.845   -0.905  -23.6029]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.705   -0.98   -24.2267]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.72    -0.66   -10.7072]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -81.00266774144521, time: 128.292
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.185   -0.545   -6.4508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.08    -0.855  -23.5352]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.365   -0.995  -24.0297]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.16    -0.665  -11.7429]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -84.09031757723835, time: 124.615
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.105  -0.58   -7.214]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.015   -0.795  -22.8375]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.21    -1.     -24.5184]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.955   -0.71   -16.2925]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -82.24060420776011, time: 126.3
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.54    -0.625   -8.7776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.99   -0.865 -24.156]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.305   -1.     -24.4336]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.495   -0.875  -16.4093]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -76.13586762342517, time: 126.394
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.12    -0.66   -10.3972]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.85    -0.91   -24.0228]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.435   -1.     -24.5619]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.885   -0.925  -17.8815]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -88.81803507622485, time: 125.228
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.825   -0.745  -10.8071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.205   -0.915  -24.3549]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.23    -0.995  -24.5295]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.585   -0.925  -18.8535]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -83.91367557420199, time: 126.171
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.53    -0.685  -10.7573]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.31    -0.98   -24.5361]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.84    -0.98   -24.9311]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.095   -0.95   -20.0896]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -82.78011401300415, time: 124.588
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.115  -0.695 -13.666]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.365   -0.98   -24.5285]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.725   -1.     -24.3698]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.415   -0.99   -23.0792]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -80.95590285494818, time: 125.513
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.35   -0.65   -8.331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.955   -0.43    -7.3004]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.115   -1.     -24.9398]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.935  -1.    -25.467]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -76.56493428514709, time: 125.544
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.315  -0.565  -5.4384]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.49   -0.43   -5.4133]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.98    -0.98   -22.6216]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.925   -1.     -25.4738]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -64.66971958154511, time: 124.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.115   -0.555   -7.3489]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.41   -0.415  -5.5366]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.88    -0.975  -22.7149]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.915   -1.     -25.4705]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -67.71727465595552, time: 125.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.065   -0.685   -6.7882]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.415   -0.525   -7.1141]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.005   -0.995  -24.2233]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.64    -0.99   -25.2455]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -68.62283251977107, time: 124.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.275  -0.69   -5.5133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.73    -0.67   -12.7459]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.19    -0.99   -24.5814]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.945   -1.     -25.4754]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -65.31190631874534, time: 127.529
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.63    -0.735   -9.8115]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.195   -0.74    -8.7026]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.315   -0.99   -25.1255]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.875   -1.     -25.4572]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -63.475089574189894, time: 126.498
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.575  -0.865  -8.388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.905   -0.685   -9.0373]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.515   -0.99   -23.5452]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.905   -1.     -25.4558]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -66.27606981227413, time: 125.863
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.315  -0.67   -6.2294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.77    -0.68    -9.1837]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.265   -0.99   -24.4179]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.935   -1.     -25.4565]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -70.59319088663682, time: 125.321
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.35    -0.6     -6.5669]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.795   -0.54    -7.8479]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.215   -0.97   -23.4114]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.91    -1.     -25.4486]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -76.17581680097258, time: 127.086
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.37   -0.565  -5.6662]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.205   -0.575  -11.8586]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.555   -0.975  -24.5891]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.79    -1.     -25.3895]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -68.99718449366513, time: 128.215
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.72   -0.575  -5.8312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.245   -0.68   -13.3358]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.31    -0.985  -25.0113]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.91    -1.     -25.4477]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -66.20569794188499, time: 125.409
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.905   -0.785  -11.5103]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.545   -0.68   -13.8391]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.735   -0.99   -24.6584]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.89    -1.     -25.4437]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -66.93683997596037, time: 124.47
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.51    -0.745   -8.3497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.85    -0.64   -11.5173]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.8     -0.99   -22.8605]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.915   -0.995  -25.4572]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -78.34351913416054, time: 125.404
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.29    -0.82    -8.7977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.67   -0.645 -10.33 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.25    -1.     -23.7643]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.9     -0.995  -25.4514]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -79.37492082897342, time: 128.119
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.01    -0.825   -9.3981]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.525   -0.87   -16.9881]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.245   -0.91   -22.5611]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.9     -1.     -25.4535]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -69.30100097360595, time: 128.516
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.67    -0.815  -11.1819]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.67    -0.9    -16.3003]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.625   -0.995  -24.3173]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.44    -0.995  -25.2382]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -66.205826105675, time: 125.972
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.9     -0.875  -10.8214]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.405   -0.84   -16.8445]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.585   -0.945  -22.9373]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.77    -0.995  -25.3674]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -67.17301381809324, time: 127.214
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.815   -0.885  -10.4062]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.855   -0.885  -16.8552]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.595   -0.97   -22.9555]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.705   -0.995  -23.9601]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -60.59155767365331, time: 126.019
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.64    -0.905  -11.2994]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.47    -0.89   -16.8342]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.085   -0.965  -23.0091]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.48    -1.     -25.2656]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -62.35918308640159, time: 127.099[-27.99    -0.85   -16.3294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.09  -0.38  -3.351]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.055  -0.94  -19.406]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.42   -0.495  -3.6465]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -88.26321678454796, time: 126.705
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.255   -0.995  -17.4503]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.525  -0.44   -3.7612]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.58   -0.915 -17.979]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.555 -0.37  -2.119]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -97.64604953629498, time: 127.132
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.105   -1.     -19.1125]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.675  -0.51   -3.9542]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.695   -0.895  -18.7347]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.81   -0.375  -2.3207]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -84.96897074652581, time: 127.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.995   -0.975  -17.9048]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.81   -0.57   -3.3618]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.425   -0.87   -14.8741]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.89  -0.4   -2.472]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -88.63553105900449, time: 127.072
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.715   -0.995  -18.7508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.835 -0.365 -2.448]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.175   -0.895  -14.0451]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.03   -0.465  -2.5743]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -95.38181954228847, time: 127.312
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.91   -1.    -16.567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.665  -0.495  -3.0929]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.485   -0.935  -17.0255]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.705  -0.43   -2.9829]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -85.72171176386018, time: 128.229
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-34.745   -0.995  -19.4799]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.325  -0.45   -2.8403]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.75    -0.925  -14.6673]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.515 -0.365 -2.158]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -88.03319930422356, time: 127.536
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.2    -0.99  -16.958]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.29   -0.61   -3.6456]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.455   -0.97   -14.9538]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.8    -0.39   -2.3932]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -89.14909108181917, time: 127.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.845   -0.94   -17.9697]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.915  -0.625  -3.4516]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.72    -0.98   -19.9317]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.005  -0.385  -2.5495]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -91.65835456619644, time: 128.385
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.925   -0.95   -18.0175]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.9    -0.38   -2.3764]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.405   -0.97   -16.2499]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.245  -0.445  -2.8289]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -89.46942642806509, time: 129.006
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.295  -0.97  -18.74 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.865 -0.555 -3.36 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.775   -0.955  -11.7634]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.745  -0.445  -3.0601]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -85.43648080292252, time: 128.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.785   -0.995  -16.4015]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.27   -0.565  -3.6586]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.77    -0.99   -14.9269]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.3   -0.43  -2.818]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -89.19618075975391, time: 127.802
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.505   -0.995  -14.9649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.68   -0.45   -3.1572]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.85    -0.985  -13.5836]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.14   -0.45   -2.7317]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -87.15112961709173, time: 127.844
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.185   -1.     -16.0293]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.555  -0.46   -3.7024]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.99    -0.98   -12.2882]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.755  -0.405  -2.3462]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -84.03563174165592, time: 129.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.35    -0.975  -13.9756]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.595  -0.575  -4.6416]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.515   -0.97   -13.2425]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.935  -0.46   -3.2703]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -84.0138941887248, time: 129.106
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.24   -0.995 -17.424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.855 -0.46  -4.036]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.955   -1.     -12.9658]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.89   -0.47   -3.2612]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -86.72048536550133, time: 127.471
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.81   -1.    -17.906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.14   -0.435  -3.5323]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.045  -0.985 -11.157]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.3    -0.475  -3.6045]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -86.75225855943461, time: 128.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.035   -1.     -16.9181]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.49   -0.46   -3.7267]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.4    -0.985 -11.346]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.985  -0.5    -3.3313]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -83.84044938837258, time: 127.208
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.79    -0.995  -16.1248]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.19   -0.44   -3.5136]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.085   -1.     -11.2971]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.415  -0.46   -2.8887]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -77.53775017187317, time: 127.642
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.295   -1.     -15.0223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.59    -0.985  -22.0549]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.515  -0.505  -5.8452]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.765  -0.01   -0.3788]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -96.83058255894782, time: 129.558
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.12   -0.895 -13.473]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.295   -1.     -19.8656]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.59    -0.475   -7.7581]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.025  -0.005  -0.0157]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -83.39035078036912, time: 129.857
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.175   -0.82   -11.8935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.745   -0.995  -21.7007]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.205   -0.57    -8.0189]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[0. 0. 0.]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -102.51395503362961, time: 127.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.995   -0.99   -13.9864]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.61    -0.98   -18.5688]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.24   -0.5    -5.5518]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.02    0.     -0.0144]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -97.1054009111146, time: 128.363
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.595  -0.795 -13.179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.12   -0.975 -20.148]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.06   -0.595 -10.185]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.035  -0.005  -0.0159]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -94.12672793131645, time: 129.442
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.45    -0.695  -13.6601]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.385   -1.     -21.2452]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.445  -0.53   -7.419]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.035  -0.005  -0.0168]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -88.16011951381141, time: 127.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.16    -0.725  -12.2804]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.165   -0.995  -20.6455]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.275  -0.47   -4.0704]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.345  -0.03   -0.7352]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -95.83570558357653, time: 127.666
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.34   -0.785 -14.034]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.505   -0.99   -19.3755]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.605   -0.615   -8.1161]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.43   -0.01   -0.2463]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -99.23476761135056, time: 129.462
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.055   -0.91   -14.7139]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.81    -0.995  -19.3623]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.99    -0.675   -9.5547]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.315  -0.01   -0.1873]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -88.27281583002919, time: 129.94
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.535   -0.835  -14.7602]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.295   -1.     -18.7228]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.695  -0.655  -6.8943]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-0.63   -0.045  -0.3476]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -86.6010946219772, time: 130.448
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.45    -0.79   -14.1401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.85    -0.98   -18.7838]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.055  -0.56   -4.8534]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.185  -0.03   -0.6128]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -81.23346504271395, time: 129.985
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.395   -0.885  -13.5795]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.905   -1.     -19.0363]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.86   -0.47   -4.2536]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.005  -0.05   -0.5737]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -96.86047245914862, time: 128.401
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.525   -0.91   -14.7407]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.105  -0.995 -16.766]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.25    -0.69    -7.5289]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.575  -0.185  -1.6387]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -97.28943849856405, time: 130.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.145   -0.93   -15.9168]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.085   -1.     -18.5531]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.565  -0.79   -7.4019]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.18    -0.32    -6.0557]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -98.11485772021979, time: 130.247
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.015   -0.86   -14.4705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.525   -0.905  -14.0049]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.485  -0.735  -6.4508]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.015 -0.5   -5.687]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -87.07276090283172, time: 129.226
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.795   -0.985  -16.0808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.935   -0.915  -11.7264]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.12    -0.785   -7.6224]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.8    -0.44   -5.9573]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -83.70943412399218, time: 129.31
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.1    -0.96  -14.766]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.67    -0.98   -14.0422]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.865   -0.86   -10.4096]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.225  -0.395  -4.9458]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -93.5185035049148, time: 127.264
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.72    -0.98   -15.5205]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.835   -0.96   -10.7915]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.38   -0.85   -9.083]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.36   -0.375  -4.0875]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -84.51705143916777, time: 128.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.475   -0.985  -16.9179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.245   -0.995  -14.4275]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.795   -0.9    -11.0679]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.305 -0.34  -3.189]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -85.18090617824795, time: 128.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.12    -0.995  -15.3603]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.05   -0.135  -0.8347]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.34   -0.99  -17.244]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.875   -1.     -14.1086]
19400 50
steps: 969950, episodes: 19400, mean episode reward: -88.18503008877407, time: 125.862
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.27   -0.335  -4.0226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.45  -0.29  -1.889]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.66    -0.99   -18.3243]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.57   -1.    -14.096]
19600 50
steps: 979950, episodes: 19600, mean episode reward: -83.67703544562504, time: 125.451
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.12   -0.285  -3.6259]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.55   -0.1    -0.4964]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.465   -0.975  -16.7315]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.325   -0.985  -14.0534]
19800 50
steps: 989950, episodes: 19800, mean episode reward: -84.3435151681741, time: 126.183
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.93   -0.38   -3.3778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.555  -0.35   -2.1886]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.07    -0.985  -15.8529]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.67    -0.995  -13.1078]
20000 50
steps: 999950, episodes: 20000, mean episode reward: -84.31957468577001, time: 125.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.28   -0.33   -3.2442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.595  -0.28   -1.3973]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.675   -0.995  -16.8877]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.3     -0.975  -13.0105]
20200 50
steps: 1009950, episodes: 20200, mean episode reward: -81.47352406974036, time: 125.476
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.495  -0.375  -4.0391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-0.78   -0.145  -0.7126]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.645   -1.     -15.3005]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.125   -1.     -11.8256]
20400 50
steps: 1019950, episodes: 20400, mean episode reward: -91.78233537109573, time: 125.869
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.66  -0.375 -3.544]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.46   -0.32   -2.1092]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.195   -0.985  -16.9299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.745   -0.995  -12.5919]
20600 50
steps: 1029950, episodes: 20600, mean episode reward: -79.6705443297442, time: 126.457
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.53   -0.31   -3.1424]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.385  -0.43   -3.5506]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.61    -0.995  -14.8787]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.275   -0.995  -12.1934]
20800 50
steps: 1039950, episodes: 20800, mean episode reward: -86.31974582781484, time: 126.238
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.75   -0.255  -3.1126]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.35  -0.27  -1.198]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.73    -0.985  -16.1512]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.25    -1.     -12.6808]
21000 50
steps: 1049950, episodes: 21000, mean episode reward: -83.59532717428353, time: 126.282
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.275  -0.275  -3.8345]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-1.7    -0.27   -1.4974]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.845   -0.99   -15.7071]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.32    -0.995  -11.2347]
21200 50
steps: 1059950, episodes: 21200, mean episode reward: -86.48056409003428, time: 125.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.69   -0.34   -4.5745]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.77   -0.44   -3.1534]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.145   -0.98   -13.6064]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.995   -1.     -11.3292]
21400 50
steps: 1069950, episodes: 21400, mean episode reward: -87.62448160024756, time: 126.219
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.15   -0.41   -4.8275]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.065  -0.325  -1.8038]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.075   -0.98   -12.9882]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.945   -1.     -12.4187]
21600 50
steps: 1079950, episodes: 21600, mean episode reward: -78.52728063357925, time: 126.452
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.65   -0.45   -4.9942]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-3.17   -0.38   -2.6486]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.8     -0.99   -13.3045]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.92    -1.     -10.1489]
21800 50
steps: 1089950, episodes: 21800, mean episode reward: -73.57278863725648, time: 128.167
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.13   -0.4    -4.7298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-2.58   -0.415  -2.2575]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.905   -0.995  -15.2328]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.79    -0.995  -10.1447]
22000 50
steps: 1099950, episodes: 22000, mean episode reward: -83.03578086372649, time: 126.81
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.755  -0.52   -5.8871]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.745  -0.555  -3.9447]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.16    -0.995  -14.1846]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.18    -1.     -11.0225]
22200 50
steps: 1109950, episodes: 22200, mean episode reward: -90.44635388608904, time: 128.76
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.14    -0.66    -7.4143]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.95   -0.59   -6.5207]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.305   -0.975  -13.1719]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.22    -0.995  -10.5791]
22400 50
steps: 1119950, episodes: 22400, mean episode reward: -78.16838721368484, time: 125.829
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.645  -0.64   -5.4031]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.795   -0.61    -9.0223]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.815   -0.99   -12.7393]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.965   -1.     -10.2879]
22600 50
steps: 1129950, episodes: 22600, mean episode reward: -82.18786816018245, time: 125.969
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.355  -0.705  -6.2438]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.69    -0.635  -10.0693]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.7     -0.985  -12.9829]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.08    -1.     -10.4751]
22800 50
steps: 1139950, episodes: 22800, mean episode reward: -83.00582354194317, time: 125.778
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.525   -0.695   -8.0327]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.82  -0.645 -6.67 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.655   -0.995  -13.7113]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.425  -0.925  -9.808]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -80.35868812221283, time: 126.773
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.125   -0.64    -8.2596]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.51   -0.585  -5.5031]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.39    -0.765  -12.7137]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.75    -0.995  -24.8825]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -89.97723527433214, time: 127.807
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-41.715   -0.595  -19.4004]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.59    -0.99   -23.1121]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.4     -0.815  -14.0017]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.315   -0.995  -24.6275]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -82.9382344752065, time: 127.358
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-43.37    -0.68   -20.4525]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.19    -0.895  -21.6019]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.775   -0.91   -14.4906]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.96    -1.     -24.9459]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -84.80513421901006, time: 128.57
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.79   -0.705 -18.776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.93    -0.99   -18.0167]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.155   -0.9    -12.6243]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.435   -0.99   -23.9857]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -90.65451041828463, time: 128.15
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-37.75    -0.695  -17.7818]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.14    -0.95   -18.6358]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.165   -0.845  -12.9215]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.52    -1.     -24.7294]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -90.13024037685591, time: 131.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.83    -0.6    -16.7035]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.065   -0.965  -19.6318]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.46    -0.875  -12.7674]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.7     -0.985  -24.3673]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -77.25883154335457, time: 127.622
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.66    -0.765  -19.3548]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.955   -0.935  -18.9284]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.475   -0.85   -13.1004]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.085   -0.98   -24.5444]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -80.51617864172619, time: 128.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-39.3     -0.665  -18.9952]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.89    -0.95   -20.0022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.73    -0.92   -15.1713]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.045   -1.     -24.5165]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -95.04023236147019, time: 128.527
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-38.925   -0.69   -18.9041]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.225   -0.99   -19.0453]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.73    -0.915  -15.5757]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.975  -1.    -22.747]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -82.65186187208343, time: 129.283
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.65    -0.71   -18.1689]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.61    -0.985  -17.6868]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.285   -0.83   -14.9396]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.415   -0.995  -23.2778]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -88.59576190753276, time: 129.016
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.845   -0.62   -15.5318]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.99    -0.965  -17.7429]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.77    -0.87   -14.4683]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.775   -1.     -24.4549]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -86.24235085950835, time: 129.705
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.675   -0.72   -16.3561]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.555   -0.955  -17.6716]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.915   -0.925  -16.8641]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.93    -0.98   -23.1443]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -86.81517705713722, time: 129.258
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.225   -0.55   -12.7868]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.29    -0.885  -16.5183]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.445   -0.87   -14.4838]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.455   -0.975  -22.9459]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -76.88688615228217, time: 128.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-35.      -0.735  -17.1729]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.615   -0.86   -15.1795]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.545   -0.92   -12.7817]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.98   -0.99  -23.743]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -99.18789025473922, time: 127.292
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.015   -0.72   -16.3307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.365   -0.885  -14.8476]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.755   -0.89   -15.5762]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.76    -0.84   -21.5325]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -86.44911043594344, time: 129.638
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.195   -0.655  -13.9406]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.11    -0.97   -17.8867]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.355   -0.845  -16.4399]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.725   -0.98   -23.2001]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -95.36866092746374, time: 130.082
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.955   -0.625  -11.2042]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.06    -0.95   -15.0615]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.78    -0.925  -15.1703]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.375   -0.945  -20.8875]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -84.04414869556979, time: 131.027
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.36    -0.7    -12.8097]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.815   -0.94   -16.5892]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.5     -0.875  -13.7274]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.39    -0.755  -18.9959]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -85.86096662239504, time: 129.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.56    -0.66   -15.2692]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.125   -0.94   -16.9469]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.44   -0.935 -15.692]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.245   -0.995  -24.2803]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -85.82931137731029, time: 127.998
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.66    -0.69   -13.4225]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.68    -0.84   -10.9349]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.095   -0.925  -19.8315]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.445   -0.975  -23.5945]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.      -0.995  -24.9573]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -65.27074460052776, time: 127.244
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.56    -0.88   -12.6792]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.46    -0.915  -20.7926]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.495   -0.98   -23.0261]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.685   -0.98   -24.2244]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -63.10486211817955, time: 127.717
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.52    -0.85   -11.2576]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.075   -0.96   -20.9246]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.58    -0.97   -22.8913]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.075   -0.99   -24.4308]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -62.458946298193226, time: 126.131
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.8     -0.875  -12.4121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.475   -0.95   -22.4074]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.85    -0.99   -22.0293]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.54    -0.885  -24.5476]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -71.27557735190524, time: 126.458
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.565  -0.835 -10.715]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.695   -0.96   -22.3035]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.29    -1.     -23.1931]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.785   -0.985  -24.8471]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -67.69919671933843, time: 126.321
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.995   -0.875  -12.6171]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.135   -0.995  -22.2412]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.035   -0.99   -23.2163]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.86    -0.965  -24.9314]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -71.58820375268714, time: 125.744
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.06   -0.86  -11.228]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.42    -0.985  -21.7355]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.96    -0.97   -21.3912]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.945   -0.97   -24.9577]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -57.41899637363545, time: 127.378
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.22    -0.845  -11.5252]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.375   -0.895  -20.0015]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.655  -0.995 -22.179]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.345   -0.95   -25.0993]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -57.411310791972184, time: 126.101
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.7     -0.915  -14.2887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.05    -0.825  -17.8024]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.53  -0.99 -23.83]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.85    -0.815  -23.0612]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -60.452167299435, time: 127.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.      -0.915  -13.3484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.195   -0.85   -19.0084]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.475   -0.94   -18.0811]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.665   -0.97   -24.7397]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -56.24905558929521, time: 126.053
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.68    -0.885  -14.0682]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.83    -0.95   -19.6103]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.445   -0.98   -18.5878]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.885   -0.81   -20.8969]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -56.290126474580795, time: 127.736
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.755   -0.89   -14.2639]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.21    -0.88   -15.8856]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.06    -0.995  -20.1136]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.355   -0.82   -20.9467]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -65.14859395368991, time: 126.525
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.115   -0.88   -13.9151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.425   -0.855  -16.7812]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.47    -0.88   -18.0922]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.415   -0.81   -20.4372]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -72.77587321400638, time: 127.251
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.995   -0.875  -14.5098]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.22    -0.77   -15.1587]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.555   -0.92   -19.6044]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.41    -0.745  -17.8163]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -60.5256046122704, time: 127.022
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.67    -0.925  -13.2933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.56    -0.835  -18.4503]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.905   -0.835  -17.1781]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.92    -0.855  -21.9363]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -59.57146013793201, time: 128.243
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.585   -0.94   -13.5217]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.87    -0.905  -17.6587]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.98    -0.975  -21.9437]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.465   -0.93   -22.6629]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -66.26933224797561, time: 130.329
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.82    -0.95   -13.8436]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.415   -0.88   -19.5169]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.635   -0.98   -16.6392]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.      -0.83   -21.4097]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -53.261271334008995, time: 128.816
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.2     -0.93   -14.7445]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.065   -0.92   -18.0274]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.245   -0.995  -20.8385]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.675   -0.975  -22.3852]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -58.09725370112366, time: 127.019
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.335   -0.945  -14.5549]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.08    -0.92   -16.5094]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.485   -0.98   -21.8685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.4     -1.     -22.8707]

[-18.17    -0.835  -11.0921]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.535   -0.99   -25.1736]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.67    -0.99   -25.2968]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.225   -1.     -15.6908]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -84.41263261720894, time: 129.162
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.465   -0.745  -10.9135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.865   -0.995  -25.4151]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.635   -1.     -25.2816]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.49    -0.955  -17.1538]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -75.04203474101469, time: 127.217
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.89    -0.69    -9.4573]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.88    -1.     -25.4191]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.76    -1.     -25.3673]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.71    -1.     -17.8204]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -75.76531686788073, time: 128.532
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.205   -0.785  -11.5718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.32    -0.99   -25.0741]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.735   -0.995  -25.3286]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.425   -1.     -17.5625]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -71.8285231897154, time: 129.303
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.51    -0.725  -10.0272]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.61    -1.     -25.2325]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.85    -0.99   -25.4115]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.03    -0.995  -14.9898]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -72.89626883316379, time: 128.664
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.315   -0.745   -9.6109]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.52    -1.     -25.1499]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.285   -1.     -25.0114]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.1     -1.     -15.4699]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -73.9881199280971, time: 128.171
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.98   -0.835 -10.014]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595   -1.     -25.2071]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.3     -0.985  -24.5209]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.895   -0.91   -13.1288]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -75.67709800455, time: 129.539
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.975   -0.875  -11.9822]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.505   -0.995  -25.1339]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.875   -1.     -24.7289]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.97    -0.98   -14.0005]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -76.0102478685175, time: 128.345
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.465   -0.85   -12.2198]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.375   -1.     -25.0168]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.29    -0.985  -25.0146]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.73    -0.97   -15.9276]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -69.89619518215486, time: 129.529
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.655   -0.835   -9.7534]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.595  -1.    -25.201]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.03   -0.995 -24.853]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.515   -1.     -14.3924]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -69.56619640724293, time: 128.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.78    -0.85    -9.1401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.585  -1.    -25.189]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.53    -0.98   -25.1793]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.48    -0.99   -14.3771]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -75.49022244527627, time: 129.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.97    -0.875   -9.6725]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.49    -1.     -25.1267]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.055   -0.92   -24.2369]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.465   -0.89   -11.5246]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -66.4248699716963, time: 129.798
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.57    -0.835  -10.9637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.715   -1.     -25.2828]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.985  -1.    -24.91 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.005   -0.985  -13.4003]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -71.81897667218237, time: 128.639
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.97    -0.935  -11.8168]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.195   -1.     -24.8699]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.625   -1.     -24.0629]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.075   -0.94   -13.1826]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -68.09127004531263, time: 128.627
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.06    -0.855   -8.7196]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.85    -1.     -25.3952]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.625   -0.995  -24.8965]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.075   -0.945  -10.1977]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -73.0482856158974, time: 129.18
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.955   -0.89   -11.0753]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.87    -1.     -25.4077]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.295   -1.     -25.1159]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.115   -0.845   -9.5847]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -69.00434993887606, time: 130.524
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.17    -0.93   -10.6842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.855   -1.     -25.3931]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.195  -0.94  -22.805]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.495   -0.77    -9.9264]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -69.04996124061472, time: 132.541
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.965   -0.9    -11.3311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.755   -1.     -25.3163]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.48    -0.985  -25.2469]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.87    -0.8    -10.6397]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -68.36432517496011, time: 130.235
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.08    -0.86    -9.9541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.885   -1.     -25.4345]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.31    -0.98   -25.1662]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.465   -0.905  -10.8451]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -83.4038605711138, time: 129.034
[-20.295   -0.9    -13.0122]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.575   -0.99   -21.6432]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.62    -1.     -25.2369]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -88.82395013582068, time: 126.72
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.85    -0.865  -10.7093]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.11    -0.875  -10.9158]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.745  -0.96  -19.949]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.36    -0.995  -25.0804]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -65.63941128630036, time: 126.071
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.115  -0.87  -10.171]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.295   -0.92   -12.7927]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.42    -0.955  -21.4013]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.595   -0.965  -25.2393]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -71.98287563892173, time: 127.773
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.075   -0.865   -9.3872]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.22    -0.94   -14.0417]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.45    -0.82   -17.0684]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.345   -0.855  -24.9363]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -70.36044124341218, time: 128.018
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.315   -0.84    -8.7118]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.375   -0.905  -11.2209]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.195   -0.765  -16.6574]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.67    -0.875  -24.5906]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -84.29269807852963, time: 126.953
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.5     -0.735   -8.8335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.685   -0.865  -11.0152]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.635   -0.655  -16.1427]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.91    -0.685  -20.0398]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -81.01351378781115, time: 127.091
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.395   -0.79    -9.2159]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.68   -0.925 -15.973]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.155  -0.96  -18.491]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.88    -0.94   -24.6593]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -74.4053810195698, time: 127.859
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.59    -0.87    -9.1053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.03    -0.81    -8.4191]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.63    -0.985  -18.4209]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.795   -0.985  -24.7592]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -66.66485093775654, time: 127.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.255   -0.83    -8.7017]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.595   -0.8     -9.3008]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.235   -0.995  -17.9587]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.26    -0.98   -22.9928]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -74.05362644725815, time: 128.098
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.99    -0.78    -8.1322]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.98    -0.875   -9.8824]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.265   -0.98   -19.8487]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.03    -1.     -24.4728]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -73.9865074482086, time: 128.026
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.305   -0.86    -8.8499]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.375   -0.79    -9.7432]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.165  -0.985 -20.563]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.43    -1.     -23.7312]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -65.72130524765662, time: 129.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.71   -0.845  -7.5157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.32    -0.865   -9.7448]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.23    -0.84   -15.8135]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.985   -0.995  -22.6276]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -74.69686656561454, time: 127.517
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.87    -0.845   -9.0212]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.465   -0.93   -11.1003]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.41    -0.78   -17.6069]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.75    -1.     -24.9843]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -71.27051019268714, time: 127.337
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.25    -0.845   -9.6527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.05    -0.905  -13.3201]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.045   -0.91   -18.1386]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.365   -1.     -25.1863]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -95.0406328301373, time: 127.699
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.425   -0.83    -8.8884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.565   -0.94   -11.9248]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.925   -0.92   -19.4204]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.99    -1.     -25.0082]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -78.78893583201896, time: 127.792
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.485   -0.865   -9.6251]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.3     -0.9     -9.7644]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.15    -0.91   -19.1824]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.39   -1.    -25.195]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -72.50001859599634, time: 132.051
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.96    -0.905  -10.0022]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.33    -1.     -12.0998]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.255   -0.94   -18.8045]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.81    -1.     -25.3873]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -78.12226948685158, time: 128.175
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.475   -0.885   -9.8136]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.495   -0.98   -14.3757]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.11    -0.94   -17.8256]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.165  -1.    -24.965]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -66.62963664379429, time: 128.01
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.465   -0.83    -9.1973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.44    -0.965  -11.7525]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.245   -0.965  -16.3504]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-49.64    -1.     -25.2973]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -72.45985451827238, time: 129.172
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.43    -0.815   -9.4128]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.39   -0.86  -10.895]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -79.14888585199373, time: 129.254
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.68    -0.895   -9.2406]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.405   -0.955  -13.8616]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.4     -0.99   -13.1834]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.43    -0.835  -10.0691]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -72.68049431329818, time: 128.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.72    -0.815   -8.2223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.82    -0.965  -14.0821]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.78    -1.     -12.3322]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.795   -0.86    -9.9551]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -77.82151497831906, time: 127.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.47    -0.845   -9.4036]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.23    -0.95   -13.1767]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.93    -0.975  -12.2495]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.355  -0.89   -9.265]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -77.33439203046096, time: 126.993
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.31    -0.92   -10.1457]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.19    -0.975  -11.4176]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.35   -0.99  -11.991]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.265   -0.945  -10.5146]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -78.57476624301465, time: 129.122
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.945   -0.905   -9.9058]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.645   -0.955  -10.3499]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.455   -0.99   -11.9027]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.145   -0.95   -10.2472]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -80.6137557888021, time: 127.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.115   -0.95   -10.9024]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.06    -0.98   -12.0914]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.52    -0.99   -12.4614]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.225  -0.965 -10.967]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -77.45733535059469, time: 128.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.37    -0.93    -9.3075]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.94    -0.95    -9.3905]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.97    -0.98   -12.7047]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.08    -0.925  -10.8615]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -75.12921690800076, time: 127.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.235   -0.92    -9.9435]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.85   -0.96  -10.279]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.17    -0.99   -13.8138]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.29   -0.905 -10.631]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -77.05830935246433, time: 127.174
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.2     -0.945  -12.0732]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.7     -0.985  -11.6119]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.725   -0.99   -12.5698]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.26    -0.85   -10.1051]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -78.40587733361507, time: 127.461
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.915   -0.935  -11.0827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.045   -0.985  -11.2647]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.545   -1.     -14.7564]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.845   -0.93   -10.2565]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -83.8110516324306, time: 128.16
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.1     -0.93   -11.5504]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.43    -0.95   -10.9607]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.74    -1.     -12.8913]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.61    -0.855   -9.2972]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -77.45681378913262, time: 128.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.36    -0.915   -9.6157]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.96    -0.975  -11.2895]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.16    -0.995  -12.0515]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.22    -0.925  -10.1363]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -74.60086989442156, time: 129.137
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.695   -0.93   -10.2133]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.895   -0.935   -9.8311]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.1     -0.995  -12.1843]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.53    -0.915   -9.8522]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -73.98143416650845, time: 127.951
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.86    -0.93   -11.8071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.615   -0.955   -9.2913]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.3     -1.     -12.9559]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.555   -0.96   -10.0635]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -76.91900322403828, time: 128.682
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.815   -0.96   -12.6681]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.755   -0.91   -10.1107]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.51    -0.995  -13.3724]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.73    -0.885  -10.1983]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -78.47265820552182, time: 131.136
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.725   -0.9    -11.5307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.295   -0.955  -11.6877]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.38    -1.     -12.5053]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.67   -0.97  -10.617]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -80.66789946606669, time: 131.421
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.22    -0.965  -12.1352]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.135  -0.935  -9.69 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.835   -0.995  -12.5914]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.02   -0.92   -9.944]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -77.34514981054991, time: 130.061
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.725   -0.975  -13.0587]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.235   -0.91   -10.3497]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.765   -1.     -12.0184]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.995   -0.875   -8.8792]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -79.41591467810485, time: 128.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.85   -0.965 -12.883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.16    -0.895  -10.3551]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.64    -0.98   -22.5213]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -84.30294585347339, time: 126.641
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.025  -0.43   -3.4553]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.835   -0.995  -25.3858]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.31    -0.995  -25.0278]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.635   -0.99   -23.9127]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -90.78972126829365, time: 126.938
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.135  -0.48   -4.1788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.925   -1.     -25.4554]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.305   -1.     -24.9941]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.945  -0.99  -24.854]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -95.59083646374789, time: 125.616
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.695  -0.345  -3.0298]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.855   -0.995  -25.4249]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.1     -0.995  -24.4926]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.33    -0.88   -22.5771]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -96.72132686068282, time: 128.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.15   -0.51   -4.4486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.8     -0.995  -25.3902]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.17    -1.     -24.4345]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.14    -0.99   -21.9778]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -86.99204231675914, time: 125.173
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.015  -0.445  -3.9884]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.67   -0.995 -25.337]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.52    -1.     -25.1682]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.22    -0.88   -19.9917]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -94.11027143453929, time: 126.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-4.185  -0.415  -3.1698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.57    -0.98   -25.2063]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.515   -0.995  -23.0333]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.82    -0.87   -21.7114]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -96.2701165011101, time: 127.393
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.835  -0.485  -4.3121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.115   -0.975  -23.0985]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.05   -1.    -25.031]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.655   -0.94   -23.5362]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -97.64901624597002, time: 124.405
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.03   -0.535  -5.1817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.79   -0.985 -24.535]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.77    -0.99   -23.6537]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.035   -0.875  -24.2454]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -96.69031738042366, time: 125.275
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.44   -0.51   -4.2428]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.85    -0.99   -24.6456]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.06    -0.95   -22.6801]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.495   -0.97   -24.5196]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -88.00638079489183, time: 126.605
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-3.845  -0.42   -3.0371]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.61    -0.99   -23.3986]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.48    -0.995  -23.0652]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.675   -0.96   -23.7667]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -100.26664101596596, time: 126.555
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.02   -0.615  -5.7027]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.285   -0.98   -24.3627]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.88    -1.     -23.2859]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.375  -0.875 -23.442]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -81.97428210413963, time: 126.368
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.225  -0.695  -6.8402]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.12   -0.99  -24.679]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.905   -0.97   -22.6862]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.19    -0.835  -24.3094]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -83.98820609514456, time: 123.673
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.025 -0.61  -5.018]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.205   -0.995  -22.7763]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-49.64    -0.995  -25.3178]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.165   -0.86   -24.2725]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -79.19365217237845, time: 126.134
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.49   -0.605  -4.5652]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.285  -0.99  -19.401]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.455   -0.995  -21.7906]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.695   -0.885  -24.6287]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -87.7027145818144, time: 126.764
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.055  -0.665  -6.9789]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.54    -1.     -21.7132]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.255   -1.     -24.5059]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.28    -0.89   -24.1248]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -84.25711734044776, time: 126.413
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.08    -0.71    -7.8637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.16    -0.995  -21.1052]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.1    -0.975 -21.114]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.375   -0.99   -21.4897]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -84.40127809797914, time: 128.617
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.795  -0.705  -6.3994]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.97    -1.     -22.6275]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.34    -0.89   -19.5287]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.315  -0.97  -22.532]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -84.83547912236028, time: 125.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.245  -0.69   -7.1363]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.26    -0.975  -20.8825]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.03    -0.84   -20.3856]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.77    -0.955  -22.4531]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -81.70769111952674, time: 125.637
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.5    -0.51   -4.2435]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.135   -1.     -22.1847]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.875   -0.98   -21.8079]
[-30.23   -0.75  -15.688]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.76    -0.985  -23.9236]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.655   -1.     -22.4601]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.895   -0.975  -19.8359]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -77.50925771769033, time: 125.789
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.185   -0.8    -14.9634]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.15    -0.995  -25.0142]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.745   -1.     -23.1533]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.375   -0.98   -18.8333]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -75.37952929677327, time: 124.045
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.68    -0.865  -16.3521]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.38    -0.99   -24.5054]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.45    -0.995  -23.7035]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.985   -0.965  -17.9213]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -75.80557122782422, time: 125.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.18    -0.88   -17.0725]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.515   -0.995  -23.0635]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.965   -0.975  -22.8269]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.93    -0.975  -16.4374]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -74.08956035227297, time: 124.298
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.165   -0.93   -16.7895]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.43   -0.97  -23.484]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.69    -1.     -22.0152]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.655   -0.995  -19.4952]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -80.02929157230815, time: 127.576
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.76    -0.82   -11.1311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.09    -1.     -23.4562]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.58    -1.     -22.7661]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.19    -0.995  -21.1329]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -82.33990306137618, time: 126.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.35   -0.92  -12.045]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.735   -0.98   -23.1106]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.125   -0.97   -22.2134]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.955   -0.99   -18.0366]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -76.91915410676972, time: 125.185
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.265   -0.96   -14.7708]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.77    -1.     -24.1055]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.93    -0.975  -24.4568]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.81    -0.97   -15.9107]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -83.02723110911934, time: 126.258
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.645   -0.9    -11.4561]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.095   -1.     -23.4405]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.185   -0.98   -23.5886]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.79    -0.95   -14.2416]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -73.94852493291987, time: 124.111
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.555  -0.8   -10.738]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.055   -0.995  -24.8489]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.58    -0.995  -23.3032]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.315   -0.97   -15.9361]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -70.89455189100632, time: 126.141
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.865   -0.87   -11.5335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.845   -0.995  -24.0978]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.145   -0.985  -23.1836]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.54    -0.985  -15.8559]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -77.4503555829802, time: 125.826
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.57    -0.895  -10.9425]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.81    -0.995  -20.8659]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.89    -0.845  -22.1526]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.555   -0.97   -16.3807]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -89.05258355978717, time: 126.054
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.13    -0.935   -9.7199]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.39    -0.995  -18.9474]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.07    -0.995  -23.6665]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.28    -0.975  -17.2144]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -76.29220865529057, time: 124.426
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.045   -0.9     -8.8817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.925   -0.99   -20.7106]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.61    -0.835  -22.2058]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.56    -0.99   -16.9709]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -72.58372592716852, time: 127.048
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.705   -0.92   -10.8059]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.065   -0.995  -24.0523]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.22    -0.775  -22.5553]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.91    -0.975  -16.6254]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -67.50632514842567, time: 126.976
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.58    -0.9     -9.1375]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.41    -0.995  -19.5735]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.455   -0.84   -20.9116]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.76   -0.975 -13.406]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -69.45323205668025, time: 128.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.275   -0.89    -9.4449]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.51    -1.     -19.9673]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.21    -0.93   -18.9079]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.37    -0.965  -15.0489]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -72.3245227305258, time: 126.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.185   -0.955  -10.9062]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.635   -1.     -18.6098]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.615   -0.845  -17.6787]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.265   -0.965  -15.9755]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -79.52237333478237, time: 126.961
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.01    -0.945   -9.9099]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.71   -0.995 -18.933]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.005   -0.78   -20.1195]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.23  -0.99 -16.33]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -65.92333853908514, time: 125.641
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.67    -1.     -12.0709]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.18    -0.965   -9.5145]
23000 50
steps: 1149950, episodes: 23000, mean episode reward: -83.13168868830269, time: 124.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.78    -0.945  -12.8474]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.825   -0.935  -12.7262]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.55    -1.     -13.4149]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.135  -0.97  -10.634]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -75.88681677966528, time: 128.238
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.87    -0.84    -9.9932]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.89    -0.95   -12.3891]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.47    -0.995  -12.2468]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.3     -0.91    -9.7463]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -83.07921438921392, time: 127.239
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.875   -0.96   -10.8599]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.29    -0.965  -13.7371]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.875   -0.995  -12.7012]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.5     -0.98   -11.0389]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -83.69672911867096, time: 127.278
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.845   -0.94   -11.5484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.485   -0.965  -12.5601]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.525   -0.99   -12.3529]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.005   -0.975  -10.5911]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -78.41729407116166, time: 126.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.955   -0.89   -10.3726]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.27    -0.975  -12.3148]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.175   -1.     -12.0136]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.625   -0.985  -10.3056]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -77.90984326115435, time: 128.775
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.81    -0.93   -10.2883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.595   -0.935  -11.2594]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.925  -0.99  -10.817]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.64    -0.985   -9.2585]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -71.63359450933501, time: 130.138
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.32    -0.885   -9.0083]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.36    -0.955  -11.1586]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.615   -0.975   -9.9013]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.745   -0.98    -9.2273]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -78.02168207456921, time: 128.532
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.785   -0.93   -11.2909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.49    -0.965  -12.0461]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.015   -0.99   -10.6769]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.665   -0.965   -9.9019]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -70.87416995102413, time: 128.828
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.15    -0.925   -9.3705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.185   -0.98   -12.1448]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.24    -0.94    -9.3892]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.44    -0.95    -8.8809]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -82.7249230095484, time: 128.206
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.81    -0.935  -11.1831]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.285   -0.97   -13.6242]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.87   -0.975 -11.526]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.95   -0.99   -9.491]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -77.09504674824994, time: 128.371
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.485   -0.925   -9.7661]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.775   -0.945  -12.4674]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.07    -0.985  -10.4468]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.735   -0.915   -9.1114]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -80.48980271857616, time: 128.459
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.435   -0.925   -9.9637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.76   -0.965 -12.486]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.915   -0.76    -9.9233]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.375   -0.975   -9.6006]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -78.60448524915633, time: 125.513
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.22    -0.875   -9.3193]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.905   -0.97   -13.5834]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.07    -0.905  -10.4908]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.81    -0.84    -8.9906]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -76.80809430584942, time: 128.124
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.93    -0.895  -10.3348]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.335   -0.995  -13.1403]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.645   -0.93    -9.9319]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.505   -0.825   -8.7701]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -72.84353763713796, time: 130.141
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.395   -0.9     -9.1402]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.785   -0.98   -12.5238]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.045   -0.805   -9.3823]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.56   -0.865  -9.582]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -76.10754750183409, time: 128.93
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.23    -0.885   -8.7763]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.07    -0.955  -12.0056]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.72    -0.86    -9.6584]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.775   -0.8     -9.6964]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -69.91366003509795, time: 130.607
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.01    -0.945   -9.5887]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.405   -0.955  -12.3024]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.765   -1.     -10.2341]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.6     -0.795   -8.2934]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -69.92663550051452, time: 129.946
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.195  -0.925  -7.6763]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.185   -0.985  -13.5034]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.765   -0.995   -9.5746]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.215   -0.82    -8.7108]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -72.48313627474654, time: 128.121
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.29    -0.935   -8.2933]
[-4.445  -0.5    -3.7435]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.69    -0.98   -10.0922]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.21   -0.51   -3.6013]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -81.93558022242972, time: 129.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.945   -1.     -13.3136]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.365  -0.55   -4.4233]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.47    -0.975  -10.3034]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.26   -0.57   -4.4571]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -78.69855246947083, time: 127.731
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.14    -0.975  -13.4036]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.235  -0.615  -5.1906]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.21    -0.995  -10.8971]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.595  -0.765  -6.9378]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -97.24940680351548, time: 130.997
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.65    -0.975  -15.2951]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.39   -0.45   -3.6513]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.35    -0.99   -10.4259]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.09   -0.655  -5.0627]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -84.00750083588548, time: 127.519
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.89    -1.     -13.8396]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.35   -0.545  -4.4335]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.21    -1.      -9.8811]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.05  -0.71  -5.752]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -89.24821519862344, time: 126.887
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.585   -1.     -13.8987]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.815  -0.61   -4.0694]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.02    -0.99   -10.3665]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.715  -0.78   -6.4617]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -75.61506005232177, time: 128.652
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.425   -0.985  -12.7289]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.65   -0.58   -3.7841]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.505   -0.97    -9.8563]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.095  -0.69   -6.3902]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -75.36962922814666, time: 128.209
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.605   -0.86   -11.7037]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.32   -0.615  -4.4011]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.015   -1.      -9.6058]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.945  -0.695  -6.8048]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -85.01479514346407, time: 129.151
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.8     -0.955  -11.5168]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.15   -0.615  -4.8667]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.565   -0.995  -10.0462]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.35    -0.85    -8.6784]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -86.793723026379, time: 128.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.99    -0.92   -11.1031]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.755  -0.68   -5.1745]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.75   -0.995 -10.127]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.14    -0.815   -8.9046]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -93.74828555970817, time: 127.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.235   -0.92   -11.1244]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.23   -0.635  -4.7735]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.22    -0.99   -10.4351]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.57    -0.85    -8.7372]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -79.38257705229658, time: 130.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.425   -0.86   -10.4827]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.915  -0.68   -5.5998]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.49   -0.98  -10.135]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.205   -0.77    -7.7555]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -73.45694923754014, time: 127.652
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.09    -0.995  -11.0628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.275  -0.75   -6.9513]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.425  -0.995  -9.302]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.75   -0.79   -6.8575]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -73.88487770085214, time: 127.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.805   -0.975  -10.7689]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.67   -0.725  -6.8488]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.695   -0.98    -9.2905]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.37   -0.78   -6.6369]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -77.5720868171064, time: 129.577
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.775   -0.995  -12.1913]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.715   -0.655   -7.0618]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.86    -0.915   -8.8565]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.12    -0.77    -7.8919]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -92.51171625978975, time: 129.492
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.295  -0.66   -8.455]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.96   -0.655  -6.4552]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.145   -0.905   -9.3864]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.985  -0.745  -6.9592]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -88.36870268777402, time: 130.534
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.625   -0.805  -12.9063]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.19   -0.665  -6.2234]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.87    -0.885   -8.7781]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.735  -0.835  -7.7942]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -87.16049100658306, time: 129.65
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.16    -0.835  -12.0411]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.025   -0.75    -8.4888]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.52   -0.975 -10.764]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.71   -0.835  -7.2038]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -79.70364100799253, time: 127.251
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.18   -0.985 -13.588]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.745  -0.68   -6.7692]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.635   -0.98   -10.3564]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.08   -0.775  -6.7309]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -84.2100289787007, time: 129.449
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.61    -0.89   -11.6085]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.165   -0.805   -9.6183]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.605   -0.99   -12.7126]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.495   -0.96    -9.1351]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -80.37829793132511, time: 127.079
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.69   -0.71   -7.2842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.83   -0.645  -5.9432]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.505   -1.     -12.9614]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.805 -0.675 -7.688]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -80.45422218948521, time: 127.102
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.88    -0.68    -9.0728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.99   -0.58   -5.0631]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.78    -0.985  -12.5432]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.8     -0.845   -9.5871]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -86.95670616204421, time: 126.933
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.415   -0.745   -9.2101]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.755  -0.555  -4.7055]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.74    -0.965  -11.1027]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.275   -0.96    -9.7032]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -85.12150363339737, time: 124.747
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.21    -0.72    -7.9527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.695  -0.6    -4.5508]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.005   -1.     -12.9382]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.625   -0.95    -9.7329]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -88.58901909174381, time: 126.392
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.805   -0.695   -8.7811]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.485 -0.62  -4.41 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.115   -0.985  -11.6594]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.8     -0.905   -9.2384]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -83.82574231604173, time: 126.918
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.755   -0.685   -8.4057]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.31   -0.535  -4.1271]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.25   -1.    -12.865]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.08    -0.86    -8.5986]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -82.31360693942887, time: 125.771
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.555   -0.75    -9.0897]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.31   -0.635  -3.5553]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.115   -0.995  -12.4805]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.72    -0.855   -9.0366]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -80.85638554687263, time: 125.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.71    -0.75    -7.6562]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.12   -0.65   -4.7279]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.675   -1.     -12.1063]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.98   -0.73   -7.0986]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -77.56664765290975, time: 128.84
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.62    -0.82    -9.0802]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.005  -0.59   -3.4204]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.26    -1.     -10.5858]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.11   -0.9    -8.784]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -85.97810589864842, time: 126.638
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.82    -0.815   -9.6676]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.96  -0.675 -4.086]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.975   -0.975  -11.4821]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.375  -0.605  -5.8018]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -103.24025367351891, time: 126.714
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.275   -0.865  -13.6409]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.41   -0.73   -6.5048]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.905  -0.435  -6.4295]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.71    -0.875  -11.3841]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -82.02896950136491, time: 125.919
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.735   -0.81    -9.8904]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.38   -0.725  -5.0779]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.81    -0.97   -11.9883]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.385   -0.815   -8.1638]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -100.23850040987041, time: 126.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.925   -0.905  -11.5387]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.7    -0.795  -5.1894]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.13    -0.91   -11.7979]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.43    -0.835   -9.4167]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -92.58365729726417, time: 128.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.55    -0.905  -11.5812]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.485  -0.56   -3.5796]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.915  -0.93  -12.167]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.54   -0.625  -7.191]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -84.07276053238715, time: 128.326
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.165   -0.895  -11.9612]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.755  -0.605  -4.3378]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.515   -0.89   -10.9001]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.92   -0.61   -6.5879]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -82.90219241259902, time: 128.556
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.36    -0.805   -9.7802]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.05   -0.655  -4.0099]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.74    -0.97   -10.8741]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.125   -0.895  -10.0575]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -81.7459793199215, time: 128.469
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.64    -0.835   -9.0805]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.365  -0.68   -5.9471]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.89    -0.82   -10.4375]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.83    -0.865   -9.8273]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -79.70443741890723, time: 126.888
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.295   -0.82    -7.7323]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.745  -0.75   -6.8108]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.71   -0.825 -10.234]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.745  -0.795  -8.647]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -78.90686667435749, time: 129.319
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.305   -0.845   -8.3128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-6.6   -0.745 -5.249]
[-15.08    -0.89   -10.8453]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.64    -0.895   -9.9933]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.955  -0.215  -2.1165]
23200 50
steps: 1159950, episodes: 23200, mean episode reward: -89.12659011527066, time: 130.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.74    -0.955  -14.5897]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.58    -0.96   -12.5482]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.12   -0.93  -11.271]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.065  -0.225  -2.3566]
23400 50
steps: 1169950, episodes: 23400, mean episode reward: -89.28692431236124, time: 128.161
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.885   -0.93   -15.9333]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.58    -0.955  -13.8878]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.15   -0.9    -9.612]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.75   -0.225  -2.0556]
23600 50
steps: 1179950, episodes: 23600, mean episode reward: -92.56930376594522, time: 129.257
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.375   -0.88   -13.8476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.95    -0.86   -11.9588]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.035   -0.88    -9.4806]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.285  -0.16   -0.9788]
23800 50
steps: 1189950, episodes: 23800, mean episode reward: -103.07867573170866, time: 131.489
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.555   -0.87   -13.8631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.61    -0.83    -9.8666]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.485   -0.85   -10.2392]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.045  -0.475  -4.7864]
24000 50
steps: 1199950, episodes: 24000, mean episode reward: -88.86045790283657, time: 129.335
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.855   -0.855  -13.5279]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.925   -0.77   -10.2428]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.715   -0.85   -12.2161]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.615  -0.29   -2.6749]
24200 50
steps: 1209950, episodes: 24200, mean episode reward: -85.10362016192832, time: 129.223
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.965   -0.955  -15.4486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.805   -0.77    -9.5392]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.635   -0.85   -10.4954]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.415 -0.17  -1.188]
24400 50
steps: 1219950, episodes: 24400, mean episode reward: -86.05955282592018, time: 129.657
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.315   -0.925  -13.9374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.235   -0.93   -13.2149]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.675   -0.89   -11.8407]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.96   -0.275  -2.3116]
24600 50
steps: 1229950, episodes: 24600, mean episode reward: -83.7275779821861, time: 128.573
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.82    -0.885  -13.8024]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.01    -0.945  -13.4889]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.9     -0.9    -10.8026]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.225  -0.21   -3.5265]
24800 50
steps: 1239950, episodes: 24800, mean episode reward: -74.32904945305704, time: 127.608
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.435  -0.865 -12.598]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.105   -0.76   -10.4688]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.965   -0.83    -9.8642]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.87  -0.385 -5.475]
25000 50
steps: 1249950, episodes: 25000, mean episode reward: -81.98975573232488, time: 128.773
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.82    -0.97   -13.6769]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.115  -0.785 -11.34 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.815  -0.88   -9.329]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.34   -0.445  -3.4328]
25200 50
steps: 1259950, episodes: 25200, mean episode reward: -92.81045638322892, time: 130.404
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.155   -0.905  -14.5477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.87    -0.745   -8.0802]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.38    -0.88    -9.6684]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.725  -0.315  -2.5789]
25400 50
steps: 1269950, episodes: 25400, mean episode reward: -101.07466278408887, time: 129.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.46    -0.895  -14.9613]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.965   -0.64   -10.1661]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.55    -0.89   -10.4232]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.78   -0.375  -2.9868]
25600 50
steps: 1279950, episodes: 25600, mean episode reward: -89.77938717967702, time: 130.326
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.645   -0.85   -14.1026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.305   -0.71    -8.1885]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.845   -0.885   -9.3044]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.76   -0.445  -6.4609]
25800 50
steps: 1289950, episodes: 25800, mean episode reward: -103.7974827988177, time: 131.216
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.07  -0.9  -14.  ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.605   -0.89   -10.6659]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.41    -0.925  -10.3339]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.66   -0.435  -4.0842]
26000 50
steps: 1299950, episodes: 26000, mean episode reward: -98.60777683725577, time: 129.326
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.225   -0.81   -13.8759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.22    -0.79    -9.6712]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.78    -0.885   -9.0166]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-2.415  -0.33   -2.0635]
26200 50
steps: 1309950, episodes: 26200, mean episode reward: -88.31964787056211, time: 131.431
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.815   -0.88   -15.1154]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.03   -0.93  -13.528]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.88    -0.93    -8.9657]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-1.97   -0.35   -1.7734]
26400 50
steps: 1319950, episodes: 26400, mean episode reward: -78.36494537323551, time: 131.707
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.5     -0.86   -12.8313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.7     -0.85   -11.6465]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.45    -0.94    -8.1774]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.46   -0.43   -2.9492]
26600 50
steps: 1329950, episodes: 26600, mean episode reward: -82.57440146588989, time: 128.579
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.94    -0.96   -15.2055]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.97    -0.675   -9.5024]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.105   -0.875   -9.9122]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.465   -0.675  -11.0315]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -86.18024435523952, time: 130.407
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.66    -0.92   -14.3959]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.43    -0.97   -13.9043]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.935   -0.91   -14.9375]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.2     -0.92   -23.3859]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -89.11007247341335, time: 128.429
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.975   -0.675  -11.8903]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.1     -0.995  -16.9436]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.58    -0.965  -15.5738]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.645   -0.99   -23.9753]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -86.41337591668922, time: 127.011
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.675   -0.79   -13.8897]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.155   -0.99   -17.8533]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.425   -0.935  -14.1934]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.41    -0.975  -23.4245]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -78.52094697309853, time: 129.899
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.58    -0.725  -14.2335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.465   -0.945  -16.2884]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.415   -0.905  -15.8476]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.66    -0.98   -20.7571]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -79.04745317825297, time: 127.532
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.965   -0.69   -10.7401]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.665   -0.975  -15.3449]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.66    -0.925  -12.9278]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.61    -0.915  -19.3499]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -77.0011535214188, time: 128.994
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.16    -0.715  -12.6872]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.59    -0.965  -12.4684]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.035   -0.975  -14.6937]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.83    -0.925  -18.1624]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -90.44302173046407, time: 127.972
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.855   -0.645  -10.8569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.56    -0.985  -13.7344]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.18    -0.94   -13.1758]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.49    -0.905  -19.6714]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -84.10498744511442, time: 124.127
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.37    -0.66   -10.8556]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.985   -0.985  -17.4774]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.18    -0.89   -12.9113]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.455   -0.99   -17.1184]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -79.50655122925656, time: 125.518
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.625  -0.605  -9.781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.305   -0.985  -13.9994]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.73    -0.89   -12.8081]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.365   -1.     -18.8386]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -80.74173283754862, time: 125.178
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.38   -0.745 -11.414]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.305   -0.875  -14.2214]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.12    -0.94   -13.0908]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.      -0.995  -18.2596]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -82.68293675081023, time: 124.614
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.86    -0.625   -8.1059]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.595   -0.885  -13.4169]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.12    -0.85   -12.9928]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.805   -0.985  -17.6576]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -69.73093458718063, time: 124.903
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.675   -0.71    -8.2111]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.255  -0.96  -13.251]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.13    -0.93   -13.1723]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.715   -1.     -17.2872]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -82.55075858740045, time: 124.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.595   -0.645   -7.3843]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.695   -0.85   -14.4924]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.92    -0.935  -13.4697]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.945   -1.     -19.3437]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -77.01998267384843, time: 125.146
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.055   -0.645   -7.2968]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.265   -0.99   -16.1978]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.725  -0.915 -12.63 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.285   -0.97   -16.9584]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -74.84227099836973, time: 124.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.13    -0.67    -7.9293]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.035   -0.865  -14.1827]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.315   -0.945  -10.4704]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.625   -0.99   -16.9641]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -77.5373697922913, time: 123.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.915  -0.66   -6.9116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.3     -0.95   -14.7017]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.89    -0.935  -11.6832]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.325   -1.     -17.9208]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -76.1721945262962, time: 122.846
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.335   -0.69    -7.3092]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.755  -0.94  -17.107]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.22    -0.955  -13.1823]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.405   -0.98   -17.1115]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -78.01853949979345, time: 124.244
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.255   -0.675   -7.1902]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.59    -0.815  -12.3957]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.405   -0.985  -14.2683]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.765   -0.99   -16.8781]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -71.22024700976343, time: 126.738
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.68   -0.685  -5.9606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.785   -0.965  -14.3039]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.545   -0.925  -12.6349]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.19    -0.91   -15.4806]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -72.14988879962688, time: 125.27926800 50
steps: 1339950, episodes: 26800, mean episode reward: -58.34449950664669, time: 127.125
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.76    -0.965  -15.1402]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.995   -0.89   -15.9325]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.405   -0.98   -17.6402]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.4     -0.94   -21.3276]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -74.88525395532564, time: 127.371
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.035   -0.945  -14.3176]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.665   -0.95   -18.7406]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.34   -0.89  -13.514]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.98    -0.855  -18.5729]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -61.85126582015946, time: 129.117
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.765   -0.915  -14.4066]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.46    -0.86   -17.2706]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.23    -0.92   -14.5461]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.795   -0.915  -20.8749]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -62.61665200932173, time: 127.184
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.26    -0.925  -13.8956]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.085   -0.875  -17.2867]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.475   -0.935  -18.5843]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.48    -0.945  -21.6456]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -64.63978866880005, time: 129.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.205   -0.915  -14.5481]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.295   -0.915  -16.0327]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.915   -0.975  -18.8638]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.27    -0.94   -22.7612]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -67.06255933836417, time: 128.18
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.835   -0.94   -15.5631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.775   -0.92   -16.8271]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.355   -0.975  -22.5876]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.82    -0.99   -21.9141]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -55.64152605435691, time: 127.233
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.94    -0.925  -15.2291]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.345   -0.915  -17.2434]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.82    -0.955  -22.3532]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.94    -0.955  -21.6805]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -58.14260856821127, time: 128.488
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.41    -0.9    -15.4029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.16    -0.935  -19.2455]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.09    -0.8    -20.4652]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.835   -0.905  -20.5773]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -57.34304374540339, time: 127.376
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.755   -0.94   -16.1877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.79    -0.935  -18.9046]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.27    -0.875  -22.1589]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.81   -0.83  -20.814]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -55.479132292665334, time: 126.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.87    -0.925  -15.0325]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.03    -0.97   -19.6757]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.045   -0.98   -23.7924]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.555   -0.945  -22.8191]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -54.63352633646631, time: 126.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.855   -0.94   -15.9917]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.98    -0.96   -18.7506]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.63    -0.91   -21.1152]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.115   -0.91   -23.0815]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -61.31063657038577, time: 126.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.87    -0.915  -15.0744]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.105   -0.92   -18.5403]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.87    -0.85   -20.3598]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.64    -0.94   -22.9942]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -65.26499909919917, time: 127.482
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.275   -0.955  -15.0726]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.705   -0.945  -18.6584]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.21    -0.92   -21.0083]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.855   -0.915  -21.7018]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -61.128695696035784, time: 125.864
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.395   -0.935  -15.8776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.02    -0.95   -18.1849]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.795   -0.975  -21.2055]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.885   -0.94   -21.6216]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -51.80435614843684, time: 126.55
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.405   -0.99   -17.2541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.365   -0.98   -18.5636]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.77    -0.93   -20.5413]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.46    -0.97   -21.4817]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -60.70275296350018, time: 126.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.295  -0.96  -15.961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.825   -0.935  -18.7235]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.495   -0.8    -19.0749]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.955   -0.95   -20.7687]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -51.404659028586735, time: 125.495
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.855   -0.945  -14.1643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.495   -0.945  -18.8064]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.105   -0.935  -21.3589]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.535   -0.955  -20.6804]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -64.72201830579627, time: 128.72
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.035   -0.95   -14.6732]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.68    -0.96   -18.2944]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.87   -0.89  -21.575]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.56    -0.975  -22.0516]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -61.29781211129571, time: 129.675
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.07    -0.975  -15.6519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.695   -0.96   -18.6144]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.535   -0.925  -20.2082]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.825   -0.78   -20.9819]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -84.10063382090634, time: 127.924
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.115  -0.545  -4.3096]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.225   -0.995  -23.4425]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.72    -0.99   -23.0192]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.955   -0.92   -21.9926]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -87.9218173031028, time: 125.889
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.835   -0.735   -7.9385]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.91    -1.     -21.8357]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.505   -0.95   -22.7882]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.97    -0.905  -23.1461]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -75.94172298924228, time: 127.117
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.93   -0.75   -6.5881]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.455  -1.    -22.693]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.405   -0.995  -21.5543]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.83    -0.985  -24.0371]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -82.30284289936907, time: 127.217
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.595  -0.61   -4.7064]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.35    -0.995  -23.5933]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.625   -0.99   -21.4343]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.795   -0.975  -23.5933]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -83.96951190833086, time: 127.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.475  -0.695  -5.5768]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.935   -1.     -21.8519]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.24    -1.     -22.7333]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.95    -0.975  -24.1098]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -84.99281387088311, time: 126.86
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.195  -0.68   -5.2715]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.64    -1.     -21.6588]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.84    -1.     -21.9931]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.015   -0.875  -23.5181]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -74.59444643178541, time: 125.682
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.615  -0.82   -6.5393]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.325   -1.     -21.8009]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.615   -1.     -22.4739]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-48.475   -0.925  -24.6906]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -73.19463162905218, time: 126.37
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.07  -0.67  -5.252]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.985   -0.995  -22.8462]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.605   -1.     -22.6731]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.83    -0.95   -23.4218]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -79.78423978260803, time: 126.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.64   -0.655  -4.8703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.625   -1.     -24.0502]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.57    -0.995  -23.9539]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.085   -1.     -23.7328]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -76.59975515685598, time: 124.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.225  -0.645  -4.5374]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.11    -1.     -21.6676]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.32    -0.98   -22.9051]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.915   -0.99   -23.8107]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -81.16368520832808, time: 126.589
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.055  -0.665  -5.2361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.01    -1.     -21.1568]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.695   -1.     -23.3879]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.91    -0.99   -21.7573]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -78.12637341685982, time: 125.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.06  -0.675 -5.146]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.425   -0.995  -21.7637]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-45.93    -1.     -22.8399]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.03    -0.995  -22.7249]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -75.76454590092398, time: 125.859
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.22   -0.745  -6.1114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.18    -1.     -21.7495]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-47.265   -0.995  -23.6874]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.875   -0.995  -22.7089]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -74.57648196115049, time: 125.156
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.14   -0.595  -4.4169]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.125   -1.     -19.6504]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.07    -1.     -23.2026]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.665   -0.995  -23.1644]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -87.079609419591, time: 124.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.82   -0.685  -4.9054]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.995   -1.     -19.9239]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.78    -0.985  -17.6831]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.435   -0.995  -20.0065]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -76.43112280455261, time: 126.73
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.435  -0.78   -5.4579]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.975   -0.945  -21.4276]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.46    -0.995  -21.0596]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.17    -0.97   -23.2067]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -77.22984295018409, time: 125.75
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.75   -0.805  -7.0303]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.655   -1.     -21.0205]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.42    -0.995  -21.5953]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.2     -0.95   -17.8964]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -81.24824667117515, time: 128.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.28  -0.695 -6.607]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.795   -0.94   -18.7052]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.34    -1.     -20.7397]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.135   -1.     -17.1502]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -77.37667007763933, time: 126.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.475  -0.71   -6.2618]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.63    -0.905  -16.5065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.12    -0.94   -11.1056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.555   -1.     -25.1973]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.86    -0.985  -24.9537]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.29    -0.705   -8.0402]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -68.78559390653265, time: 129.568
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.83    -0.89    -9.8864]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.325   -0.955  -24.9537]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-48.385   -1.     -24.8724]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.17    -0.905   -9.0881]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -76.22782971343558, time: 128.691
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.54    -0.915   -9.8145]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.74    -1.     -25.2964]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.725   -0.945  -23.2952]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.215   -0.8     -9.1188]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -79.4034806876096, time: 129.559
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.625  -0.925  -9.158]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.825   -0.995  -25.3648]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.85    -0.975  -21.9982]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.24    -0.555   -7.0607]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -70.96710790513075, time: 129.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.525   -0.93   -10.7414]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.915   -1.     -25.4473]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.185   -0.98   -24.0665]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.155   -0.865  -11.0568]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -72.34541226367537, time: 131.202
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.025   -0.92   -11.0498]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.835   -1.     -25.3862]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.05    -0.94   -22.1192]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.45    -0.975  -12.3197]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -65.27942636728822, time: 129.303
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.725   -0.95   -10.2725]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.92    -1.     -25.4399]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.69    -0.96   -17.5913]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.615   -0.925  -10.4189]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -72.12245655452195, time: 131.442
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.415   -0.95   -11.7114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.95    -1.     -24.4341]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.365   -0.97   -21.3944]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.925   -0.95   -11.3705]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -69.07217354135916, time: 128.88
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.655   -0.91   -10.4122]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.135   -1.     -23.8027]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-46.095   -0.96   -23.7983]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.16    -0.91   -10.1868]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -73.42877807304752, time: 129.411
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.185   -0.945  -10.7151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.245  -0.935 -24.349]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.135  -0.995 -22.868]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.78    -0.905  -10.6274]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -71.63162298800627, time: 128.352
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.83   -0.935 -10.116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.985   -1.     -24.7396]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.085   -0.985  -20.2679]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.955   -0.92   -10.8971]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -65.3392873132904, time: 128.791
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.48    -0.945   -9.8224]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.805   -1.     -24.5749]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.79    -0.975  -19.5847]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.09    -0.865  -10.6672]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -65.35291811053602, time: 128.168
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.27    -0.935   -9.7359]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.365   -1.     -25.0698]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.865   -0.995  -17.3717]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.725   -0.88   -11.2599]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -61.057966652876054, time: 131.159
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.175   -0.925   -9.4421]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.79    -1.     -25.3623]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.525   -0.99   -15.4727]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.8     -0.9    -10.9546]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -71.56492234651918, time: 127.555
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.585   -0.955   -9.9232]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.31    -0.975  -24.9801]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.075  -0.925 -15.105]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.29  -0.86 -10.68]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -70.69473671850494, time: 126.495
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.045  -0.965 -10.83 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.695   -1.     -24.8396]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.65   -0.84  -13.963]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.825   -0.905  -10.7109]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -61.902897505181954, time: 128.674
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.235   -0.99   -10.8308]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.855   -1.     -24.3596]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.655   -0.975  -16.5802]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.86    -0.88   -10.1354]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -75.01567362942028, time: 128.824
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.485   -0.99   -10.9231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.26    -1.     -23.4413]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.775   -0.83   -18.1723]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.1     -0.885  -11.1939]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -64.74183453066598, time: 129.853
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.695  -0.975 -10.617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.545   -1.     -23.8916]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.455   -0.935  -18.5222]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.225   -0.93   -10.4685]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -63.49780210623704, time: 129.188
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.81   -0.935 -10.326]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.01   -0.995 -17.008]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.175   -0.935  -20.5771]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.43    -0.985  -16.7639]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -73.36096463054389, time: 126.543
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.975   -0.94   -10.9979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.62    -0.97   -17.3648]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-37.265   -0.93   -19.9004]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.96    -0.97   -17.4982]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -68.9358062188295, time: 126.622
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.395  -0.99  -13.626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.265   -0.995  -17.8607]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.54    -0.845  -21.0187]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.495   -0.955  -16.1152]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -67.83292489621961, time: 125.203
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.75    -0.975  -12.1785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.55    -0.985  -17.8765]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.18    -0.83   -18.8731]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.465   -0.975  -14.4649]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -65.88808027102945, time: 127.299
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.675   -0.95   -12.6593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.31    -0.975  -18.9964]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.15    -0.935  -22.0527]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.955   -0.965  -15.3818]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -68.33033622742413, time: 127.603
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.86    -0.98   -10.9637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.41    -0.985  -18.0532]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.64    -0.915  -20.4941]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.675   -0.945  -12.5035]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -68.99378867388114, time: 126.195
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.415   -0.965  -10.9231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.63    -1.     -17.9601]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.21    -0.77   -19.0551]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.885   -0.99   -13.1838]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -73.320086669407, time: 127.005
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.185   -0.95   -11.5488]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.925   -1.     -19.7295]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.855   -0.885  -18.3539]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.315   -0.955  -14.7405]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -71.12430381326878, time: 125.311
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.39    -0.975  -12.6731]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.61    -1.     -20.7513]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.195   -0.855  -15.9269]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.92    -0.95   -11.9698]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -72.56188930840818, time: 125.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.82    -0.985  -11.9844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.81    -1.     -19.5771]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.17    -0.84   -16.3326]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.545   -0.95   -11.1583]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -85.12839958615658, time: 124.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.67    -0.955  -10.4943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.985   -0.995  -19.5388]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.72    -0.885  -19.3699]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.66    -0.895  -11.6131]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -67.60771582219148, time: 125.899
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.49    -0.965  -10.1638]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.635   -1.     -19.1521]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.79    -0.965  -17.1156]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.175   -0.98   -14.6274]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -70.93219322332781, time: 126.967
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.545   -0.965  -12.4946]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.625   -0.995  -20.2138]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.585   -0.965  -14.6111]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.33    -0.945  -11.7798]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -68.30337646815791, time: 126.696
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.485   -0.965  -11.0755]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.97    -0.985  -19.6784]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.25    -0.9    -14.2363]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.39   -0.95  -11.197]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -63.410381839333525, time: 125.553
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.72    -0.93   -10.1343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.065   -0.995  -19.5252]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.455   -0.945  -13.3639]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.475   -0.985  -11.1073]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -67.75955318358275, time: 125.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.38    -0.955   -9.7617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.76    -1.     -19.5036]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.015   -0.815  -13.2979]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.46   -0.96  -14.237]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -63.696738834068185, time: 126.445
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.92    -0.915   -8.0969]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.39    -1.     -19.2352]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.825   -0.92   -12.9503]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.74    -0.95   -10.4982]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -68.60449357156475, time: 126.781
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.49    -0.92   -10.8723]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.89    -0.995  -17.8234]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.965   -0.955  -14.1519]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.255   -0.985  -13.5457]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -71.69669426127095, time: 127.813
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.78    -0.89   -10.4877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.37    -1.     -18.5844]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.695   -0.935  -16.4142]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.375   -0.965  -13.0403]
30600
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.165   -0.965  -13.6013]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.795   -0.885  -15.4434]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.68   -0.805 -21.742]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -73.47900176732483, time: 129.485
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.285   -0.86    -9.7935]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.845   -0.955  -13.1333]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.73    -0.935  -16.1556]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.19    -1.     -22.8967]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -65.78546956277825, time: 128.223
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.695   -0.93    -9.9313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.49    -0.97   -12.6652]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.18    -0.87   -12.2623]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.21    -0.985  -18.8296]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -75.7778610740244, time: 128.105
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.375   -0.895  -11.2373]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.555   -0.92   -10.8333]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.95    -0.9    -13.3439]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.695   -1.     -18.6141]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -75.66633753493159, time: 128.682
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.82    -0.82    -9.4135]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.525   -0.965  -11.9711]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.81    -0.925  -13.1091]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.26    -1.     -20.0439]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -64.26968082028625, time: 129.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.245  -0.865  -8.725]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.91    -0.96   -14.9789]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.265   -0.92   -13.2895]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.92  -1.   -17.93]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -75.14270101444048, time: 129.578
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.94    -0.82    -9.2811]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.515   -0.975  -11.5992]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.59    -0.95   -12.4112]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.37    -0.93   -18.9424]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -67.69627407732175, time: 128.206
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.29    -0.905  -11.2292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.935   -0.97   -12.1439]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.81   -0.985 -13.811]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.635   -0.895  -19.2474]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -70.81672298507685, time: 129.376
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.91    -0.81    -9.0672]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.095   -0.95   -12.0856]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.07    -0.9    -11.8802]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.54    -1.     -20.6618]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -64.86146138580558, time: 127.437
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.74    -0.835   -9.1363]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.845  -0.995 -12.538]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.14   -0.875 -11.149]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.245   -1.     -20.6325]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -77.78450199112166, time: 127.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.38    -0.845   -9.8324]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.015   -0.95   -11.2101]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.195   -0.88   -10.8623]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.85    -1.     -20.2568]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -59.68955855315042, time: 127.403
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.35    -0.855   -9.3315]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.89    -0.965  -10.7615]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.35    -0.945  -12.8922]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.545  -1.    -19.611]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -70.32694578705278, time: 127.498
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.98    -0.87    -9.7202]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.12    -0.98   -11.5191]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.195   -0.975  -13.9591]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.96    -0.845  -17.7773]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -66.01883544426951, time: 127.774
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.98   -0.85  -10.315]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.32    -0.98   -11.2137]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.165   -0.94   -12.6808]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.275   -0.825  -19.3374]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -70.01646828547254, time: 128.414
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.16    -0.885  -10.8785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.28    -0.985  -11.5586]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.255   -0.935  -15.0379]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.11    -1.     -19.7548]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -69.19043271862988, time: 126.823
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.24    -0.815   -9.2836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.91    -0.96   -11.8229]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.68    -0.91   -13.8053]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.205   -0.995  -20.0228]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -65.12350833427219, time: 128.109
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.25    -0.74    -8.8029]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.23    -0.97   -11.8566]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.495   -0.96   -12.9672]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.7     -1.     -19.3033]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -70.43967059896022, time: 128.339
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.64    -0.83    -8.9231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.29    -0.995  -12.8252]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.37   -0.85  -11.071]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.155   -1.     -18.4751]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -68.86585177322162, time: 128.648
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.765   -0.87    -9.8799]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.305   -0.995  -12.5714]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.795   -1.     -13.3309]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.845   -1.     -19.4312]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -64.67753839955415, time: 129.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.72    -1.     -14.4387]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.13    -0.805   -8.2816]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -74.43572512620362, time: 129.185
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.315   -0.935  -11.7757]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.93   -0.875  -9.864]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.795   -0.975  -12.5225]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.07    -0.84    -8.8727]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -75.6495896114753, time: 128.111
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.595   -0.97   -13.2195]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.565   -0.945   -9.8836]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.905   -0.985  -12.4783]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.83    -0.835   -8.4676]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -76.15216048877774, time: 130.963
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.835   -0.92   -12.8698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.385   -0.93    -8.6977]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.535   -0.98   -11.7989]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.445   -0.915   -9.0269]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -73.81599642091564, time: 131.65
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.42    -0.985  -14.1513]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.49    -0.955  -10.5344]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.41    -1.     -11.3476]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.785  -0.865  -7.8707]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -68.93074077279887, time: 129.452
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.315   -0.985  -12.1626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.58  -0.89  -9.53]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.705  -1.    -11.959]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.89   -0.905  -8.0455]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -69.38399033443505, time: 128.33
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.98    -0.95   -11.5622]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.64    -0.865   -9.4259]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.78    -0.995  -11.5786]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.865   -0.87    -8.8747]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -71.37208732988147, time: 128.715
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.395   -0.965  -11.4079]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.695   -0.91    -9.8841]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.85    -0.98   -11.3331]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.02   -0.89   -9.327]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -73.89451437384925, time: 128.974
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.945   -0.965  -11.6756]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.58    -0.98   -11.7926]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.445   -0.995  -12.4763]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.435   -0.875   -8.1444]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -78.21708400575508, time: 129.91
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.08    -0.97   -11.4232]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.555   -0.9     -9.0235]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.535   -0.915  -11.2305]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.375   -0.83    -9.1586]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -77.08687504277117, time: 127.839
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.47    -0.96   -11.2683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.58    -0.89    -8.3592]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.745   -0.99   -11.9567]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.3   -0.875 -7.638]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -69.39991196386669, time: 128.406
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.42    -0.97   -11.2683]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.215   -0.825   -9.1297]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.695  -0.995 -12.379]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.035  -0.85   -7.5099]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -67.36755280167505, time: 129.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.125   -0.955  -10.6513]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.96    -0.905  -10.2044]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.035   -0.965  -11.8095]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.425  -0.875  -7.7982]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -77.73485185769647, time: 130.2
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.855   -0.905  -10.7907]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.1     -0.915  -10.6865]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.94    -0.88   -13.0803]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.625   -0.89    -8.6248]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -84.45240322642172, time: 129.744
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.5     -0.9    -11.9131]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.545   -0.89   -10.8493]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.24    -0.82   -11.8538]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.99    -0.865   -9.2853]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -70.27322075947292, time: 128.19
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.765   -0.965  -13.0442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.275   -0.925   -9.6044]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.37    -0.845  -12.7634]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.515  -0.865  -9.137]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -69.1496653024385, time: 128.034
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.285   -0.955  -11.0294]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.31    -0.915  -11.0781]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.135   -0.745  -10.4736]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.6     -0.84    -8.5786]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -80.3732538899782, time: 129.589
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.945  -0.95  -11.509]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.265   -0.85    -9.4681]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.765   -0.745  -10.4654]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.09    -0.91    -8.7053]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -72.5758190664664, time: 133.975
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.505   -0.93   -11.3407]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.595   -0.875   -9.8167]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.035   -0.92   -11.5518]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.425   -0.88    -8.4528]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -69.42980048449306, time: 129.943
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.325  -0.97  -10.7  ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.235   -0.875   -9.1614]

agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.88    -0.97   -13.4666]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.44    -0.98    -9.5116]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.49   -0.82   -8.407]
26800 50
steps: 1339950, episodes: 26800, mean episode reward: -77.59112768228789, time: 127.344
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.985   -0.9     -8.8113]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.735   -0.955  -13.2434]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.24   -0.84   -9.109]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.745  -0.805  -7.9482]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -73.88025768041557, time: 127.922
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.45    -0.9     -9.0801]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.43    -0.955  -13.3317]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.81    -0.925   -9.7793]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.75  -0.81  -7.944]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -68.1141804894765, time: 130.036
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.825   -0.91    -8.5769]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.24    -0.97   -13.2547]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.825  -0.915  -8.1992]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.27    -0.855   -8.5297]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -74.86785802747491, time: 130.17
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.865   -0.815   -8.2754]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.285   -0.955  -12.1972]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.725   -0.975   -9.0028]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.56    -0.83    -8.4133]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -77.12005215373892, time: 130.405
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.64   -0.91   -8.0023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.795   -0.95   -12.0956]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.255  -0.81   -7.7214]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.755  -0.81   -8.261]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -71.84364564929605, time: 129.547
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.56   -0.925  -7.7917]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.09    -0.985  -12.3451]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.835  -0.97   -8.997]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.105   -0.86    -7.9781]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -67.28218602121278, time: 128.269
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.21   -0.895  -7.4726]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.905   -0.985  -13.2286]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.445  -0.87   -7.8092]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.66    -0.87    -9.0489]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -76.06622799646684, time: 130.285
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.385   -0.89    -8.9845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.36   -0.995 -13.677]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.775  -0.705  -6.9909]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.335   -0.88    -8.8385]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -72.84699550880804, time: 127.572
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.125   -0.895   -8.9483]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.375   -0.965  -13.9064]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.1   -0.99  -9.69]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.835   -0.845   -8.5502]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -76.56786810206437, time: 128.501
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.375   -0.9     -9.2883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.89    -0.975  -13.4244]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.39   -0.775  -7.7355]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.02    -0.94    -9.9146]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -79.15822592990651, time: 127.379
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.805   -0.935  -10.2739]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.425   -0.965  -12.7863]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.26   -0.615  -6.6005]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.405  -0.94   -9.466]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -73.17222412907878, time: 129.646
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.85    -0.88    -9.2335]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.375   -0.97   -13.3413]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.73    -0.88    -8.8082]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.535   -0.915   -9.1798]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -75.82523007885928, time: 129.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.87    -0.91   -11.9183]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.375   -0.96   -13.9973]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.455   -0.995   -8.8155]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.26    -0.97    -9.6444]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -73.97810168870267, time: 127.334
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.395   -0.905  -11.6172]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.985   -0.97   -13.9364]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.455   -0.985   -9.4932]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.535   -0.895   -9.0753]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -74.69354357402428, time: 127.659
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.995   -0.895  -11.0363]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.04   -0.94  -13.083]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.94    -0.975   -9.1687]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.79    -0.945   -9.4636]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -80.53315366628595, time: 128.822
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.3     -0.91   -12.4028]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.62    -0.955  -14.1901]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.24    -0.835   -9.0612]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.025   -0.935   -9.4411]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -74.70195264205087, time: 130.131
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.69    -0.915  -12.7513]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.805   -0.99   -14.6107]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.36    -0.915   -9.0953]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.58    -0.935   -9.7048]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -69.82813077029549, time: 129.734
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.69    -0.895  -11.8664]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.965   -0.985  -13.8271]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.18    -0.95    -8.4523]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.58    -0.94    -9.5198]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -77.11768909918327, time: 128.484
agent0_energy_min, agent0_energy_max, agent0_energy_avg
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.425   -0.875   -9.6092]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.505   -0.855   -8.9346]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -94.77956832922162, time: 128.497
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.365   -0.905  -10.5932]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.385  -0.855  -6.8128]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.63    -0.815   -9.7814]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.805   -0.71    -8.5011]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -83.42039376831428, time: 127.309
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.76   -0.945  -9.626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.53   -0.825  -4.6043]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.56    -0.825   -9.4097]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.39    -0.74    -9.2624]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -86.04508472401807, time: 126.824
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.355  -0.905  -9.91 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.72   -0.82   -5.8228]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.825   -0.67    -8.6976]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.005   -0.93   -10.4924]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -79.25437346626765, time: 129.912
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.6     -0.885   -9.0915]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.345  -0.76   -4.5121]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.01    -0.705   -9.2983]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.87    -0.94    -9.6502]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -77.04541008466636, time: 126.766
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.635   -0.92    -9.1324]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.49   -0.75   -4.6417]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.465   -0.605   -8.6743]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.42    -0.94   -10.0705]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -78.12163309389716, time: 127.391
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.97    -0.935   -9.6666]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-4.755 -0.665 -3.961]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.445   -0.65    -8.6997]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.67    -0.815   -9.9277]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -89.34434248363549, time: 128.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.88    -0.845   -8.6528]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.605  -0.675  -4.4032]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.145   -0.68    -8.8558]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.015   -0.945  -11.2626]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -66.90861564793357, time: 127.902
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.245   -0.82    -8.9815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-5.485  -0.67   -4.2298]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.775   -0.75    -8.7271]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.27    -0.9     -9.7554]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -83.19242706290079, time: 127.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.675   -0.925   -8.4595]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.43   -0.755  -6.4783]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.165   -0.785   -8.9893]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.265   -0.825   -8.0675]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -77.41802856066515, time: 126.561
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.275   -0.875   -8.4872]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.21   -0.755  -5.5999]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.55    -0.74    -8.5676]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.06    -0.825  -10.8972]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -77.6878279446399, time: 127.346
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.535   -0.92    -9.8329]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.405  -0.8    -5.5969]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.69    -0.79    -7.7639]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.935   -0.935  -12.6016]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -78.25228362086413, time: 126.975
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.36   -0.855  -8.509]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.81  -0.805 -6.585]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.305   -0.81    -8.0263]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.48   -0.98  -10.846]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -78.61502165263559, time: 127.888
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.605  -0.865  -8.637]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.805  -0.815  -6.3103]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.92   -0.73   -7.4811]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.01    -0.955  -10.0935]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -76.3655662756794, time: 126.216
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.85    -0.84    -8.3304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.56    -0.78    -7.0613]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.34    -0.735   -7.6018]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.06    -0.95   -10.4912]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -78.79707185679442, time: 126.151
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.72    -0.92    -8.9539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.37    -0.73    -6.6161]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.915   -0.75    -8.1283]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.08    -0.96   -11.3508]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -92.98230742455449, time: 128.069
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.14    -0.83    -9.1405]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.645  -0.755  -7.243]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.49    -0.78    -7.4308]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.385   -0.915  -10.6477]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -80.7660273656339, time: 129.317
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.915   -0.935   -9.6121]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.65   -0.745  -6.0154]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.11    -0.81    -7.5437]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.695   -0.81    -9.0942]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -80.86744517321762, time: 128.6
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.38    -0.955   -9.1132]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.905   -0.925   -9.5234]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.37    -0.885   -9.1379]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.78    -0.735   -8.8616]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -74.02812212630782, time: 126.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.39   -0.945  -8.977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.95    -0.945   -9.8126]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.98   -0.845  -6.7454]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -73.89813573177716, time: 128.746
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.91    -0.91   -10.7438]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.985  -0.74   -6.4525]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.74   -0.985  -8.873]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.56   -0.865  -7.2683]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -81.4186169554371, time: 130.29
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.73    -1.     -12.4563]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.805   -0.81    -8.7104]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.41    -0.945   -9.2989]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.525  -0.84   -7.6919]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -78.6517205466666, time: 130.332
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.675   -0.785   -9.9699]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.565   -0.82    -8.6552]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.32    -0.99    -9.2431]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.935  -0.825  -7.0459]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -82.74936045697777, time: 131.208
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.965   -0.875  -10.1636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.63    -0.81    -8.3683]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.375   -0.98    -9.1962]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.61   -0.84   -6.9951]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -71.76487242217229, time: 130.185
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.03    -0.925   -9.8618]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.21    -0.835   -9.7619]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.13    -0.945   -8.4652]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.55   -0.79   -6.7701]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -83.23375771305751, time: 130.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.825   -0.85    -9.9153]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.51   -0.77   -6.9718]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.945   -0.915   -8.8823]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.395   -0.85    -7.9269]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -71.42079716978438, time: 133.346
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.4     -0.905   -9.5751]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.42    -0.86    -8.0998]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.69   -0.895  -7.1286]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.465  -0.87   -7.4803]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -77.00219536147884, time: 127.985
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.665   -0.76    -8.3927]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.64   -0.895  -9.644]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.11    -0.925   -8.2371]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.455   -0.895   -8.6676]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -69.05108187278095, time: 128.824
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.42    -0.845   -8.4167]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.24   -0.88   -8.548]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.44   -0.965  -7.9281]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.835   -0.91    -8.5304]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -82.75949315760892, time: 127.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.11    -0.82    -8.7292]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.44  -0.83  -5.746]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.585   -0.925   -8.4102]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.065   -0.96   -10.5148]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -78.48690902735453, time: 127.972
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.225   -0.84    -8.3143]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.935  -0.92   -8.754]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.615   -0.985   -8.5273]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.43    -0.935  -10.1454]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -73.74801075742405, time: 130.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.27    -0.895   -9.6568]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.05    -0.88    -7.9503]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.72    -0.98    -8.7093]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.345   -0.925   -9.4194]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -84.92834323584358, time: 128.669
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.495   -0.88    -9.7461]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.805  -0.815  -8.26 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.945   -0.965   -8.9557]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.925   -0.88    -9.7752]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -79.04834023407604, time: 126.633
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.74    -0.75   -10.0149]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.71   -0.935 -10.2  ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.32    -0.965   -8.9573]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.255   -0.88    -9.5944]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -80.30345590051029, time: 127.987
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.34    -0.81    -8.8442]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.345 -0.905 -7.488]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.975   -0.98    -9.5497]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.875   -0.955  -10.2971]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -79.84069854230968, time: 128.965
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.545  -0.825  -9.359]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.99   -0.855  -7.1683]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.885   -0.945   -8.7692]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.335  -0.94   -9.565]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -88.18198438311767, time: 131.468
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.065   -0.75    -8.9692]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.51   -0.885  -6.7813]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.695   -0.905   -8.5714]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.495   -0.955   -9.1024]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -78.79481140283164, time: 128.986
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.955   -0.875  -12.2925]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.77    -0.9     -8.4314]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.33    -0.92    -9.0309]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.225   -0.85    -8.3833]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -79.61718525583184, time: 128.059
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.25    -0.89   -10.6037]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.365   -0.87   -10.5803]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.61    -0.95   -10.0656]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.965   -0.64    -8.3421]
27000 50
steps: 1349950, episodes: 27000, mean episode reward: -78.79806077864488, time: 130.306
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.08    -0.895  -15.3983]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.675   -0.925  -12.3317]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.15    -0.85    -9.1415]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.325  -0.665  -8.72 ]
27200 50
steps: 1359950, episodes: 27200, mean episode reward: -76.644968425038, time: 131.714
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.315   -0.99   -16.3025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.215   -0.95   -12.0663]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.65    -0.925   -9.6755]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.97  -0.495 -3.991]
27400 50
steps: 1369950, episodes: 27400, mean episode reward: -127.18456814600958, time: 131.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.71    -0.985  -17.3433]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.55    -0.775  -11.8837]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.66    -0.825   -9.5674]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.73   -0.615  -4.3831]
27600 50
steps: 1379950, episodes: 27600, mean episode reward: -93.14116257085404, time: 131.879
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.345   -0.985  -15.3119]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.18   -0.795  -9.045]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.055   -0.835  -10.3779]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.495  -0.37   -2.8465]
27800 50
steps: 1389950, episodes: 27800, mean episode reward: -89.45120760140287, time: 129.161
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.275   -0.95   -16.7045]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.595   -0.96   -10.1844]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.095   -0.83    -8.8704]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.65   -0.48   -3.7785]
28000 50
steps: 1399950, episodes: 28000, mean episode reward: -88.72700053843863, time: 128.803
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.1     -0.85   -15.5123]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.12    -0.885  -11.4694]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.265   -0.885   -9.0544]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.28   -0.475  -3.4578]
28200 50
steps: 1409950, episodes: 28200, mean episode reward: -95.60160207095615, time: 131.164
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.025   -0.815  -12.2821]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.845   -0.94   -11.1285]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.495   -0.89    -9.2076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.96   -0.575  -6.2419]
28400 50
steps: 1419950, episodes: 28400, mean episode reward: -87.01826763292873, time: 129.354
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.885   -0.865  -15.3301]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.175   -0.835  -11.3558]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.12    -0.79    -8.6917]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.97   -0.49   -3.8861]
28600 50
steps: 1429950, episodes: 28600, mean episode reward: -82.75582971363986, time: 129.006
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.22    -0.925  -15.1268]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.055   -0.835  -11.4627]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.7     -0.855   -7.7861]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.425 -0.56  -5.161]
28800 50
steps: 1439950, episodes: 28800, mean episode reward: -103.79048002372538, time: 130.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.44   -0.875 -13.382]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.215   -0.75   -10.5075]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.995   -0.865   -9.5563]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.285  -0.36   -3.8956]
29000 50
steps: 1449950, episodes: 29000, mean episode reward: -81.96070944579306, time: 129.215
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.795   -0.905  -13.7737]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.61  -0.84 -11.65]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.085   -0.925   -8.4308]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.22   -0.595  -5.5489]
29200 50
steps: 1459950, episodes: 29200, mean episode reward: -87.00744789681498, time: 130.039
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.04    -0.895  -13.1274]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.16   -0.805 -10.878]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.67   -0.87   -7.6924]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.04  -0.495 -4.697]
29400 50
steps: 1469950, episodes: 29400, mean episode reward: -75.57769305394666, time: 127.945
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.49    -0.81   -12.1986]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.105   -0.825   -9.9228]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.685  -0.91   -7.1761]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.82   -0.61   -4.8159]
29600 50
steps: 1479950, episodes: 29600, mean episode reward: -81.72351387655938, time: 127.989
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.225   -0.915  -13.5661]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.655   -0.805  -11.2814]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.27   -0.94   -7.6045]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.43   -0.595  -4.8928]
29800 50
steps: 1489950, episodes: 29800, mean episode reward: -91.92775110662038, time: 128.552
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.45    -0.925  -15.0344]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.325   -0.785  -12.4557]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.62    -0.9     -8.2091]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-3.425 -0.54  -2.845]
30000 50
steps: 1499950, episodes: 30000, mean episode reward: -79.30651191987197, time: 131.621
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.115   -0.945  -12.9413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.7     -0.915  -11.4058]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.85    -0.92    -8.2631]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.3    -0.62   -4.7912]
30200 50
steps: 1509950, episodes: 30200, mean episode reward: -94.88806143700894, time: 130.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.34    -0.94   -13.9622]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.795   -0.965  -12.1469]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.45    -0.905   -8.4862]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.475  -0.675  -6.2318]
30400 50
steps: 1519950, episodes: 30400, mean episode reward: -77.4664024666579, time: 129.438
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.635   -0.975  -14.7635]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.32   -0.955 -12.103]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.645  -0.94   -8.0377]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.77   -0.705  -7.1222]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -74.91518852585077, time: 128.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.4     -0.95   -14.1961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg 50
steps: 1529950, episodes: 30600, mean episode reward: -71.12728095023168, time: 126.968
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.12    -0.845   -8.9989]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.83    -1.     -17.8212]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.91    -0.995  -17.9489]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.34    -0.99   -12.4334]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -70.20080964137045, time: 127.097
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.74    -0.89   -10.1345]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.105  -1.    -17.488]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.      -0.99   -16.1439]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.52    -0.975  -11.7134]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -67.64819344134574, time: 126.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.365   -0.92    -9.6928]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.6    -0.99  -19.848]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.72    -0.98   -16.3563]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.98    -0.945  -11.8901]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -71.98795203202218, time: 128.802
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.8     -0.875  -10.2231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.3     -1.     -19.9139]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.055   -0.995  -17.5272]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.2     -0.985  -12.5423]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -70.02702704816875, time: 125.316
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.545   -0.94   -11.5943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.925   -1.     -20.3273]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.63    -0.995  -15.9648]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.25    -0.985  -13.0557]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -68.43795148351197, time: 127.455
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.985   -0.88    -9.8968]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.725   -0.995  -17.3921]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.67    -0.995  -15.4135]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.295   -0.975  -12.1207]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -59.88161200255812, time: 126.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.215   -0.87   -11.1446]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.465   -1.     -16.7242]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.735   -0.995  -13.1705]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.45    -0.955  -12.4846]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -65.8229039556949, time: 126.522
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.785   -0.9     -9.8825]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.18    -0.97   -17.1999]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.015   -1.     -15.3447]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.55    -0.96   -13.3838]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -72.54766646240716, time: 127.892
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.9     -0.885  -10.2523]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.165   -1.     -16.6513]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.17    -0.99   -15.9044]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.25    -0.965  -14.4062]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -67.31726501636956, time: 127.358
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.775   -0.895  -10.3547]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.945   -0.985  -16.4803]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.87    -0.815  -15.7131]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.81    -0.97   -13.6678]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -68.3237453904358, time: 127.962
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.06    -0.93    -9.6764]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.36    -1.     -16.9035]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.94    -0.995  -15.2842]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.33    -0.965  -12.8643]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -67.04275437800034, time: 126.03
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.985   -0.935  -11.4085]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.34    -1.     -17.2914]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.03   -0.985 -15.035]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.655   -0.935  -10.9673]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -63.27802563575784, time: 128.264
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.675   -0.92    -9.6576]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.42    -0.995  -19.4694]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.185   -0.995  -14.3716]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.925   -0.98   -11.5386]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -74.81450615526413, time: 128.423
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.005   -0.945   -9.5243]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.34    -0.99   -18.5754]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.385   -0.915  -12.7545]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.325   -0.95   -11.2047]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -67.9615884084216, time: 126.51
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.155   -0.92    -8.6507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.35    -0.995  -19.4815]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.45    -0.915  -12.6057]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.03    -0.955  -11.7866]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -58.84274015331121, time: 126.337
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.315   -0.895   -8.6783]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.155   -1.     -17.7605]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.085   -0.98   -11.9644]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.3     -0.955  -11.9018]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -65.46325553932813, time: 127.641
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.745   -0.975  -11.8307]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.23    -0.995  -16.3748]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.28    -0.98   -11.8702]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.985   -0.965  -11.9599]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -63.59096930636581, time: 126.835
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.43    -0.95    -9.1188]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.415   -0.995  -17.0211]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.815   -0.98   -11.8679]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.05    -0.965  -11.5014]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -64.37327852222063, time: 127.684
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.395   -0.96   -14.1511]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.72    -0.995  -18.4932]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.605   -0.965  -12.4948]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-5.7    -0.565  -4.3128]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.77    -0.955  -14.5407]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.78   -0.83  -10.044]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.585   -0.81   -14.5416]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -73.56916989057137, time: 126.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.815   -0.675   -7.1328]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.33    -0.94   -11.5619]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.355   -0.98   -14.1636]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.69    -0.93   -18.6493]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -75.89678385932126, time: 124.944
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.07   -0.58   -5.0997]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.04    -0.91   -12.9055]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.      -0.98   -11.9843]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.675  -0.93  -16.883]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -72.01610432587218, time: 125.081
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.615  -0.5    -5.3691]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.805   -0.945  -12.6732]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.4     -0.94   -11.4361]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.57    -0.945  -18.0816]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -74.46931156564504, time: 125.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.685  -0.575  -5.7898]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.805   -0.925  -11.3085]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.7     -0.95   -15.9121]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.475   -0.8    -15.1141]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -72.3960797314216, time: 125.295
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.33   -0.65   -5.5872]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.445   -0.96   -13.7081]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.56   -0.94  -13.012]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.46    -0.95   -18.6254]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -86.53589237130589, time: 125.251
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.49    -0.605   -7.3022]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.29    -0.855  -14.9291]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.26    -0.97   -13.2099]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.26    -0.88   -16.9067]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -70.11542142530152, time: 125.122
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.76    -0.81   -11.5071]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.335   -0.98   -13.3405]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.695   -0.985  -13.0349]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.205   -0.855  -16.3179]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -76.51901019656347, time: 124.914
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.735   -0.66    -7.3648]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.085   -0.945  -13.7385]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.745   -0.97   -12.1421]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.05    -0.73   -14.7513]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -74.0403235046912, time: 127.345
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.515   -0.695   -7.9476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.035   -0.97   -13.2035]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.335   -0.935  -12.1011]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.845   -0.835  -17.0089]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -70.37067315477077, time: 126.729
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.73   -0.59   -4.9668]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.885   -0.92   -12.1102]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.97    -0.915  -12.2217]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.      -0.78   -14.9704]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -67.98631242172327, time: 128.341
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.05   -0.725  -6.8762]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-28.04    -0.995  -15.7213]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.37    -0.91   -14.4415]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.04   -0.905 -16.071]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -72.46827714529701, time: 126.979
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.545  -0.62   -5.1341]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.555   -0.97   -16.4117]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.64    -0.91   -11.4767]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.285   -0.825  -17.6594]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -69.89941034353019, time: 127.416
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.82    -0.735   -7.5692]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.945   -0.93   -16.6758]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.85    -0.86   -11.0338]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.39    -0.89   -17.6883]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -75.60330280429804, time: 125.554
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.885   -0.66    -7.2362]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.125  -0.835 -14.671]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.65    -0.965  -12.7541]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.505   -0.82   -14.1982]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -75.4578397445913, time: 126.531
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.08    -0.65    -8.0411]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.285  -0.945 -15.988]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.555   -0.97   -13.1204]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.995   -0.835  -15.4461]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -76.8726998786714, time: 125.084
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.71    -0.66    -7.0649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.205   -0.935  -15.0806]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.715   -0.96   -11.5955]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.365   -0.78   -15.1898]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -75.2893352263557, time: 127.807
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.31    -0.77    -9.2888]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.095   -0.97   -13.9401]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.72    -0.94   -13.8619]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.74    -0.835  -16.9849]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -77.45434519713623, time: 124.3
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.905  -0.665  -5.7611]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.63    -0.93   -13.6566]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.15    -0.98   -13.7181]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.605  -0.775 -17.472]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -77.94121915207259, time: 125.741
[-20.925   -0.985  -12.9531]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.15    -0.925  -19.5646]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -83.6240837253639, time: 125.891
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.925  -0.79   -7.2411]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.26   -0.845 -14.573]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.06    -1.     -15.9754]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-34.245   -0.93   -18.5783]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -72.75577561518277, time: 127.008
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.01   -0.735  -5.9367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.88    -0.78   -13.9609]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.47    -1.     -14.3306]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.03    -0.98   -17.2213]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -86.82329695598717, time: 126.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.85   -0.73   -5.6491]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.845   -0.885  -17.6501]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.76    -0.585   -8.2255]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.33    -0.96   -17.7117]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -76.56951991642319, time: 126.124
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.78   -0.765  -6.7607]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.795   -0.9    -20.0575]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.775   -0.925  -13.4797]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.615   -0.84   -15.6965]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -84.90943395539468, time: 128.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.655  -0.815  -7.0636]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.11    -0.715  -14.5233]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.29   -0.55   -7.106]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.435  -0.785 -15.304]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -79.72339506197768, time: 127.472
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.83  -0.83  -7.096]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.34    -0.78   -17.3272]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.735   -0.82   -15.0105]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.43   -0.69  -15.409]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -82.7206264333852, time: 126.485
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.115  -0.84   -7.897]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.765   -0.755  -14.1916]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.08    -0.835  -12.2233]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.965   -0.81   -16.5168]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -86.78506593696325, time: 126.494
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.605  -0.755  -6.6389]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.365   -0.68   -11.5239]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.905   -0.885  -15.1229]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.09    -0.915  -17.9312]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -75.60858254485977, time: 128.061
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.455  -0.75   -6.2065]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.725   -0.825  -16.6258]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.185   -0.73    -7.7454]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.805   -0.645  -12.4047]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -80.62185240173496, time: 126.989
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.395   -0.745   -7.3011]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.47    -0.825  -17.8111]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.825   -0.82   -12.4568]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.075  -0.65  -10.649]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -100.17296312458333, time: 126.234
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.625  -0.755  -6.2064]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.095   -0.805  -18.1684]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.465   -0.845   -9.4377]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.77    -0.675   -8.7939]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -71.18400626722364, time: 128.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.04   -0.73   -6.2168]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.155   -0.81   -17.9015]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.28    -0.97   -16.9685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.175   -0.965  -15.3432]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -81.68895285031456, time: 128.531
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.54 -0.68 -5.38]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.74    -0.87   -20.1377]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.86    -0.85    -9.2948]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.085   -0.85   -15.7386]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -80.52305581946352, time: 127.46
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.925  -0.8    -5.8617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.09    -0.865  -19.3125]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.34    -0.795  -10.7238]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.94    -0.925  -16.7505]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -69.77718098274704, time: 126.36
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.14   -0.765  -5.1867]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.695   -0.885  -21.2955]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.35    -0.705   -8.5859]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.14    -0.94   -16.9886]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -71.77530407013977, time: 128.388
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.015 -0.815 -5.961]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.41    -0.9    -21.3829]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.93    -0.8     -9.4322]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.59   -0.965 -17.296]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -77.07919927160202, time: 127.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.     -0.715  -4.9581]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.55    -0.885  -20.5266]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.08    -0.8     -9.9188]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.675   -0.95   -16.3555]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -78.34334985832287, time: 125.359
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.15    -0.78    -7.6907]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.805   -0.91   -21.2463]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.615   -0.76    -7.9164]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.485   -0.84   -14.3673]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -77.40190579782897, time: 128.724
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.235  -0.8    -6.4896]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.73    -0.9    -21.6497]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.655   -0.955  -22.2006]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -59.214814379415365, time: 128.831
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.78    -0.97   -15.5003]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.32    -0.975  -17.9721]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.505   -0.695  -19.3854]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.77    -0.88   -21.0101]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -69.2299241430724, time: 127.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.555   -0.95   -14.6977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.865   -0.965  -17.3135]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-43.38    -0.865  -21.7095]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.795   -0.83   -20.6415]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -53.4137865375694, time: 127.271
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.975  -0.975 -16.698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.37    -0.95   -19.0754]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-44.08    -0.765  -21.8669]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.21   -0.98  -22.493]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -68.18940281847921, time: 131.607
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.635   -0.95   -17.0519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-38.13    -0.97   -19.4918]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-40.835   -0.76   -20.5255]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.025   -0.865  -21.1231]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -49.4045754799852, time: 126.79
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.82   -0.965 -15.281]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.215   -0.97   -18.3823]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.185   -0.87   -17.6646]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.475   -0.995  -22.9164]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -59.92873059648316, time: 128.145
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.6     -0.965  -16.9869]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.47    -0.95   -17.6942]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-41.895   -0.85   -21.3959]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.755   -0.945  -21.7089]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -54.01623620078615, time: 128.646
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.47    -0.965  -16.3033]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.94    -0.985  -17.9084]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.215   -0.87   -19.7785]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.57    -0.99   -22.6704]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -58.96449057717097, time: 127.507
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.16    -0.885  -13.0842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.89    -0.97   -17.7359]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.875   -0.95   -18.0417]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.065   -0.995  -22.3009]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -57.364637470356044, time: 129.907
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.21    -0.93   -15.6759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.23    -0.96   -16.0079]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.385   -0.85   -19.7159]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.195  -0.96  -21.711]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -54.53546617783978, time: 129.141
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.735   -0.965  -14.3524]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.16    -0.955  -17.3839]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.695  -0.91  -18.69 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.97    -0.975  -23.0198]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -51.40228568292132, time: 128.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.425   -0.95   -14.7419]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.595   -0.97   -17.9908]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.95    -0.915  -16.8749]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.63    -0.995  -22.6543]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -62.4870325599422, time: 128.817
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.47    -0.98   -14.2386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.04    -0.98   -17.4837]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.965   -0.935  -14.7241]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.18    -0.99   -22.4267]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -62.97299617589006, time: 129.695
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.605   -0.955  -16.0287]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.43   -0.975 -18.089]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.16    -0.935  -14.2217]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.025   -0.99   -22.7084]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -50.78143806200784, time: 129.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.345   -0.96   -13.8956]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.42    -1.     -18.5011]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.065   -0.95   -16.2452]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.08    -0.97   -21.7807]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -56.95594305930798, time: 127.894
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.77    -0.96   -15.0158]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.19    -0.97   -17.6195]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.095   -0.895  -14.7593]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.405   -0.98   -23.0385]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -52.71931492354048, time: 129.926
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.98    -0.97   -15.0261]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.075   -0.98   -18.2373]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.98   -0.98  -18.178]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.465   -0.92   -22.3547]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -63.51473573705998, time: 129.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.805   -0.925  -15.5781]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.775   -0.955  -16.1698]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.745   -0.875  -17.9173]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.02    -0.96   -22.5928]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -58.25589439987984, time: 129.063
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.63    -0.93   -14.7434]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.17    -0.985  -16.3126]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.96    -0.92   -16.4688]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.85   -0.97  -22.931]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -54.208503244076674, time: 127.874
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.415   -0.97   -16.0715]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.585   -0.95   -21.2705]

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.18    -0.95    -9.8461]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.34    -1.     -24.5449]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.43    -0.965  -15.3014]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.65    -0.93    -9.9749]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -65.85982463773162, time: 129.471
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.62   -0.98   -9.594]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.95    -0.985  -24.1118]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.855   -0.925  -16.2558]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.65   -0.845  -9.271]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -64.61998433173514, time: 128.469
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.645   -0.91    -9.4344]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.645  -1.    -25.281]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.035   -0.87   -15.7114]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.025   -0.825   -8.8939]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -64.83060389093406, time: 129.676
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.335   -0.91    -9.8892]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.      -1.     -24.8411]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.83   -0.96  -16.861]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.02    -0.905   -9.7589]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -68.03375996598648, time: 128.06
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.505   -0.935   -9.8435]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.175  -1.    -24.908]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.17    -0.98   -13.8343]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.795  -0.73   -7.6828]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -68.17248962422242, time: 127.106
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.12    -0.98    -9.7809]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.015   -1.     -24.8029]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.58    -0.965  -14.2622]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.835   -0.765   -9.1379]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -68.04998772422213, time: 128.018
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.695   -0.94    -9.2304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.21    -0.995  -24.9419]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.44    -0.97   -17.2137]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.935   -0.85    -8.4688]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -69.58577106161823, time: 127.049
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.58    -0.945   -9.8199]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.325  -0.945 -25.031]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-28.095   -0.98   -16.9352]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.355   -0.895   -9.5714]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -65.05558169846395, time: 128.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.12    -0.995  -10.3315]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.775  -0.995 -25.328]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.585  -0.99  -15.36 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.735   -0.875   -9.3834]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -64.78092404864998, time: 128.886
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.605   -0.975   -9.8791]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.75    -0.99   -25.3555]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.435   -0.98   -18.0787]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.45    -0.82    -8.9272]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -63.2077965208372, time: 129.387
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.89    -0.96   -10.2004]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.33    -0.99   -25.0729]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.295   -0.96   -12.3109]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.305   -0.855   -9.4087]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -67.9726059229297, time: 125.667
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.665  -0.995 -11.123]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.245   -1.     -24.3267]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.675   -0.97   -12.2547]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.535   -0.87    -9.1349]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -61.38290266395103, time: 129.497
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.35    -0.945   -9.9734]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.825   -0.995  -25.3903]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.79    -0.98   -12.6859]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.13    -0.95    -9.9466]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -75.54226826775808, time: 125.635
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.38    -0.97   -10.4525]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.87    -0.875  -23.9415]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.29    -0.865  -11.5749]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.73    -0.895  -10.4894]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -63.8063801994821, time: 126.952
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.9     -0.965  -11.1951]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.655   -0.98   -24.4647]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.47    -0.82   -10.0246]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.705   -0.935  -10.8411]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -69.55428815367482, time: 125.042
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.31    -0.985  -11.0922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.36    -1.     -25.0398]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.095   -0.915  -12.4563]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.795  -0.885 -11.399]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -66.64171962093131, time: 126.734
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.515   -0.975   -9.5356]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.51    -1.     -25.1979]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.115   -0.975  -17.2961]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.555   -0.86    -8.9419]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -64.71623487568509, time: 126.786
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.435   -0.965  -10.0253]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.205   -1.     -24.9133]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.31   -0.985 -17.184]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.575   -0.915  -10.9817]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -58.721374261115734, time: 124.465
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.83    -0.94    -8.7908]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.475   -1.     -25.1253]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.19    -0.77   -10.1206]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.97    -0.94   -11.1973]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -64.36282770551519, time: 126.441
[-12.61    -0.855   -9.2748]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.935   -0.965  -10.9682]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.71    -0.985  -12.9521]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-32.345  -0.995 -18.645]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -67.82692696414482, time: 129.515
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.615   -0.825  -10.0262]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.3     -0.97   -12.3191]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.35   -0.97  -15.418]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.98    -1.     -18.3219]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -71.76491481991046, time: 128.129
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.46    -0.83    -9.7205]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.115   -0.97   -11.6149]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.415   -1.     -15.0703]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.345   -1.     -21.2436]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -68.43708148862851, time: 129.856
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.25    -0.815  -10.7196]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.28    -0.98   -12.5857]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.865   -0.99   -12.9109]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-35.96   -1.    -20.077]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -64.05830633747667, time: 127.78
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.2     -0.77    -9.0821]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.09    -0.98   -12.0913]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.21    -0.94   -11.7702]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.28    -1.     -17.3645]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -74.63232004079066, time: 128.911
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.16    -0.805   -9.6815]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.35    -0.975  -11.6966]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.6     -0.955  -12.9495]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.08   -1.    -20.812]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -62.764278865656, time: 128.101
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.335   -0.77    -9.5703]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.      -0.945  -10.6568]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.63    -0.99   -13.0646]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.165   -1.     -20.8445]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -71.46048901871596, time: 127.462
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.965   -0.81   -11.0577]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.4     -0.96   -10.8532]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.035   -0.98   -13.1013]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.35    -1.     -20.3196]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -73.15534534701271, time: 129.12
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.64    -0.775   -9.4708]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.57    -0.965  -11.4263]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.845   -0.98   -12.6038]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-37.16    -1.     -20.4471]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -64.75328546138203, time: 128.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.59    -0.825  -10.8847]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.485  -0.96  -12.431]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.575   -0.97   -11.3647]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.345   -1.     -21.8543]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -66.7283421064379, time: 128.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.545   -0.84   -10.8502]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.285   -0.98   -13.2924]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.875   -0.985  -12.1402]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.28    -1.     -17.3975]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -68.04819509887373, time: 129.045
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.91    -0.805  -10.6008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.495   -0.995  -13.0461]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.015  -0.965 -11.53 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.695  -1.    -19.223]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -62.879757065822595, time: 130.687
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.12    -0.825  -10.7219]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.285   -0.945  -10.3828]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.11    -0.965  -11.0246]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.6     -0.995  -17.3605]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -62.63868048609278, time: 130.738
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.095   -0.89   -11.5456]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.51    -0.98   -11.9205]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.665   -0.995  -12.5499]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.41    -0.99   -14.8638]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -73.42983150907362, time: 129.944
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.6     -0.89   -12.0336]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.435   -0.97   -12.7398]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.86    -0.955  -10.8772]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.83    -0.985  -16.7073]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -68.283264405027, time: 127.366
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.74    -0.905  -10.9238]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.025   -0.965  -11.1375]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.63    -0.94   -11.2348]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-30.585   -0.95   -17.8684]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -68.71459928697703, time: 130.012
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.855   -0.905  -12.0794]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.12  -0.96 -11.48]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.555   -0.97   -11.3408]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.83    -0.8    -13.9267]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -71.7315246369261, time: 127.328
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.75    -0.865  -11.7779]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.075   -0.99   -12.4009]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.98    -0.89   -10.5411]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.72    -0.93   -14.4244]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -64.2521901301272, time: 127.878
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.39   -0.88  -10.743]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.47    -0.97   -11.3661]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.9     -0.95   -10.9778]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.045   -0.925  -14.0018]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -64.32745337864387, time: 130.391agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.97    -0.865  -11.8001]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.32   -0.86   -7.8071]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -69.04236613824938, time: 129.714
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.28    -0.99   -11.6274]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.255   -0.91    -9.7792]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.515   -0.92   -11.8166]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.065   -0.87    -8.2861]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -73.30986967517707, time: 129.366
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.74    -0.935  -10.1016]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.365  -0.88  -10.863]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.625   -0.95   -12.7961]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.745 -0.82  -7.929]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -66.63056997522425, time: 128.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.115   -0.96   -10.8223]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.35    -0.89   -10.1807]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.235   -0.99   -11.5394]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.42   -0.875  -7.2159]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -71.96673827097199, time: 130.647
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.9     -0.965  -10.9579]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.93    -0.93    -9.6241]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.51   -0.98  -11.673]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.49   -0.865  -7.2075]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -71.91240393090276, time: 130.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.175   -0.955  -10.3535]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.32    -0.965   -9.7704]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.81    -0.945  -11.3048]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.98   -0.865  -7.5757]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -74.82685854688837, time: 128.562
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.83    -0.96   -10.3778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.635   -0.855   -9.3021]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.33    -0.98   -12.8306]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.305   -0.905   -8.5953]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -71.0805766613025, time: 129.024
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.27    -0.96    -9.6111]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.38   -0.855  -7.5492]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.84    -0.995  -10.6303]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.7    -0.885  -8.1078]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -65.6532271687978, time: 131.516
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.86    -0.995  -10.6978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.105   -0.9    -10.1366]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.02    -0.995  -11.4509]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.3    -0.86   -7.1082]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -74.71901288228455, time: 130.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.125   -0.935  -10.2643]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.395   -0.9    -10.4666]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.895   -1.     -11.6376]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.515 -0.915 -7.875]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -69.94522077841721, time: 127.917
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.46    -0.99   -11.8019]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.37    -0.89   -10.2696]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.52    -1.     -12.7505]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.315  -0.885  -8.538]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -70.14045499043381, time: 130.106
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.435   -0.975  -10.4505]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.8     -0.935   -9.3556]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.88    -1.     -11.3137]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.46   -0.915  -8.0215]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -72.94674008388843, time: 131.208
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.065   -0.96   -10.6035]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.785   -0.95   -10.9405]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.505  -1.    -11.804]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.73   -0.94   -8.2677]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -68.16333562948621, time: 129.175
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.315   -0.94   -10.7245]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.435  -0.935  -9.501]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.135  -0.995 -12.011]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.135 -0.945 -7.738]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -64.27690233932297, time: 128.706
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.42    -0.94   -10.0224]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.03    -0.89    -9.7354]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.375   -0.96   -10.4301]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.05   -0.9    -7.6194]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -67.72362219834218, time: 129.82
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.245   -0.985  -10.6977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.19    -0.88    -9.3479]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.17    -0.98   -11.7513]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.315  -0.925  -7.8282]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -71.24179548521626, time: 128.456
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.86    -0.925  -11.5742]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.81    -0.875  -10.0067]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.365   -1.     -12.5229]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.955  -0.9    -8.3714]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -67.33889727239777, time: 128.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.615   -0.975  -11.6865]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.865  -0.875 -10.896]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.025   -0.985  -12.3174]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.99   -0.925  -8.2217]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -66.45453714891602, time: 127.946
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.855   -0.97   -13.1456]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.92    -0.945  -10.0771]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.955  -1.    -11.833]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.99   -0.865  -7.6632]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -67.08098345638149, time: 130.211
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.445   -0.955  -11.5227]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.32    -0.935   -9.0908]
[-16.205   -0.92   -10.9508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.86    -0.975  -13.4653]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.495   -0.94    -9.3899]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.685   -0.895   -8.9445]
30600 50
steps: 1529950, episodes: 30600, mean episode reward: -76.2979225584948, time: 128.698
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.88    -0.915  -11.1877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.92    -0.985  -14.0128]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.065   -0.915   -9.1152]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.595   -0.91    -9.4883]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -67.76218375879354, time: 128.939
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.445   -0.875   -9.2631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.615   -1.     -14.6653]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.045   -0.985   -9.2517]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.96    -0.94    -8.7029]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -62.66063769535574, time: 129.129
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.185   -0.915  -11.9519]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.995   -1.     -13.2105]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.035  -0.825  -6.8443]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.205   -0.915   -7.9932]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -68.84522899307149, time: 129.902
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.93    -0.86    -9.6842]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.52    -0.98   -12.1581]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.72   -0.96   -8.3279]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.385   -0.95    -9.2591]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -72.84834157021818, time: 129.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.3     -0.96   -10.9517]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.555   -0.98   -11.9969]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.72   -0.895  -8.0855]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.29    -0.965   -9.5624]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -71.9833274004409, time: 129.057
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.34    -0.955  -11.3879]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.835   -0.98   -12.3672]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.035  -0.79   -7.6104]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.22    -0.935   -9.9064]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -66.75776982045338, time: 129.796
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.225   -0.905  -12.7933]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.43    -0.99   -11.4034]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.1    -0.84   -7.6653]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.395  -0.89   -9.356]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -69.9085787904929, time: 130.556
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.825   -0.905  -11.4054]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.66    -0.99   -11.8455]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.47    -0.895   -8.6313]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.935   -0.83    -8.7978]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -77.55130848876108, time: 128.963
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.3     -0.91   -10.7108]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.775   -0.975  -11.9193]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.45   -0.835  -7.7309]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.17    -0.855   -9.0955]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -65.94327665257035, time: 128.878
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.935   -0.9    -11.1216]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.455  -0.975 -10.784]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.235   -0.835   -8.3578]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.015   -0.885   -7.7895]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -71.1322574590785, time: 130.358
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.13    -0.835   -8.5934]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.08    -0.995  -11.7156]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.825   -0.915   -9.0551]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.35    -0.795   -7.6746]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -72.43224337798992, time: 130.025
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.01   -0.805  -8.174]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.4     -0.995  -11.4991]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.72   -0.77   -7.2299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.01   -0.75   -6.8675]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -68.31781558324697, time: 129.057
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.41    -0.9     -8.6862]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.97    -0.935  -11.2033]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.89   -0.86   -8.1299]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.28   -0.81   -6.4544]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -68.01121756870909, time: 130.094
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.095   -0.87    -9.1808]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.915   -1.     -10.7586]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.46    -0.955   -8.6701]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.92    -0.915   -8.3824]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -62.26608479872237, time: 130.058
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.15   -0.89   -7.3541]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.26    -0.975  -10.4255]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.43   -0.9    -7.9537]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.795   -0.89    -8.7606]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -72.52161312733932, time: 129.172
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.35    -0.92    -9.7043]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.8     -0.995  -11.7003]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.625   -0.93    -8.9322]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.85    -0.835   -9.4775]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -69.14798728297262, time: 127.732
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.025   -0.945   -8.6804]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.565   -0.985  -11.3088]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.34    -0.98    -9.4275]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.985   -0.88    -9.4749]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -72.92067237622481, time: 130.271
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.46    -0.915   -9.7272]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.805   -1.     -11.0054]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.955   -0.91    -9.5488]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.495   -0.945   -9.7659]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -72.81794454488946, time: 130.679
[-9.665  -0.755  -6.2083]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.865  -0.82   -6.9822]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.125   -0.94   -10.1577]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -79.48262839516106, time: 127.641
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.61   -0.92   -7.8472]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.86    -0.8     -7.3762]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.205  -0.715  -4.9399]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.275   -0.925  -11.9858]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -80.66528488893684, time: 128.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.295  -0.89   -7.3769]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.945   -0.79    -9.5203]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.33   -0.53   -5.1254]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.795   -0.935  -17.7772]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -82.09051303699808, time: 127.516
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.655   -0.885   -8.4313]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.745   -0.73    -7.8421]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.87   -0.59   -6.5115]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.25   -0.96  -16.487]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -72.90804128890628, time: 127.582
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.02   -0.89   -7.5262]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.7     -0.805   -7.7065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.565  -0.72   -6.5085]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.71    -0.865  -11.5601]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -74.76903080497065, time: 128.432
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.62    -0.915   -8.5603]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.99    -0.815   -9.3897]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.795   -0.785  -10.1032]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.565  -0.96  -11.634]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -80.10457149204831, time: 128.151
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.095   -0.965   -9.3871]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.35    -0.785   -9.3662]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.075  -0.675  -6.2417]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.535   -0.94   -13.1288]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -91.05962563397233, time: 128.476
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.6     -0.945   -9.0476]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.73    -0.865  -10.6485]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.     -0.655  -6.3902]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.845   -0.96   -12.7416]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -75.48468489379133, time: 130.363
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.975   -0.95    -9.6413]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.25    -0.85    -9.7334]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.875   -0.815   -9.3945]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.025   -0.925  -11.7466]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -77.43583703935988, time: 129.651
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.82    -0.89    -7.9551]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.695   -0.83   -10.3683]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.44    -0.735   -7.5358]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.78    -0.965  -12.6119]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -88.09170195293696, time: 128.079
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.605   -0.92    -9.2186]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.805   -0.775   -8.3747]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.53    -0.695   -8.6492]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.805   -0.895   -9.8842]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -75.72826440440583, time: 129.997
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.14    -0.965  -10.0208]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.455   -0.895  -10.7074]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.5   -0.68  -7.71]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.54   -0.635  -7.1358]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -84.03737603787718, time: 127.561
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.265   -0.925   -9.5774]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.905  -0.83  -12.635]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.01   -0.755  -8.516]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.31    -0.825   -8.0029]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -91.45709746323796, time: 131.954
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.84    -0.96    -9.7331]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.745   -0.765   -7.7588]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.98    -0.725  -10.3785]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.235  -0.635  -5.5194]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -76.38893053964647, time: 127.562
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.305  -0.96   -7.6595]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.73    -0.855   -9.4254]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.35    -0.755  -11.2079]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.06    -0.82    -8.4855]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -93.18016543639314, time: 127.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.68    -0.945   -9.7556]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.605   -0.895  -11.1669]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.065   -0.735   -9.1037]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.505   -0.835   -8.6386]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -76.20129328242871, time: 125.525
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.285   -0.87    -9.3877]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.685   -0.85    -8.4279]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.96    -0.67    -7.9454]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.36    -0.935  -11.1739]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -80.94710317696045, time: 127.413
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.51    -0.96    -9.9533]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.4     -0.865   -8.8529]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.575   -0.765  -12.0885]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.085   -0.955  -10.8257]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -73.38144648570214, time: 129.941
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.525   -0.94    -8.6384]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.925  -0.78   -6.9085]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.88    -0.68   -10.3127]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.68    -0.94    -9.4898]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -78.93819907799069, time: 126.763
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.06    -0.925   -8.5524]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.735   -0.92    -8.2154]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.69   -0.625  -6.7301]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.525   -0.86    -7.9125]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -91.33292743042792, time: 130.733
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.14    -0.85   -12.2423]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.245  -0.88   -7.3194]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.63   -0.4    -4.2643]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.375   -0.865   -8.3862]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -80.25938574645112, time: 129.324
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.165  -0.995 -11.226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.555  -0.93   -7.0589]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.93   -0.72   -7.2954]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.825  -0.94   -8.947]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -82.85018715628983, time: 129.266
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.8     -0.995  -10.7495]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.805   -0.915   -8.2758]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.415  -0.7    -6.8321]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.515   -0.92    -9.8606]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -73.75956796044683, time: 129.573
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.85   -0.985 -12.184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.96   -0.89   -7.3191]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.315  -0.785  -6.8829]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.67  -0.89  -7.491]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -76.43162979820636, time: 129.782
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.895   -0.88   -10.1322]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-7.435  -0.9    -6.2437]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.205  -0.795  -6.8776]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.22    -0.91    -8.1678]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -73.8521835726578, time: 129.558
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.83    -0.77   -11.4079]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.41    -0.94    -9.7452]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.105  -0.785  -5.9311]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.59    -0.95    -9.0615]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -74.67766046990832, time: 131.556
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.995   -0.875  -11.4785]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.29   -0.93   -8.394]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.87   -0.61   -4.9396]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.48    -0.935   -8.3255]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -83.7517609628057, time: 129.925
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.84    -0.805  -12.5909]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.585   -0.91    -8.2708]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.45   -0.575  -4.5623]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.255   -0.91    -8.6055]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -73.8877338016327, time: 128.828
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.745   -0.95   -14.3394]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.155   -0.965   -9.3361]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.21   -0.61   -5.1603]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.11    -0.915   -9.2203]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -76.90271988419116, time: 129.676
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.85    -0.885  -11.7219]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.94    -0.96    -9.9792]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.26   -0.515  -4.2683]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.05    -0.915   -9.1921]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -76.8771153808853, time: 131.242
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.345   -0.98   -11.8873]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.075   -0.965  -10.1626]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.47   -0.555  -4.4848]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.135   -0.835   -8.0671]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -77.20110380653126, time: 129.325
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.47    -0.995  -11.6995]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.86    -0.955  -10.0849]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.66   -0.46   -3.0157]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.83   -0.89   -8.969]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -81.78856417307895, time: 129.979
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.14    -0.995  -12.4539]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.02   -0.92   -6.9053]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.005  -0.485  -4.3498]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.33    -0.89    -8.7699]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -84.67215419704175, time: 130.643
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.58    -0.99   -11.1273]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.21    -0.935   -9.2318]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.365  -0.425  -3.4499]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.975   -0.915   -9.6761]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -75.09622754136952, time: 128.89
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.995  -1.    -13.486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.09    -0.975   -9.3793]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.775  -0.45   -3.6398]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.16   -0.94   -9.217]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -82.0753569507405, time: 127.727
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.405  -0.995 -15.059]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.365   -0.965   -8.9392]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.02   -0.405  -3.1871]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.62    -0.915   -8.3886]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -75.56842833557954, time: 129.144
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.605   -0.985  -11.2045]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.325   -0.93    -9.6289]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-3.695  -0.33   -2.7762]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.455   -0.885   -8.2632]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -72.42585480659226, time: 129.597
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.455   -0.965  -12.5846]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.495   -0.95    -8.3793]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.28   -0.46   -4.1417]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.2     -0.96    -8.9286]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -76.53607743330566, time: 128.776
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.605   -0.955  -10.5517]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.895   -0.87   -11.1353]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.765 -0.91  -6.664]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.12   -0.57   -4.6091]
30800 50
steps: 1539950, episodes: 30800, mean episode reward: -73.37113625788503, time: 129.677
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.73    -0.985  -13.1382]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.62    -0.985  -10.6985]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.545  -0.945  -8.0474]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.375  -0.545  -4.7829]
31000 50
steps: 1549950, episodes: 31000, mean episode reward: -78.339603562533, time: 129.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.185   -0.905  -14.3987]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.82    -0.955  -11.0759]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.085  -0.94   -7.5796]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.09   -0.665  -4.7701]
31200 50
steps: 1559950, episodes: 31200, mean episode reward: -81.25413492824957, time: 130.038
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.96    -0.98   -12.7114]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.555   -0.915  -10.1556]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.325 -0.89  -7.165]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.56   -0.62   -5.9832]
31400 50
steps: 1569950, episodes: 31400, mean episode reward: -78.20210252944656, time: 130.396
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.31   -0.925 -14.105]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.995   -0.9    -11.4713]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.605  -0.92   -7.3309]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.805  -0.625  -4.7916]
31600 50
steps: 1579950, episodes: 31600, mean episode reward: -84.54045900215073, time: 129.885
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.11    -0.96   -15.5604]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.26    -0.9    -10.4291]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.3     -0.94    -8.7259]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.22   -0.745  -6.9584]
31800 50
steps: 1589950, episodes: 31800, mean episode reward: -96.22051687367852, time: 129.608
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.455   -0.96   -13.2819]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.995   -0.855  -10.0776]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.09    -0.92    -8.0873]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.2    -0.655  -5.5452]
32000 50
steps: 1599950, episodes: 32000, mean episode reward: -76.43811926806806, time: 130.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.225   -0.965  -13.6224]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.585   -0.885  -10.0505]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.175   -0.945   -8.4116]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-4.84   -0.63   -4.1709]
32200 50
steps: 1609950, episodes: 32200, mean episode reward: -91.88152870592123, time: 130.875
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.54    -0.855  -12.7179]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.43    -0.97   -11.8263]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.88   -0.9    -8.0519]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.62   -0.71   -6.0502]
32400 50
steps: 1619950, episodes: 32400, mean episode reward: -84.40317426881484, time: 129.092
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.39    -0.91   -14.9113]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.455   -0.895   -9.2722]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.275 -0.93  -7.743]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-8.24   -0.665  -6.4295]
32600 50
steps: 1629950, episodes: 32600, mean episode reward: -98.93711593331868, time: 132.095
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.635  -0.905 -15.526]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.06    -0.695   -7.4948]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.175  -0.84   -6.9428]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.     -0.735  -7.687]
32800 50
steps: 1639950, episodes: 32800, mean episode reward: -95.01386760058736, time: 130.44
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.335   -0.955  -15.8477]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.1     -0.91   -11.3225]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.615  -0.88   -7.1599]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.98   -0.61   -6.0341]
33000 50
steps: 1649950, episodes: 33000, mean episode reward: -78.63846983953931, time: 130.466
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.67    -0.95   -16.9707]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.81    -0.865   -9.4006]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.7    -0.92   -7.3474]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.375   -0.8     -8.1643]
33200 50
steps: 1659950, episodes: 33200, mean episode reward: -73.07938254465563, time: 129.582
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.935   -0.99   -18.0931]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.09    -0.915  -10.3728]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.145  -0.935  -6.9036]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-7.995  -0.785  -6.7726]
33400 50
steps: 1669950, episodes: 33400, mean episode reward: -83.68798578259346, time: 131.345
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-30.31    -0.97   -17.8249]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.3     -0.88   -11.4901]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.8    -0.975  -7.5247]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.965   -0.7     -8.1089]
33600 50
steps: 1679950, episodes: 33600, mean episode reward: -73.26220802446383, time: 130.131
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.745   -0.985  -15.7161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.935   -0.92   -13.9455]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.815  -0.965  -8.1173]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-6.465  -0.625  -5.2448]
33800 50
steps: 1689950, episodes: 33800, mean episode reward: -78.97887014256206, time: 130.695
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.955   -0.935  -16.3184]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.47    -0.885  -10.5338]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.41   -0.95   -7.6149]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.58   -0.695  -7.4802]
34000 50
steps: 1699950, episodes: 34000, mean episode reward: -76.21634694503089, time: 130.523
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.715  -0.945 -16.626]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.415   -0.885  -11.5762]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.64    -0.925   -8.3954]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.65   -0.61   -6.8711]
34200 50
steps: 1709950, episodes: 34200, mean episode reward: -80.9689678847637, time: 131.63
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.285   -1.     -17.9569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.43    -0.83    -9.9736]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.61    -0.955   -9.3931]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.615   -0.705   -9.3188]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -84.27481167421571, time: 130.545
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-32.88    -0.99   -18.8346]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.565   -0.81   -10.5257]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.52  -0.675 -5.952]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.945  -0.895 -13.517]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.25    -0.97   -13.0332]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-38.685   -0.835  -19.3693]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -75.57562496413722, time: 126.325
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.15   -0.7    -6.2977]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.83    -0.965  -15.3896]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.125   -0.98   -14.5692]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-36.695   -0.885  -18.8927]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -71.64037962920297, time: 126.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.605  -0.675  -5.3111]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.825   -0.93   -13.4084]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.03    -0.945  -13.4821]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-33.135   -0.88   -17.7783]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -65.52853848866606, time: 125.566
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.155  -0.745  -6.5352]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.67    -0.925  -12.9982]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.145   -0.95   -11.8711]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.08    -0.845  -15.1104]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -71.96690415638555, time: 125.924
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.325  -0.73   -6.8096]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.98    -0.945  -14.0018]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.145   -0.96   -12.6111]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.835   -0.91   -16.1354]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -67.18815313148494, time: 126.285
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.705  -0.69   -6.1964]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.535   -0.945  -13.1065]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.975   -0.945  -12.1115]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.305   -0.9    -14.9861]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -64.373151922001, time: 125.532
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.24    -0.79    -7.3922]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.67    -0.93   -13.3923]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.3     -0.985  -16.4124]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.855   -0.875  -14.6153]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -63.963815455554304, time: 125.859
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.695  -0.705  -5.3631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.35    -0.915  -12.9735]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.135   -0.935  -12.1922]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.355   -0.895  -13.9768]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -74.08644194344375, time: 124.892
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.825  -0.685  -5.7108]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.005   -0.915  -16.5313]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.46    -0.95   -11.6809]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.955   -0.865  -14.3533]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -65.00266473364877, time: 126.71
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.865  -0.765  -6.5182]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.085   -0.915  -14.4191]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.28   -0.965 -11.944]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.95    -0.88   -14.8222]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -74.49345757796486, time: 127.358
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.93    -0.755   -7.4853]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.255   -0.945  -12.7816]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.43    -0.985  -13.8105]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.64    -0.87   -15.3808]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -61.33227870365554, time: 126.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.29    -0.765   -8.1007]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.67   -0.965 -13.155]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.255   -0.995  -13.3042]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.61    -0.85   -14.3784]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -70.96018053129224, time: 125.159
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.68    -0.71    -7.1979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.315   -0.98   -15.4936]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.37    -0.99   -13.6631]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.13    -0.945  -14.5851]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -74.58693787924152, time: 127.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.43    -0.77    -7.3602]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.59   -0.955 -13.355]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.015   -0.99   -14.0734]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.165   -0.89   -15.4239]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -65.19041852009448, time: 127.746
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.775   -0.69    -8.5139]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.075   -0.95   -14.7472]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.01    -0.975  -11.8125]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.855   -0.89   -13.8802]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -65.11566680551213, time: 128.69
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.12    -0.74    -8.3112]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.52   -0.96  -13.101]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.235   -0.99   -13.3244]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.69    -0.93   -15.4001]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -62.35368526912376, time: 127.113
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.71    -0.835   -8.3376]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.055   -0.945  -12.4818]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.91    -0.97   -12.3036]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.625   -0.915  -15.3861]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -70.52054683341878, time: 127.812
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.32    -0.8     -8.8997]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.985   -0.96   -14.1022]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.805   -0.99   -12.5477]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.895   -0.875  -14.7325]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -70.65401544221933, time: 126.524
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.63    -0.77    -7.9662]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.875   -0.95   -15.9458]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.27    -0.985  -12.8406]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.65    -0.915  -15.2873]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -70.94566218472545, time: 127.131
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.965   -0.94   -11.5982]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -68.90025824357639, time: 126.781
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.74    -0.91   -10.1609]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.08    -0.995  -16.5687]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.165   -0.97   -12.5391]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.3     -0.97   -11.4629]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -56.18727441901903, time: 127.895
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.145   -0.945   -8.9902]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.115  -0.995 -15.713]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.64    -1.     -11.7743]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.57    -0.975  -11.2254]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -62.747508812711075, time: 127.314
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.475  -0.95  -12.448]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.62    -1.     -18.2976]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.66    -0.985  -11.8068]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.305   -0.97   -10.8028]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -61.09263577476524, time: 126.645
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.135  -0.91  -11.241]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.755   -0.995  -18.2548]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.84    -0.995  -12.0935]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.525   -0.975  -11.1058]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -68.78660691572922, time: 128.455
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.035   -0.965  -10.4926]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.44    -1.     -19.2494]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.385   -0.99   -12.0575]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.59   -0.99  -12.301]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -68.52589645683074, time: 127.174
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.27    -0.975  -11.8516]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.06    -0.99   -18.3139]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.18    -0.985  -12.0625]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.94    -0.97   -11.4536]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -56.667908248421064, time: 127.793
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.035   -0.925   -9.7844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.935   -1.     -17.1392]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.105   -0.98   -11.9378]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.265   -0.995  -10.6371]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -62.54732185054435, time: 128.257
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.14   -0.935  -9.198]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.88    -1.     -19.2166]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.325   -0.99   -11.3742]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.15    -0.995  -11.0594]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -69.27565131500677, time: 127.703
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.335   -0.965  -10.1204]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.975   -1.     -17.4832]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.1     -0.99   -11.9303]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.065   -0.965  -12.1441]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -65.82596925241224, time: 128.898
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.97    -0.96   -10.8025]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.675   -1.     -18.1268]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.525   -0.985  -10.5339]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.555   -0.975  -12.1789]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -59.53106318416858, time: 128.211
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.81    -0.985  -11.3883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.49    -0.995  -19.3027]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.705   -0.97   -11.1625]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.565  -0.985 -11.466]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -66.60597776064684, time: 128.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.875   -0.95   -10.1616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.135   -0.98   -16.8243]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.885   -0.99   -10.9179]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.175  -0.975 -11.009]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -67.47726679100914, time: 127.486
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.895   -0.97   -11.3244]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.13    -0.995  -16.5196]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.245   -0.985  -11.2712]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.63   -0.99  -12.008]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -60.21023416268583, time: 129.598
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.595   -0.96   -11.3497]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.325   -1.     -18.3627]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.935   -0.965  -10.7925]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.765   -0.985  -11.0678]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -60.968985360122495, time: 130.453
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.475   -0.93    -8.5049]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.64    -1.     -20.1507]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.14    -0.995  -10.5198]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.515   -0.99   -10.9513]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -62.63728273746288, time: 129.137
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.27    -0.965   -9.4684]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.025   -0.97   -16.1473]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.56    -1.     -11.3158]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.785  -0.975 -11.539]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -57.90025091693224, time: 127.373
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.33    -0.995   -9.5504]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.725   -0.99   -15.7495]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.805   -0.995  -10.9292]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.04    -0.98   -10.4952]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -60.2504226765113, time: 128.322
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.535   -0.98    -8.4486]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.395   -0.94   -13.3889]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.87    -0.99   -10.9951]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.05    -0.98   -11.9197]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -57.710451954781746, time: 129.106
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.025   -0.975   -8.5755]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.025   -0.97   -11.2549]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.185   -0.97    -9.8285]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.26   -1.    -24.999]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.925   -0.92   -13.0991]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.35    -0.93    -9.9872]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -66.65471574134274, time: 126.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.125   -0.98   -11.1193]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.89    -1.     -24.9258]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.43    -0.765  -11.5909]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.895   -0.92   -10.7456]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -66.01360864727347, time: 126.5
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.235   -0.96    -9.3023]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.445   -1.     -25.1524]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.015  -0.815 -10.486]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.505   -0.945   -9.9917]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -62.010704769604274, time: 124.54
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.895   -0.975  -10.4546]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.365   -1.     -25.0184]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.04    -0.925  -11.6338]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.68    -0.975  -10.1996]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -66.14217439697507, time: 126.651
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.945  -0.975 -10.33 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.475   -0.995  -25.1175]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.39    -0.695   -8.6868]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.375   -0.94   -10.8584]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -67.25767395594285, time: 125.474
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.73    -0.97   -10.1698]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.61    -1.     -25.1866]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.11    -0.84    -9.4751]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.75    -0.82    -8.7967]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -66.59296343196276, time: 125.365
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.175   -0.975  -10.3256]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.695   -1.     -25.2655]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.035   -0.8     -9.3072]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.845  -0.87  -10.438]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -70.2704970376441, time: 125.35
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.47    -0.985  -10.6681]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.71    -0.995  -25.2899]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.67    -0.945  -12.7288]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.64   -0.915 -11.084]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -58.68437384018602, time: 124.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.185   -0.975  -10.4661]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.995   -1.     -24.8223]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.29    -0.97   -10.2864]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.715   -0.93    -9.8444]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -66.78195936919973, time: 126.635
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.215   -0.985  -10.2659]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.72    -1.     -25.2932]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.91    -0.84   -10.3826]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.57    -0.91   -10.2078]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -62.80517834501292, time: 125.179
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.04   -0.985 -10.5  ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.495   -1.     -25.1133]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.23   -0.895  -9.677]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.275   -0.91   -10.0769]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -63.40574428773695, time: 124.19
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.325   -0.99   -10.6039]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.505   -1.     -25.1047]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.59  -0.94 -12.53]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.865   -0.9    -10.4738]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -59.52473767618713, time: 125.421
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.825   -0.98    -9.5018]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.75   -1.    -25.325]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.3     -0.83    -9.8628]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.195   -0.845   -9.9048]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -74.39176505693052, time: 127.438
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.095   -0.975  -10.0048]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.42    -1.     -25.0699]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.89    -0.64    -7.7493]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.85    -0.82    -9.2523]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -64.7164188970729, time: 126.432
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.12   -0.98  -10.381]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.85    -1.     -24.7188]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.54    -0.77   -12.4182]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.605   -0.92   -10.6841]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -63.3861235514409, time: 129.493
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.215   -0.99    -9.8226]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.01    -1.     -24.0597]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.74   -0.87  -12.903]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.705   -0.79    -9.0911]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -60.21913298171028, time: 126.065
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.22    -0.995  -11.3713]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.535   -1.     -25.1455]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.465   -0.895   -8.9392]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.045   -0.92   -10.3937]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -61.338622966201335, time: 124.418
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.905   -0.99    -9.6616]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.515   -1.     -25.1539]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.31    -0.88   -10.6065]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.      -0.98   -10.4978]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -60.787294620738614, time: 126.259
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.475   -0.98   -10.4253]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.475   -1.     -25.1275]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.42    -0.815   -8.3272]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.54    -0.97   -10.9921]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -90.46482232816166, time: 128.062
[-14.415   -0.78    -9.0781]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-31.6     -0.9    -17.1198]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -69.06063724719168, time: 127.893
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.985  -0.755  -5.7115]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.81    -0.895  -21.3666]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.17    -0.715   -7.7554]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-28.565   -0.95   -15.8595]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -76.96403641144722, time: 128.822
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.565  -0.75   -5.9606]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.785   -0.93   -18.7028]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.06    -0.87    -9.2776]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.52    -0.765  -15.1718]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -72.89374993334677, time: 127.874
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.145  -0.815  -6.8817]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.9     -0.925  -18.6336]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.125   -0.81    -7.6275]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.89    -0.74   -12.7994]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -75.63454709751537, time: 127.427
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.82   -0.87   -7.1553]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.08    -0.975  -18.6554]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.68    -0.795   -8.2045]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.57    -0.885  -15.4023]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -71.27906789124381, time: 127.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.9    -0.855  -6.5192]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.72   -0.96  -18.888]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.475   -0.825   -9.6701]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.9     -0.86   -12.9227]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -71.65260990705268, time: 128.305
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.04   -0.845  -6.7605]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.04    -0.965  -18.9624]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.455   -0.755   -8.6833]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.475   -0.745  -11.8274]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -80.21264382305517, time: 127.248
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.415   -0.845   -7.1575]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.925   -0.915  -20.5337]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.74    -0.74    -8.2382]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.63    -0.78   -13.6937]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -68.73173767103953, time: 126.048
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.875  -0.91   -7.0085]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.095   -0.945  -19.8933]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.01    -0.58    -7.1969]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-29.905   -0.85   -15.7848]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -72.28331493954681, time: 128.516
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.66    -0.87    -8.3502]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.94   -0.98  -21.307]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.84   -0.535  -5.2915]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.195   -0.825  -14.7059]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -74.50385921819155, time: 128.452
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.795   -0.895   -7.9984]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.7     -0.975  -19.7459]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.575   -0.61    -7.4847]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.255   -0.87   -13.6203]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -79.35108757769052, time: 126.041
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.69   -0.85   -6.6463]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-37.08    -0.985  -19.9383]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.97    -0.61    -7.8099]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.83    -0.875  -12.3745]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -67.85247755961447, time: 129.106
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.76  -0.795 -6.88 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.27    -0.97   -18.1309]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.545  -0.6    -6.6405]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.62    -0.825  -12.1933]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -69.5736682500364, time: 128.668
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.495  -0.87   -7.498]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.16    -0.95   -14.8301]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.785  -0.465  -4.6485]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.695   -0.875  -11.5373]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -83.25362227085908, time: 128.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.925   -0.855   -8.5734]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.525   -0.965  -18.9995]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.38    -0.52    -6.5055]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.3     -0.88   -11.4461]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -73.11207238921236, time: 130.967
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.03   -0.885  -7.1215]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.24   -0.93  -17.411]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.47    -0.645   -7.5518]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.265   -0.83   -11.3411]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -80.15646108609222, time: 128.767
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.79   -0.865  -6.6985]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.15    -0.955  -16.6823]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.175   -0.65    -6.8385]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.815  -0.83  -12.67 ]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -79.38263312290135, time: 125.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-6.3    -0.825  -5.3027]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-31.725   -0.96   -17.8879]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.785   -0.68    -7.5757]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.785   -0.855  -13.4488]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -75.92838970516908, time: 128.188
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.06  -0.91  -7.098]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.16    -0.94   -15.2244]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.5     -0.645   -7.5702]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.63    -0.805  -12.7758]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -77.75744538865847, time: 130.017
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.795 -0.95  -7.57 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.495   -0.915  -17.9709]
agent2_energy_min, agent2_energy_max, agent2_energy_avgagent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.565   -0.945  -14.3443]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.31    -0.995  -22.7321]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -57.714397620210576, time: 129.476
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-26.49    -0.98   -14.8151]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.495   -0.98   -21.3382]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.125   -0.86   -17.2174]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.84    -0.95   -22.2172]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -54.41365152270442, time: 129.42
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.01    -0.98   -16.0844]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.34    -0.975  -24.0389]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.1     -0.915  -17.6398]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.635   -0.965  -23.1607]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -56.300415583028496, time: 127.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.195   -0.95   -14.2379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.535   -0.985  -23.1545]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.665   -0.92   -15.0172]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.605   -0.96   -22.4991]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -53.35645510897039, time: 129.673
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-27.46   -0.96  -15.354]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.79    -0.995  -23.7484]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.78    -0.945  -15.8582]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.535   -0.97   -21.9357]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -48.63140294200527, time: 129.333
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.28    -0.96   -14.1104]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.855   -0.965  -22.8147]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.535   -0.89   -18.9558]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-40.83    -0.995  -21.3409]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -47.90240658462118, time: 128.023
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.725   -0.96   -13.2123]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.51    -0.98   -23.5972]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.845   -0.93   -14.6217]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.655  -0.99  -21.573]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -65.0670639408011, time: 129.565
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.655   -0.94   -12.6379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.54    -0.995  -22.8006]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.705   -0.96   -15.5813]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.98    -0.97   -20.9798]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -51.77322479447373, time: 128.607
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.595   -0.95   -12.6667]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.955   -0.99   -22.4108]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.53    -0.945  -17.9242]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.385   -0.98   -21.8015]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -55.111921264462225, time: 130.713
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-25.725   -0.945  -14.4208]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-45.9     -0.99   -23.3913]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.215   -0.965  -13.9957]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-41.265   -1.     -21.7859]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -51.11179598196739, time: 130.251
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.945   -0.98   -13.6462]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.275   -0.985  -22.7896]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-31.93    -0.975  -17.7595]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-39.1     -1.     -20.3966]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -54.71656929968342, time: 129.169
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.55    -0.965  -12.4403]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.245   -0.99   -23.4746]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.645   -0.98   -16.3125]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.66    -1.     -22.0752]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -57.02059956017398, time: 129.772
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.365   -0.95   -13.8557]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.385  -0.995 -23.691]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.5     -0.945  -16.7395]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.84    -0.995  -22.2309]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -50.996410051699115, time: 131.054
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.555   -0.95   -14.3099]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.74    -0.96   -21.2826]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.65   -0.955 -18.332]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.625  -0.99  -22.793]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -56.900265162032866, time: 129.17
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.025  -0.96  -13.361]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.905   -0.99   -22.9665]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-34.725   -0.92   -18.8865]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.725   -0.955  -23.2553]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -48.21237680038881, time: 131.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-24.465   -0.945  -14.1035]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.525   -0.99   -21.1497]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-30.42    -0.95   -17.1531]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.925   -0.97   -23.7064]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -53.85128757687991, time: 128.986
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-28.445   -0.975  -16.0038]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.175   -1.     -22.5608]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.185   -0.885  -16.6685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.765   -0.97   -23.6647]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -54.08086531230181, time: 130.182
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.     -0.98  -14.311]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.205   -0.995  -22.9086]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-32.235   -0.89   -17.5974]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.73    -1.     -23.5664]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -65.64899842731589, time: 129.664
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.695   -0.945  -13.2854]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.155   -0.995  -22.0317]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.66    -0.95   -13.6201]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.555   -0.995  -24.1198]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -51.87089893426227, time: 132.992
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.74    -0.945  -13.2001]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.575   -0.89   -11.5954]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.555  -0.955 -11.008]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.72    -0.92   -10.2163]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.41    -0.96   -15.7208]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -66.99220222127907, time: 128.757
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.065   -0.89   -11.3784]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.105   -0.97   -12.9515]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.025   -0.91   -11.2796]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-5.945  -0.44   -3.8178]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -66.55169595147792, time: 128.809
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.725   -0.895  -10.5784]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.705  -0.99  -11.91 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.88    -0.985  -12.5185]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.92    -0.68    -7.7761]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -65.52749352826524, time: 127.974
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.59    -0.85   -10.5033]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.365   -0.985  -13.1399]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.76    -1.     -11.8203]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.55    -0.845   -9.2084]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -60.15160677213979, time: 130.593
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.835  -0.945 -11.35 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.85    -0.975  -10.6686]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.215   -1.     -11.5081]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.775   -0.93   -11.3534]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -62.5730337440121, time: 126.677
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.365   -0.95   -10.3379]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.595   -0.975  -10.3279]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.62    -1.     -11.5517]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.61    -0.89    -9.3469]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -69.15471258217565, time: 129.444
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.55    -0.91   -11.1013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.18    -0.99   -10.2273]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.41    -0.99   -11.8053]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.1     -0.975  -11.2804]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -68.37174342695484, time: 128.402
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.83    -0.88   -10.8527]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.525   -0.97    -9.8723]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.76    -0.995  -12.2791]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.52    -0.965  -12.6572]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -59.68039939121283, time: 129.764
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.705   -0.9    -10.8649]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.355   -0.94    -9.5915]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.84    -1.     -11.5936]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.565   -0.965  -11.1929]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -63.15105395140939, time: 131.449
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.95    -0.865   -9.2987]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.32    -0.98   -13.3798]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.75   -0.995 -13.244]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.09    -0.845  -10.8744]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -62.34272599455582, time: 129.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.32    -0.865  -10.3904]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.17   -0.995 -10.982]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.23    -1.     -13.2452]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.475   -0.785   -9.9404]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -61.86906135942111, time: 128.075
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.915   -0.875  -11.0605]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.875  -0.96  -10.242]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.07   -0.985 -14.385]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.455   -0.995  -11.6046]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -61.487786232991226, time: 127.913
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.255   -0.925  -10.9986]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.995   -0.965  -11.4645]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.32    -1.     -11.9832]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.39    -0.99   -11.2917]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -62.19287643078779, time: 130.412
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.62    -0.955   -9.7314]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.02    -0.97   -11.1367]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.55    -0.995  -12.3727]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.205   -0.955  -11.1113]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -66.56358609061284, time: 130.016
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.315   -0.905   -9.7057]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.675   -0.955  -10.6032]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.285   -0.96   -11.3757]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.03    -0.595   -7.6711]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -63.417110462282096, time: 127.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.915  -0.915 -11.117]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.925   -0.98   -13.7191]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.655   -0.945  -10.7502]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.58   -0.605  -7.2477]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -63.22238966165138, time: 126.684
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.09    -0.945  -10.4772]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.685   -0.98   -10.0456]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.49    -0.99   -11.9203]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.905  -0.69   -9.138]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -66.36394381127782, time: 126.597
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.335   -0.92    -9.9943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.915   -0.985  -11.4063]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.915   -1.     -12.5391]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.81    -0.725   -9.6247]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -73.5267671549139, time: 128.815
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.665   -0.88    -9.4832]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.63    -0.955  -10.6633]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.325   -0.99   -11.6515]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.455   -0.94   -11.0144]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -60.143424145757926, time: 129.468
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.055   -0.99   -11.9519]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.425   -0.955   -8.6796]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -69.45333394387413, time: 129.56
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.47    -0.955  -11.8535]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.165   -0.94    -9.9469]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.99    -1.     -12.5364]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.64    -0.96    -9.4699]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -78.20507196796788, time: 130.077
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.45    -0.945  -12.0623]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.775   -0.925   -9.9607]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.57    -0.99   -11.9158]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.535   -0.935   -9.4341]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -62.03040843786182, time: 129.081
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.57    -0.935  -10.5536]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.875  -0.875  -7.8404]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.035   -0.995  -11.1956]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.06   -0.92   -7.5544]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -72.41672119508846, time: 128.618
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.52    -0.97   -11.3891]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.76    -0.905   -9.9575]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.545   -1.     -13.2533]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.715  -0.935  -8.2548]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -73.56373724011075, time: 131.293
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.42    -0.925  -10.8939]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.585   -0.955  -10.6902]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.065   -0.99   -12.5524]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.285   -0.92    -8.5628]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -74.46519612997977, time: 128.822
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.47    -0.885  -10.8718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.35    -0.925   -9.8077]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.395   -0.99   -14.1865]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.805   -0.97    -9.5562]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -70.91828329062801, time: 129.762
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.525   -0.91   -11.2916]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.505   -0.905   -9.4939]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.485   -0.88   -12.1865]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.955   -0.955   -8.9098]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -74.29860108892686, time: 130.915
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.78    -0.905  -11.9349]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.65    -0.94   -10.3834]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.96    -0.95   -11.5144]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.895  -0.96   -9.338]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -72.5337252642984, time: 129.51
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.655   -0.93   -10.8787]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.775   -0.86   -10.7622]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.41    -0.965  -11.8257]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.225   -0.92    -8.4164]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -81.5509557992312, time: 130.16
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.2     -0.895   -9.5054]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.475  -0.9   -11.485]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.505   -0.99   -12.4784]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.695   -0.975   -8.8035]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -73.75456982105912, time: 129.487
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.025   -0.945  -10.6602]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.265   -0.91    -9.6174]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.43    -0.98   -11.7186]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.955   -0.975   -8.8712]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -70.52378373545453, time: 130.54
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.245   -0.92   -11.4705]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.25    -0.905  -10.7513]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.56    -0.98   -11.4867]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.14    -0.965   -8.4427]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -77.77709823887159, time: 130.964
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.68    -0.83    -9.9738]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.125   -0.88   -11.7101]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.985   -0.93   -11.3848]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.475   -0.98    -9.3975]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -64.18641898716544, time: 130.583
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.3     -0.9    -10.3386]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.85    -0.915  -11.4891]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.97    -0.89   -10.5685]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.695   -0.925   -9.2803]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -74.63343977628323, time: 132.383
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.94   -0.88  -11.382]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.605   -0.915  -13.1719]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.475   -0.935  -11.1831]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.475   -0.98    -9.4869]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -75.18764821709951, time: 129.423
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.985   -0.81    -9.7145]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.96    -0.87   -13.5578]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.05   -0.975 -11.639]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.155   -0.965   -9.2153]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -71.74362461020101, time: 129.339
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.945  -0.85   -9.905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.305   -0.945  -14.2774]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.565  -0.915 -11.896]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.27    -0.945   -8.6636]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -72.79084523949588, time: 130.692
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.12    -0.885  -10.0408]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.745   -0.96   -13.1669]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.615  -0.975 -11.703]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.635   -0.96    -9.7631]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -72.74709170834018, time: 134.143
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.67    -0.905  -11.0677]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.785   -0.96    -8.5881]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.755   -0.985  -10.2852]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.675  -0.93   -9.333]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.74    -0.93    -9.4813]
34400 50
steps: 1719950, episodes: 34400, mean episode reward: -73.35041736362763, time: 130.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.35    -0.945   -8.8883]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.57    -0.99   -11.3512]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.53    -0.895   -8.7773]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.11    -0.905   -9.2686]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -68.65361420558271, time: 128.645
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.195   -0.925   -8.8597]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.265  -0.97  -10.515]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.235   -0.915   -8.7064]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.41    -0.915   -9.4359]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -69.6878077951546, time: 130.284
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.045   -0.91    -8.2758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.685   -0.995  -12.6528]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.565   -0.945   -8.9249]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.725   -0.95   -10.8342]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -74.30915767354185, time: 130.148
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.645   -0.915   -9.1542]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.505  -0.975 -10.692]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.11    -0.9     -9.1767]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.78    -0.925  -10.1387]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -70.55236024848024, time: 130.957
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.815  -0.875  -8.508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.29    -0.975  -11.1992]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.24   -0.795  -7.8201]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.355   -0.915  -10.4924]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -64.70986440140764, time: 129.537
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.87  -0.915 -8.036]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.505   -0.985  -11.2438]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.885  -0.905  -8.4585]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.245   -0.945  -10.1165]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -72.83304185539902, time: 130.389
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.185   -0.94    -8.7013]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.09    -0.94   -11.6044]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.595 -0.885 -8.076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.475   -0.935   -9.6314]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -75.84002497846575, time: 130.654
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.96    -0.94    -8.8759]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.13   -0.985 -10.459]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.93   -0.875  -7.5379]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.075   -0.885   -9.3204]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -73.78195520827052, time: 130.286
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.57    -0.955   -9.9676]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.285   -1.     -11.9476]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.175  -0.93   -7.8235]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.21    -0.92    -9.5263]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -73.2798191382894, time: 129.717
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.76    -0.9     -8.7753]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.025   -0.99   -11.6033]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.075  -0.92   -7.7757]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.93   -0.765  -8.734]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -81.32164979021135, time: 129.774
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.46    -0.9     -8.2194]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.885   -1.     -11.5265]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.78    -0.94    -9.6074]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.575   -0.935   -9.0391]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -64.2816429063071, time: 130.263
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.325   -0.905   -9.0533]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.115  -0.99  -10.376]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.175   -0.935   -9.3053]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.79    -0.91    -8.6362]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -64.05953167048423, time: 130.753
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.44    -0.95    -9.6232]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.9     -0.99   -10.5133]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.94   -0.905  -8.2306]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.355   -0.875   -8.0301]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -68.19168728164387, time: 131.342
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.26    -0.945   -9.1018]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.19    -0.905  -10.7766]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.875   -0.955   -9.0822]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.205   -0.875   -8.4378]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -67.5188439591178, time: 132.318
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.33    -0.975   -9.9756]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.25    -0.92   -11.0804]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.78    -0.935   -8.9706]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.88    -0.765   -7.9579]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -69.36267606312343, time: 130.746
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.28    -0.9     -8.0359]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.74    -0.96   -11.1125]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.615  -0.945  -8.1905]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.665   -0.935  -10.4499]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -80.82465833829255, time: 131.702
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.425  -0.97   -7.7927]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.49    -0.99   -11.9727]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.78   -0.825  -8.1161]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.62    -0.905   -9.6021]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -64.59993623119263, time: 130.447
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.595  -0.925  -7.2966]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.44    -0.945  -11.1334]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.11    -0.95    -9.2434]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.91    -0.79    -8.8224]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -69.6875034784112, time: 132.294
[-16.975   -0.925  -10.7861]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-19.185   -0.775  -10.7628]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.235   -0.88    -9.8953]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -73.64702598054895, time: 128.5
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.57    -0.95    -8.8593]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.57    -0.87    -7.5129]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.645   -0.765   -9.6332]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.11    -0.96   -11.2911]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -73.72519118362766, time: 129.361
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.89    -0.92    -9.5729]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.39    -0.81    -9.4902]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.385   -0.735   -9.2827]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.365   -0.975  -10.8517]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -84.83908979954681, time: 127.517
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.915   -0.89   -10.1517]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.995   -0.805   -7.8798]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.605   -0.785   -9.4672]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.42   -0.905  -9.511]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -71.99986666991154, time: 129.957
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.83    -0.955   -9.2343]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.275  -0.865  -9.49 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.44    -0.785   -8.4842]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.55    -0.845  -11.1058]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -80.40706200240034, time: 128.895
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.73    -0.93   -11.8508]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.18    -0.785   -7.7837]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.43    -0.78    -7.5113]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.84    -0.875  -10.4314]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -70.73195518647798, time: 129.276
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.295   -0.94    -9.4158]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.04   -0.88  -10.344]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.465   -0.785   -8.4439]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.955   -0.92    -9.4346]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -74.96973463783269, time: 128.528
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.98    -0.955   -9.1655]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.005   -0.86    -7.1981]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.335  -0.795  -9.15 ]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.305   -0.97   -11.5445]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -75.94149872027968, time: 129.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.005   -0.955   -8.9231]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.04   -0.845  -8.733]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.09    -0.79    -9.5048]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.4     -0.895  -10.6788]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -73.35900369726404, time: 131.128
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.54    -0.98   -11.5996]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.325   -0.92    -8.7196]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.09    -0.765   -9.5376]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.21    -0.975  -12.3164]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -68.86136885578583, time: 129.936
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.565   -0.955   -8.9954]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.24    -0.915   -8.1061]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.175   -0.75    -8.8041]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.82   -0.915 -11.586]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -66.3157675116726, time: 129.905
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.705   -0.96    -9.1628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.975   -0.905   -9.7921]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.65    -0.79    -7.5554]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.665   -0.925   -9.0521]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -78.05441966042798, time: 128.296
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.225  -0.955  -8.36 ]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.625   -0.94   -10.7613]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.115   -0.875   -8.8426]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.02    -0.935   -9.9997]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -79.45328734144653, time: 130.151
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.45    -0.965  -12.3391]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.36    -0.92    -9.9881]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.74    -0.89   -13.5322]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.16    -0.905  -11.2707]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -75.53042660517784, time: 130.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.66    -0.95   -10.0938]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.735  -0.865  -7.5341]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.12    -0.915  -10.7735]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.88    -0.91   -10.4937]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -71.35236922367422, time: 128.649
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.82    -0.955   -9.6716]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.91    -0.945   -8.7074]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.805   -0.89    -8.3323]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.28    -0.97   -11.4189]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -74.8943348687033, time: 128.088
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.145   -0.965   -8.1591]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.15    -0.86    -7.7969]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.04    -0.845   -8.8841]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.17   -0.96  -13.844]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -71.9731550986382, time: 130.395
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.97    -0.975   -8.7004]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.26    -0.825   -7.3634]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.38   -0.885 -10.874]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.74    -0.96   -13.3374]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -71.19775302488787, time: 130.402
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.965   -0.99   -11.3008]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.47   -0.91   -7.592]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.315   -0.895  -10.3393]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.3    -0.97  -10.508]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -76.05499719519615, time: 131.529
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.905   -0.96    -8.6468]
[-12.475   -0.945   -9.1862]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.52   -0.44   -3.5357]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.35    -0.955   -8.0902]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -82.32309295798409, time: 128.907
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.95   -0.935 -13.239]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.305  -0.94   -8.815]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.755  -0.475  -4.4144]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.81    -0.905   -8.3906]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -78.43874884012446, time: 130.021
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.16    -0.99   -11.8502]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.77   -0.945  -7.9759]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.045  -0.49   -4.1514]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.92    -0.92    -8.7058]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -76.37716481463441, time: 128.737
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.215   -0.97   -10.9289]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.83   -0.945  -7.4665]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.61   -0.435  -3.6515]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.69    -0.935   -8.8063]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -83.62180918941056, time: 131.769
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.135  -0.955 -10.715]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.93    -0.97    -8.9784]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.86   -0.49   -3.9633]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.3     -0.945   -9.2415]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -74.45009857934362, time: 129.613
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.565   -0.935   -9.5973]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.02   -0.94   -7.5745]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.03   -0.51   -4.0763]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.23    -0.885   -9.5037]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -82.20581723995781, time: 130.639
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.385   -0.95   -10.7936]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.46    -0.965   -9.0331]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.36   -0.535  -4.2723]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.575  -0.92   -9.481]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -62.459211775243965, time: 129.417
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.74    -0.935   -9.2131]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.94   -0.98   -8.2889]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-4.5    -0.52   -3.8302]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.125   -0.92    -8.5364]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -65.60890275878727, time: 131.611
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.245   -0.905   -9.3589]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.625  -0.955  -8.1203]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.74   -0.565  -4.6569]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.595   -0.905   -8.2736]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -78.26968915846405, time: 130.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.605   -0.905  -10.5507]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.6     -0.995  -10.8111]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.33   -0.51   -4.4128]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.35   -0.865  -7.7465]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -83.94071046888314, time: 130.76
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.41    -0.86    -9.6906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.685   -0.865   -8.5076]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.185  -0.51   -4.9647]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.255   -0.85    -8.6396]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -73.43669679600276, time: 129.5
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.12    -0.905   -8.0487]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.745   -0.955   -9.3879]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.49   -0.57   -5.2216]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.83    -0.95    -9.8117]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -65.27491288487052, time: 129.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.96    -0.905   -9.8709]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.01    -0.97    -8.3942]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.45   -0.61   -5.2708]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.895   -0.93    -9.0872]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -77.93922656451608, time: 131.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.94    -0.945   -9.7872]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.125   -0.97    -8.8685]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.79   -0.565  -4.7072]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.76    -0.87    -8.8995]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -72.16792759030622, time: 131.255
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.125   -0.935  -10.5017]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.175 -0.93  -7.666]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.88   -0.65   -6.2164]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.23    -0.905   -9.2078]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -71.53252445912197, time: 129.703
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.065   -0.99   -10.2001]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.02    -0.955   -9.2974]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.485 -0.645 -5.699]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.34    -0.81   -10.2017]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -81.89059810887433, time: 130.047
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.99    -0.88    -9.0843]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.2     -0.975  -10.2487]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.45   -0.635  -5.6714]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.59    -0.85    -9.6242]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -76.78183467221368, time: 129.919
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.115   -0.925   -8.8889]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.295   -0.935   -9.0541]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.485  -0.575  -5.1699]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.265   -0.85    -9.9051]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -65.90081663620305, time: 132.541
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.11    -0.845   -8.0718]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.56  -0.955 -7.963]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.12   -0.555  -5.5057]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.625   -0.83    -8.7736]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -73.7430479798216, time: 131.836
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.32  -0.85  -8.78]
agent1_energy_min, agent1_energy_max, agent1_energy_avg

agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.175   -0.9     -8.1562]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.44    -0.73    -8.8629]
34600 50
steps: 1729950, episodes: 34600, mean episode reward: -76.29194526749785, time: 129.268
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.085   -0.96   -18.3801]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.98    -0.945  -12.0144]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.385   -0.875   -8.6165]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.065   -0.83    -9.9863]
34800 50
steps: 1739950, episodes: 34800, mean episode reward: -83.17897211429306, time: 130.307
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.955  -0.955 -17.162]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.99    -0.81   -10.1312]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.27    -0.91    -9.4129]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.485   -0.91    -9.6384]
35000 50
steps: 1749950, episodes: 35000, mean episode reward: -82.80196185308694, time: 129.732
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-29.85    -0.92   -16.8794]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.655   -0.965  -10.4731]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.32    -0.945  -11.6207]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.43    -0.9    -10.3619]
35200 50
steps: 1759950, episodes: 35200, mean episode reward: -77.13443506704292, time: 130.555
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-33.515   -0.955  -17.9789]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.98    -0.925  -10.1804]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.865   -0.875  -10.4696]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.095   -0.945   -9.3073]
35400 50
steps: 1769950, episodes: 35400, mean episode reward: -84.27167495795067, time: 129.506
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-36.935   -0.965  -19.5099]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.745   -0.935  -10.0498]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-21.035  -0.915 -11.838]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.7     -0.91   -10.2225]
35600 50
steps: 1779950, episodes: 35600, mean episode reward: -84.45875725581155, time: 130.594
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.485   -0.955  -17.7845]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.945   -0.98   -10.8785]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.11    -0.93   -12.6226]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-19.15    -0.89   -11.8387]
35800 50
steps: 1789950, episodes: 35800, mean episode reward: -78.60567592049337, time: 130.86
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-31.405   -0.99   -17.6351]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.73    -0.995  -10.8274]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.285   -0.91   -13.1401]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.325   -0.785   -9.4099]
36000 50
steps: 1799950, episodes: 36000, mean episode reward: -73.47259363477802, time: 130.585
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-22.73   -1.    -14.721]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.08    -0.985  -13.5622]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.68    -0.955  -10.0768]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.62    -0.69    -8.0924]
36200 50
steps: 1809950, episodes: 36200, mean episode reward: -74.14221818241616, time: 130.872
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.1     -0.995  -13.6431]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.125   -0.97   -12.6209]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.045   -0.865   -8.7546]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.805  -0.87   -7.9224]
36400 50
steps: 1819950, episodes: 36400, mean episode reward: -75.43890673519125, time: 131.825
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.38    -0.995  -12.6805]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.98    -0.98   -11.5383]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.25    -0.905  -11.6142]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.05    -0.91    -8.9142]
36600 50
steps: 1829950, episodes: 36600, mean episode reward: -75.63411477607374, time: 131.842
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.26    -1.     -13.1905]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.175   -0.935  -10.6418]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.92    -0.905   -9.4147]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.44    -0.835   -9.3869]
36800 50
steps: 1839950, episodes: 36800, mean episode reward: -70.98635931949761, time: 130.995
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.82    -0.99   -12.4778]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.085   -0.95   -12.9653]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.32    -0.965  -11.4208]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.005   -0.855   -8.5811]
37000 50
steps: 1849950, episodes: 37000, mean episode reward: -71.2092568939603, time: 130.384
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.09    -0.995  -12.2874]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.575   -0.94   -11.8272]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.67    -0.88    -9.9829]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.005   -0.895   -8.6281]
37200 50
steps: 1859950, episodes: 37200, mean episode reward: -72.56583785990148, time: 133.547
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.65    -0.99   -12.8628]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.55    -0.965  -10.6714]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.55    -0.845   -8.8993]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.695   -0.875   -9.2723]
37400 50
steps: 1869950, episodes: 37400, mean episode reward: -77.58169705760109, time: 130.329
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.09   -0.99  -12.251]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.625   -0.955  -12.0987]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.47    -0.905  -10.3667]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.39    -0.96   -11.7816]
37600 50
steps: 1879950, episodes: 37600, mean episode reward: -72.73124906120827, time: 130.037
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.71    -1.     -12.9192]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.15    -0.98   -13.2143]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.245   -0.91    -8.2417]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.26    -0.885   -9.1094]
37800 50
steps: 1889950, episodes: 37800, mean episode reward: -69.75881541232718, time: 130.784
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.325  -0.97  -12.806]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.065   -0.975  -11.3089]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.005   -0.945   -8.2471]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.26    -0.91    -9.2976]
38000 50
steps: 1899950, episodes: 38000, mean episode reward: -76.37081819912437, time: 132.448
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.735   -0.995  -12.3409]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.815   -0.975  -12.1696]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-8.815  -0.93   -7.4611]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.465   -0.875   -8.6287]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -73.25468779475837, time: 131.314
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.79    -0.995  -11.9144]
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.445   -0.75    -9.0268]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.18    -0.885  -14.7017]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.465   -0.995  -11.7132]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.505   -0.85   -15.1247]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -68.8442719759127, time: 129.212
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.92    -0.775  -10.0779]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.175   -0.935  -14.4348]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.31    -0.97   -12.2747]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.925   -0.93   -16.1563]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -70.28365026109023, time: 129.066
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.115   -0.9    -11.5722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.98    -0.93   -13.7754]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.7     -0.98   -12.5704]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.97    -0.885  -14.2067]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -64.26319999561574, time: 127.35
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.11    -0.87   -10.7569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-25.22    -0.97   -14.8324]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.1    -0.985 -11.963]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-27.555   -0.905  -15.7599]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -73.31678494140561, time: 127.139
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.835   -0.805  -11.2788]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.585   -0.945  -14.3955]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.375   -0.955  -12.3638]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.955   -0.9    -14.5297]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -66.74739622672696, time: 128.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.05    -0.81    -9.6161]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.78    -0.995  -13.6526]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.62   -0.98  -11.888]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.655   -0.955  -14.2628]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -72.63789008828822, time: 127.422
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.925   -0.775   -9.8058]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.455  -0.965 -14.448]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.475   -0.975  -12.6362]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-26.4     -0.92   -15.9711]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -68.14741924850279, time: 126.756
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.6     -0.855  -11.2407]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.23  -0.95 -12.81]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.33    -0.955  -12.3049]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-23.675   -0.925  -14.9986]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -63.38488160222145, time: 128.694
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.9     -0.97   -11.3836]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-22.465   -0.975  -13.5804]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.335   -0.995  -13.1076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-25.94    -0.94   -15.6739]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -68.0910619316983, time: 126.661
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.635   -0.83    -8.8597]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.93    -0.925  -12.1679]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.435   -0.885  -10.1428]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.585   -0.965  -15.4498]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -62.30117942143875, time: 128.547
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.01    -0.85   -10.1807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.09    -0.98   -12.5821]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.96   -0.96  -10.124]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.65    -0.97   -15.0164]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.01 hr

agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.84    -1.     -10.9131]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.565  -0.985 -11.535]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -60.0996361870282, time: 130.907
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.925  -0.975  -9.254]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.5     -0.86   -11.9618]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.415  -0.995 -10.593]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.83   -1.    -11.684]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -60.70392326368539, time: 131.301
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.055   -0.95    -9.2367]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.3    -0.83  -12.543]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.05   -0.995 -10.492]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.02    -0.99   -12.1419]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -56.0944425577736, time: 130.385
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.53    -0.99   -11.1134]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.85    -0.92   -12.5993]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.16    -0.99    -9.9011]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.23    -1.     -11.8351]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -58.21013556322497, time: 129.047
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.55    -0.975   -9.3876]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.465  -0.975 -12.528]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.965  -0.985 -10.538]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.42    -0.995  -11.8834]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -64.26769883716584, time: 128.553
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.665   -0.985  -10.9162]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.455   -0.895  -12.8019]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.74    -0.99   -11.7836]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.915   -0.96   -12.3565]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -62.74101307253944, time: 130.687
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.01    -0.97   -10.8875]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.39    -0.735  -11.2596]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.575   -0.98   -11.8328]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.605   -0.855  -11.3139]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -74.7077344684509, time: 129.44
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.13    -0.98   -10.4177]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.205   -0.975  -22.7826]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.185   -0.98   -11.6733]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.08   -0.915 -11.073]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -83.51550953650406, time: 130.639
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.725   -0.95   -10.8257]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.795  -0.475 -10.606]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.52    -0.995  -12.6381]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.965   -0.985  -12.6458]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -70.17239101850517, time: 129.396
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.145   -0.98    -9.4717]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-21.175   -0.795  -13.0837]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.88    -0.98   -12.0415]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.455   -1.     -11.6656]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -59.07243094364182, time: 127.896
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.39    -0.99    -9.5617]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-26.665   -0.99   -16.1982]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.59    -0.96   -11.6935]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.475   -0.985  -11.0615]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.02 hr

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.72    -0.94   -10.5493]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.72    -0.965  -10.0805]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.36    -1.     -13.1944]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.885   -0.73   -11.8011]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -65.37597391687898, time: 127.581
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.585   -0.925   -9.4453]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.685   -0.95   -10.5731]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.93    -0.95   -11.9442]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.555   -0.86   -11.2618]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -64.16842898547289, time: 125.439
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.345   -0.945  -10.2567]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-19.49    -0.965  -12.8003]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.745   -0.97   -11.3977]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.765   -0.92   -14.3668]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -65.8093338113544, time: 125.716
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.12    -0.955  -10.4312]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.27    -0.975  -11.2546]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.565   -0.815  -10.3283]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.425   -0.855  -11.2987]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -58.69445905044745, time: 124.987
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.87    -0.95   -10.5579]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.705   -0.975   -9.8251]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.19    -0.87   -10.9526]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.075   -0.97   -11.4562]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -70.16806220724713, time: 127.042
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.2     -0.9    -10.3295]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.445   -0.985  -12.7881]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.845   -0.855  -11.0931]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.845   -0.81   -10.4944]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -63.59364994680778, time: 125.808
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.085   -0.945  -11.5397]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.155   -0.975  -10.7355]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.14    -0.935  -11.6545]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.755   -0.83   -11.8794]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -60.81495130508067, time: 125.24
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.02    -0.99   -12.5177]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.53    -0.98   -10.3522]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.91    -0.975  -11.3753]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.15   -0.855 -10.579]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -62.16433327414906, time: 129.308
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.34    -0.885  -11.5423]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.175  -0.94  -10.408]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.08    -0.98   -11.4995]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.055   -0.92   -11.7739]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -67.26931288041072, time: 124.213
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.345   -0.9    -10.3304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.06    -0.95   -10.4539]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-18.09    -0.98   -12.4764]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.8     -0.94   -11.3228]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.02 hr

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.555   -0.955   -9.7921]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.07    -1.     -24.7966]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.76   -0.81  -10.749]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.38    -0.985  -11.9515]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -70.15934533055517, time: 130.138
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.385   -0.96   -10.8864]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.75    -1.     -24.6234]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.245   -0.8     -8.2078]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-16.365   -0.96   -12.1478]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -56.545448894242966, time: 127.782
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.82    -0.975   -9.6384]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.155   -1.     -24.9524]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.64   -0.815  -8.403]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.885   -0.975  -10.7301]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -75.56492309118667, time: 126.998
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.51    -0.955   -9.9924]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-48.945   -0.995  -24.7308]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.665   -0.89   -13.5935]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.39    -0.975  -10.9838]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -62.977229613381986, time: 126.79
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.8     -0.965   -9.6722]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.235   -1.     -23.8015]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.255   -0.945  -11.1005]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.09  -0.99 -10.9 ]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -63.74376657205228, time: 126.98
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.585   -0.99   -10.9026]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-43.18    -0.805  -21.7585]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.955   -0.88    -9.7084]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.915   -0.98   -11.8656]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -59.6033888817393, time: 128.267
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.17    -0.97   -10.3712]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.425   -1.     -24.3077]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-22.48    -0.87   -13.6813]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.995   -0.995  -10.7934]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -64.49762961118104, time: 126.339
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.43    -0.99   -11.1333]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-49.52    -1.     -25.2763]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-25.815   -0.955  -15.2445]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.39    -0.905  -10.6115]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -65.06073810657415, time: 126.365
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.19    -0.96   -10.7304]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.85    -1.     -24.1545]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-20.84    -0.98   -12.9123]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.44    -1.     -11.1427]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -61.05568628099681, time: 127.77
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.125   -0.985  -11.7162]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.045   -1.     -22.2416]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.155   -0.93   -10.3362]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.85    -0.96   -11.4426]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -66.3400056612502, time: 118.728
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.045   -1.     -12.8264]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-36.325   -1.     -20.0505]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.52    -0.7     -8.5776]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.295   -0.975  -11.9641]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.03 hr

[-12.79    -0.7     -8.1058]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.4     -0.845  -13.2644]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -74.50041286020611, time: 131.991
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.31   -0.88   -7.0859]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-29.82    -0.915  -16.6485]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.66    -0.63    -7.4023]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-24.915   -0.785  -14.1498]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -71.43202507770938, time: 128.331
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.28   -0.89   -6.4673]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.48    -0.94   -17.0808]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.94   -0.68   -6.8995]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.575   -0.865  -12.8142]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -82.32217632716, time: 128.742
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.475  -0.85   -6.2395]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-35.72    -0.99   -19.9278]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.555  -0.645  -6.6563]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.99    -0.84   -10.1591]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -67.69150608250771, time: 127.955
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.82   -0.905  -6.4921]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.45    -0.99   -19.2777]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.435   -0.68    -7.2432]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.49    -0.865  -11.5127]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -69.2177718515431, time: 128.627
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.78   -0.94   -6.5728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.04    -0.98   -18.7787]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.8    -0.74   -8.724]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.475  -0.86  -12.555]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -75.36576296573693, time: 129.255
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.39   -0.88   -7.5152]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-30.075   -0.96   -17.3678]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.29    -0.83    -9.5166]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.91    -0.915  -13.4412]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -67.70172258537964, time: 128.08
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.335  -0.915  -6.8514]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-34.52    -0.98   -19.0531]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.135   -0.795   -8.5627]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-21.425   -0.91   -12.9954]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -67.36046312610239, time: 131.483
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.04   -0.93   -6.7332]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.76    -0.94   -18.1303]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.515  -0.69   -6.7924]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-20.625   -0.895  -12.7764]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -70.27041630301319, time: 129.192
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.08   -0.945  -7.2946]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-32.43    -0.98   -18.4698]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.215  -0.73   -6.8598]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.96   -0.89  -12.192]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -77.78854387318295, time: 112.147
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.715  -0.915  -7.0569]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-33.435   -0.985  -18.9808]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.365  -0.715  -6.8173]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.49   -0.94  -13.792]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.04 hr

agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.245   -0.99   -22.2684]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-29.225  -0.965 -16.936]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-47.26    -0.995  -24.0408]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -54.55310743588087, time: 133.77
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.755  -0.94  -12.802]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.305   -0.995  -21.7187]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-26.545   -0.96   -15.7656]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.475   -0.995  -23.5525]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -48.81253340282403, time: 131.048
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.59    -0.95   -12.5302]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-44.33    -1.     -22.9654]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-23.98    -0.99   -14.7877]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-44.885   -1.     -22.8358]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -51.001309583922215, time: 129.665
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.155   -0.95   -11.9369]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-39.3     -0.99   -20.9886]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-33.865  -0.97  -18.765]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.815   -0.99   -23.2495]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -49.818131325643044, time: 128.575
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.38    -0.975  -12.0382]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.065   -1.     -24.1374]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-24.425   -0.94   -15.0453]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.88    -0.99   -23.7925]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -53.36010111152379, time: 130.689
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.25    -0.98   -13.0388]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-41.045   -1.     -21.7839]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-36.175   -0.985  -19.6185]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.15    -0.955  -22.2568]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -55.87375863777718, time: 131.297
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.435   -0.955  -12.0917]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.02    -0.99   -23.5171]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-39.345  -0.93  -20.746]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-46.345   -0.99   -23.6776]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -52.72344571936117, time: 130.288
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.9     -0.965  -13.1738]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-47.975   -0.995  -24.4561]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-35.63    -0.985  -19.4512]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.365   -0.98   -23.1475]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -49.97155888168903, time: 129.956
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.72    -0.965  -13.5879]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-46.015   -1.     -23.7272]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-38.87    -0.98   -20.6741]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-45.17    -0.975  -23.2879]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -54.629191649343355, time: 129.177
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.78    -0.965  -13.5053]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-42.195   -1.     -22.2955]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-42.83    -0.975  -22.3616]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-42.76    -0.985  -21.9102]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -48.97733133741021, time: 101.356
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.105   -0.915  -11.6753]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-40.97    -0.995  -21.6239]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-27.7     -0.985  -16.2698]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-43.89    -1.     -22.6482]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.05 hr

agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.02    -0.92    -8.8977]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.685   -0.81    -7.9073]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-22.11    -0.985  -13.9012]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -72.32906743103776, time: 129.612
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.29    -0.98    -8.8642]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.505   -0.89    -8.7014]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.23   -0.76   -7.0029]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.14    -0.95   -10.5005]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -74.07626580555385, time: 127.991
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.64   -0.905  -7.6991]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.815  -0.905  -7.4922]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.7     -0.845   -9.1179]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.91   -0.84  -10.173]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -78.13404691726551, time: 127.735
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.105   -0.975   -9.7776]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.75    -0.945   -8.1787]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.385  -0.78   -7.4539]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.285   -0.955  -10.9657]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -66.80010103135575, time: 130.593
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-11.975   -0.965   -9.0044]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.39    -0.92    -8.0561]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.745  -0.875  -6.4959]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.825   -0.98   -11.8165]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -69.39787090729747, time: 129.458
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.675   -0.95    -9.7835]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.895   -0.87    -9.5606]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.37    -0.89    -9.7185]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.865   -0.97   -11.0572]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -70.35264627349652, time: 128.754
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.24    -0.965   -9.2217]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.23    -0.925  -10.9366]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.49    -0.85    -9.2302]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.81    -0.98   -12.4648]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -71.92341400422453, time: 129.323
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.435   -0.93    -9.8713]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.99    -0.965  -10.7567]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.81   -0.86   -6.6432]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.65    -0.975  -10.9911]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -67.16686495664322, time: 120.805
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.34  -0.97  -6.943]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.475   -0.885   -9.4886]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-9.115  -0.91   -7.5168]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.225   -0.975  -11.1847]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -67.28528969066325, time: 93.446
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.735   -0.985   -8.1164]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.855   -0.925   -8.8426]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.645  -0.915 -10.003]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.035   -0.91   -10.8442]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.05 hr

[-15.44    -0.895  -10.6012]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.745   -0.99   -12.3076]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.915   -0.98    -8.9793]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -63.15024022265172, time: 131.973
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.515   -0.895  -11.3906]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.32    -0.925  -10.4521]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.39   -0.99  -10.873]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.62   -0.98   -8.2166]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -65.69551282905661, time: 129.169
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.465   -0.835   -9.9485]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.505   -0.95   -10.1839]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.35    -0.985  -11.4897]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.97    -0.955   -9.0165]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -68.5348806276394, time: 129.814
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.405   -0.865  -10.9116]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.1     -0.945  -12.0179]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.59    -1.     -11.1515]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.07    -0.97    -9.1094]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -72.40944296057683, time: 130.216
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.215  -0.91  -11.054]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.36    -0.875  -10.8925]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.3     -0.96   -11.1131]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.14    -0.965   -9.1368]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -66.34934443677918, time: 131.474
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.02    -0.895  -10.6978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.135   -0.86   -10.6301]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.21    -0.97   -10.9185]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.565   -0.965   -9.4263]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -71.78143867787713, time: 130.207
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.5     -0.865  -10.5056]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.63    -0.895  -12.1327]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.095   -0.97   -11.8407]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.54   -0.97  -10.226]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -72.76692581638467, time: 129.806
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-15.305   -0.89   -11.3685]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.065   -0.875  -11.7172]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.285   -0.97   -12.0794]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.6     -0.99    -9.8686]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -67.38896095387938, time: 131.287
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-12.49    -0.89    -9.6979]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.38    -0.95   -12.1819]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-13.69   -0.965 -11.059]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-15.285   -0.975  -11.0111]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -81.02934972284369, time: 107.29
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.975   -0.91   -11.2202]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-18.995   -0.94   -12.6994]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-15.685   -0.99   -12.3393]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.22    -0.97   -11.8862]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -67.82424165380307, time: 85.375
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.525   -0.955  -11.9758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.33    -0.93   -11.3076]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.76    -0.975  -11.3709]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-10.365   -0.98    -8.7495]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.07 hr
[-10.025  -0.95   -8.26 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.495  -0.615  -5.2502]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.675   -0.89   -10.6976]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -79.3583670505474, time: 130.463
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-13.275  -0.975 -10.139]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.645  -0.915  -7.7687]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.545  -0.65   -5.8301]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.62    -0.785   -9.7382]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -76.35787851919505, time: 129.584
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-14.76    -0.97   -10.8964]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-8.98   -0.91   -7.4946]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.275  -0.63   -5.2401]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.03    -0.83    -9.0617]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -79.68398709094188, time: 129.433
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.08    -0.905  -11.3162]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.005   -0.965   -9.6936]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.17   -0.675  -5.8234]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.695   -0.865   -8.8362]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -70.86513575133029, time: 128.9
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-21.805  -0.985 -13.631]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.87   -0.95   -8.1296]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.78   -0.685  -6.2382]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.37    -0.84    -9.4144]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -68.33558996652708, time: 130.004
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.51    -0.96   -12.1758]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.29  -0.92  -7.704]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.025  -0.655  -5.0264]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.45    -0.815   -8.7535]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -78.95067843072925, time: 130.999
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.83    -0.88   -11.7658]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.66    -0.915   -8.5532]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.23   -0.67   -5.2074]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.61    -0.79    -9.7171]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -78.93281808897673, time: 130.272
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.785   -0.96   -11.4853]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-11.845   -0.965   -9.4893]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-5.995  -0.725  -5.0509]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.19    -0.775  -10.7694]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -85.83163252746546, time: 105.014
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-23.295   -0.835  -12.8484]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-10.825  -0.915  -8.413]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-6.405  -0.715  -5.2499]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.88    -0.795   -9.9209]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -71.55948772885478, time: 84.863
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.805   -0.96   -12.7978]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-9.815  -0.93   -8.0367]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-7.09   -0.745  -5.8792]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.105   -0.865   -9.4799]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.07 hr

agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.55   -0.89   -6.4951]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.29    -0.975  -11.7111]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.255  -0.945  -9.731]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.715   -0.81    -9.7082]
38200 50
steps: 1909950, episodes: 38200, mean episode reward: -68.46257056353804, time: 131.582
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.195  -0.905  -6.8105]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.85    -0.995  -11.0968]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.835   -0.94    -9.1355]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.54    -0.835   -9.7081]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -73.44680955727725, time: 129.69
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.605  -0.9    -7.2523]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.615   -0.995  -11.1314]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.45    -0.98    -9.4046]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.985   -0.805   -9.6354]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -73.1898082362303, time: 129.74
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.42   -0.905  -7.1368]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.34    -1.     -12.3705]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.07    -0.975   -9.5442]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.52    -0.805  -10.0183]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -74.20797810213584, time: 132.062
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-7.79   -0.785  -6.5189]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-17.415   -0.96   -12.3794]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.45   -0.985  -9.353]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.895   -0.93   -11.0245]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -68.73921517999923, time: 129.857
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.12   -0.93   -7.5807]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.5     -0.975  -10.6503]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.045   -0.95    -9.1301]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.41    -0.88    -9.6579]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -68.62754507112666, time: 131.658
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.39   -0.915  -7.0728]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.2     -0.98   -11.9596]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-12.415   -0.99    -9.8866]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-14.755   -0.9    -10.9989]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -68.22837000347373, time: 131.971
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-8.555  -0.87   -7.1289]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.25   -0.995 -12.11 ]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-16.195   -0.995  -12.1364]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.755   -0.92    -9.7043]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -67.84157927523388, time: 123.846
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-10.635   -0.93    -8.3588]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-14.97    -0.9    -10.6121]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.81    -0.995  -11.2319]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.96    -0.86    -9.7116]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -74.09075960171972, time: 95.524
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.94   -0.945  -7.8988]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.425   -0.93   -11.5574]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-17.675   -0.96   -12.2727]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-17.265   -0.885  -11.0733]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -67.02785670164334, time: 78.424
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-9.855  -0.92   -8.2185]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-12.49    -0.93   -10.0827]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-14.71    -1.     -11.1624]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-18.235   -0.9    -11.5774]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.07 hr

agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-15.965   -0.865  -11.7133]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.59    -0.98    -8.8769]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.335   -0.965  -10.1494]
38400 50
steps: 1919950, episodes: 38400, mean episode reward: -77.47475210646233, time: 130.941
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.255   -1.     -12.4332]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.72   -0.97  -12.031]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.995   -0.955   -9.1121]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.77    -0.93    -9.6409]
38600 50
steps: 1929950, episodes: 38600, mean episode reward: -76.63474383051413, time: 130.13
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.84    -1.     -13.0517]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-20.99    -0.955  -13.9259]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.425   -0.945   -8.7997]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.49   -0.9    -9.887]
38800 50
steps: 1939950, episodes: 38800, mean episode reward: -75.25785908835898, time: 130.63
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-20.045   -1.     -13.9565]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.54    -0.915  -10.4161]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.53    -0.96    -8.8434]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.25    -0.92   -10.5406]
39000 50
steps: 1949950, episodes: 39000, mean episode reward: -86.16034948681259, time: 129.654
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-17.44    -0.995  -13.2284]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-16.115   -0.955  -12.2451]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.04    -0.935   -9.1805]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.205   -0.905  -10.3834]
39200 50
steps: 1959950, episodes: 39200, mean episode reward: -79.67698806463547, time: 130.074
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.33    -1.     -12.8681]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-23.09    -0.99   -15.3415]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.175   -0.905   -8.5129]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-12.285   -0.925   -9.6495]
39400 50
steps: 1969950, episodes: 39400, mean episode reward: -75.74084496800248, time: 130.552
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-18.415   -1.     -13.6242]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-13.995  -0.94  -10.877]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.27    -0.95    -9.3057]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-13.09    -0.945  -10.4295]
39600 50
steps: 1979950, episodes: 39600, mean episode reward: -75.52265194513971, time: 116.964
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.29    -0.975  -12.2962]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-24.615   -0.925  -15.0654]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-11.14   -0.93   -9.096]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.615   -0.95    -9.4058]
39800 50
steps: 1989950, episodes: 39800, mean episode reward: -76.3035156564477, time: 90.73
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-16.545   -1.     -12.1794]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.15    -0.935  -15.4951]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.285   -0.905   -8.3602]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-9.59   -0.905  -8.1225]
40000 50
steps: 1999950, episodes: 40000, mean episode reward: -79.40167657726425, time: 75.587
agent0_energy_min, agent0_energy_max, agent0_energy_avg
[-19.035   -0.98   -12.9552]
agent1_energy_min, agent1_energy_max, agent1_energy_avg
[-27.26    -0.85   -15.2769]
agent2_energy_min, agent2_energy_max, agent2_energy_avg
[-10.35    -0.98    -8.6994]
agent3_energy_min, agent3_energy_max, agent3_energy_avg
[-11.65    -0.915   -9.5824]
...Finished!
Trained episodes: 1 -> 40000
Total time: 7.08 hr
